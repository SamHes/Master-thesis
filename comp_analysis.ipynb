{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import itertools\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "import heapq\n",
    "import copy\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from venn import venn\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_mutual_info_score, adjusted_rand_score\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers_df = pd.read_csv('data/1_year/reduced_transfers_1_year.csv')\n",
    "\n",
    "def filter_votes(data):\n",
    "    for proposal in data:\n",
    "        proposal['votes'] = [vote for vote in proposal['votes'] if float(vote['weight']) > 0]\n",
    "        \n",
    "    return data\n",
    "\n",
    "with open('data/proposals/compound/compound_proposals_1_year.json', 'r', encoding='utf8') as file:\n",
    "    comp_proposals = json.load(file)['data']['proposals']\n",
    "    \n",
    "with open('data/proposals/compound/compound_votes_1_year.json', 'r', encoding='utf8') as file:\n",
    "    comp_votes = filter_votes(json.load(file))\n",
    "    \n",
    "with open('data/proposals/compound/compound_delegations.json', 'r', encoding='utf8') as file:\n",
    "    delegations = json.load(file)['data']['delegateChanges']\n",
    "    \n",
    "delegations = [\n",
    "    entry for entry in delegations\n",
    "    if 1696118399 < int(entry['blockTimestamp']) < 1727740800\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMP_tokens = ['COMP', 'PolCOMP', 'ArbCOMP']\n",
    "cTokens = ['cWETHv3', 'cArbWETH']\n",
    "# cTokens = ['cUSDCv3', 'cArbUSDC', 'cUSDTv3', 'cPolUSDT', 'cArbUSDT', 'cWETHv3', 'cArbWETH']\n",
    "\n",
    "comp_df = transfers_df[\n",
    "    transfers_df['token'].isin(COMP_tokens) &\n",
    "    (transfers_df['from'].str.lower() != transfers_df['to'].str.lower()) &\n",
    "    (transfers_df['value'] != 0)\n",
    "]\n",
    "cToken_df = transfers_df[\n",
    "    transfers_df['token'].isin(cTokens) &\n",
    "    (transfers_df['from'].str.lower() != transfers_df['to'].str.lower()) &\n",
    "    (transfers_df['value'] != 0)\n",
    "]\n",
    "\n",
    "# addresses = pd.read_csv('data/addresses/comp_addresses.csv')\n",
    "\n",
    "# EOA = set(addresses.loc[addresses['type'] == False, 'address'])\n",
    "# CA = set(addresses.loc[addresses['type'] == True, 'address'])\n",
    "\n",
    "# CEX_addresses = pd.read_csv('data/exchanges/cex.csv')\n",
    "# CEX = set(CEX_addresses['address'].str.lower())\n",
    "\n",
    "G_comp_nodes = set(comp_df['from'].str.lower()).union(set(comp_df['to'].str.lower()))\n",
    "G_cToken_nodes = set(cToken_df['from'].str.lower()).union(set(cToken_df['to'].str.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP GRAPH AGGREGATED BY TRANSFER VALUES\n",
    "\n",
    "G_comp = nx.DiGraph()\n",
    "\n",
    "for _, row in comp_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if G_comp.has_edge(from_address, to_address):\n",
    "        G_comp[from_address][to_address]['weight'] += value\n",
    "    else:\n",
    "        G_comp.add_edge(from_address, to_address, weight=value)\n",
    "\n",
    "largest_wcc_nodes = max(nx.weakly_connected_components(G_comp), key=len)\n",
    "G_comp = G_comp.subgraph(largest_wcc_nodes).copy()\n",
    "\n",
    "G_comp_nodes = set(G_comp.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP TWMDG\n",
    "\n",
    "G_comp_complete = nx.MultiDiGraph()\n",
    "\n",
    "for _, row in comp_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    timestamp = row['timestamp']\n",
    "    \n",
    "    G_comp_complete.add_edge(from_address, to_address, weight=value, timestamp=timestamp)\n",
    "\n",
    "largest_wcc_nodes = max(nx.weakly_connected_components(G_comp_complete), key=len)\n",
    "G_comp_complete = G_comp_complete.subgraph(largest_wcc_nodes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP GRAPH AGGREGATED BY NUMBER OF TRANSFERS\n",
    "\n",
    "G_comp_count = nx.DiGraph()\n",
    "\n",
    "for _, row in comp_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    \n",
    "    if G_comp_count.has_edge(from_address, to_address):\n",
    "        G_comp_count[from_address][to_address]['weight'] += 1\n",
    "    else:\n",
    "        G_comp_count.add_edge(from_address, to_address, weight=1)\n",
    "\n",
    "largest_wcc_nodes = max(nx.weakly_connected_components(G_comp_count), key=len)\n",
    "G_comp_count = G_comp_count.subgraph(largest_wcc_nodes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cWETH GRAPH AGGREGATED BY TRANSFER VALUES\n",
    "\n",
    "G_cWETH = nx.DiGraph()\n",
    "\n",
    "for _, row in cToken_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if G_cWETH.has_edge(from_address, to_address):\n",
    "        G_cWETH[from_address][to_address]['weight'] += value\n",
    "    else:\n",
    "        G_cWETH.add_edge(from_address, to_address, weight=value)\n",
    "        \n",
    "G_cToken_nodes = set(G_cWETH.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cWETH TWMDG\n",
    "\n",
    "G_cWETH_complete = nx.MultiDiGraph()\n",
    "\n",
    "for _, row in cToken_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    timestamp = row['timestamp']\n",
    "    \n",
    "    G_cWETH_complete.add_edge(from_address, to_address, weight=value, timestamp=timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Governance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_counts = {}\n",
    "for proposal in comp_proposals:\n",
    "    proposer = proposal['proposer']['id']\n",
    "    if proposer in proposal_counts:\n",
    "        proposal_counts[proposer] += 1\n",
    "    else:\n",
    "        proposal_counts[proposer] = 1\n",
    "\n",
    "vote_counts = {}\n",
    "for votes in comp_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['voter'].get('id')\n",
    "        if voter in vote_counts:\n",
    "            vote_counts[voter] += 1\n",
    "        else:\n",
    "            vote_counts[voter] = 1\n",
    "\n",
    "vote_weights = {}\n",
    "for votes in comp_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['voter'].get('id')\n",
    "        weight = vote['weight']\n",
    "        if voter in vote_weights:\n",
    "            vote_weights[voter] += float(weight)\n",
    "        else:\n",
    "            vote_weights[voter] = float(weight)\n",
    "            \n",
    "delegation_counts = {}\n",
    "from_delegations = {}\n",
    "to_delegations = {}\n",
    "\n",
    "for delegation in delegations:\n",
    "    delegator = delegation['delegator'].lower()\n",
    "    if delegator in delegation_counts:\n",
    "        delegation_counts[delegator] += 1\n",
    "    else:\n",
    "        delegation_counts[delegator] = 1\n",
    "        \n",
    "    if delegator in from_delegations:\n",
    "        from_delegations[delegator] += 1\n",
    "    else:\n",
    "        from_delegations[delegator] = 1\n",
    "                \n",
    "    delegate = delegation['delegate'].lower()\n",
    "    if delegate in delegation_counts:\n",
    "        delegation_counts[delegate] += 1\n",
    "    else: \n",
    "        delegation_counts[delegate] = 1\n",
    "        \n",
    "    if delegate in to_delegations:\n",
    "        to_delegations[delegate] += 1\n",
    "    else:\n",
    "        to_delegations[delegate] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposers = set()\n",
    "\n",
    "for proposal in comp_proposals:\n",
    "    proposers.add(proposal['proposer']['id'])\n",
    "    \n",
    "proposers = proposers & G_comp_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voter in vote_weights:\n",
    "    vote_weights[voter] = vote_weights[voter] / vote_counts[voter]\n",
    "    \n",
    "voters = set()\n",
    "\n",
    "for votes in comp_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voters.add(vote['voter'].get('id'))\n",
    "        \n",
    "voters_in_G_comp = voters & G_comp_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegators = set()\n",
    "delegates = set()\n",
    "\n",
    "from_delegations_without_same_to = set()\n",
    "to_delegations_without_same_from = set()\n",
    "\n",
    "for delegate in delegations:\n",
    "    delegator = delegate['delegator']\n",
    "    delegate = delegate['delegate']\n",
    "    \n",
    "    delegators.add(delegator)\n",
    "    delegates.add(delegate)\n",
    "    \n",
    "    if delegator != delegate:\n",
    "        from_delegations_without_same_to.add(delegator)\n",
    "        to_delegations_without_same_from.add(delegate)    \n",
    "    \n",
    "all_delegations = (delegators | delegates) & G_comp_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_voters_in_G_comp = voters & G_comp_nodes\n",
    "all_proposers_voters_delegators_in_G_comp = (proposers | voters | delegators) & G_comp_nodes\n",
    "all_economic_users_in_G_comp = (set(cToken_df['from'].str.lower()) | set(cToken_df['to'].str.lower())) & G_comp_nodes\n",
    "\n",
    "only_economic_users_in_G_comp = all_economic_users_in_G_comp - all_voters_in_G_comp\n",
    "only_voters_in_G_comp = all_voters_in_G_comp - all_economic_users_in_G_comp\n",
    "both_economic_and_governance_users = all_economic_users_in_G_comp & all_voters_in_G_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_proposers_voters_delegators_in_G_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total: {len(G_comp_nodes)}\\nProposers: {len(proposers)}\\nVoters: {len(voters)}\\nDelegations: {len(all_delegations)}\\nGovernance: {len(only_governance_users)}\\nEconomic: {len(only_economic_users)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual feature computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_cWETH_igraph = ig.Graph.from_networkx(G_cWETH, vertex_attr_hashable='name')\n",
    "G_comp_igraph = ig.Graph.from_networkx(G_comp, vertex_attr_hashable='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cWETH_igraph_in_dc = [d for d in G_cWETH_igraph.indegree()]\n",
    "cWETH_igraph_out_dc = [d for d in G_cWETH_igraph.outdegree()]\n",
    "\n",
    "cWETH_igraph_in_dc = {G_cWETH_igraph.vs[i][\"name\"]: score for i, score in enumerate(cWETH_igraph_in_dc)}\n",
    "cWETH_igraph_out_dc = {G_cWETH_igraph.vs[i][\"name\"]: score for i, score in enumerate(cWETH_igraph_out_dc)}\n",
    "\n",
    "comp_igraph_in_dc = [d for d in G_comp_igraph.indegree()]\n",
    "comp_igraph_out_dc = [d for d in G_comp_igraph.outdegree()]\n",
    "\n",
    "comp_igraph_in_dc = {G_comp_igraph.vs[i][\"name\"]: score for i, score in enumerate(comp_igraph_in_dc)}\n",
    "comp_igraph_out_dc = {G_comp_igraph.vs[i][\"name\"]: score for i, score in enumerate(comp_igraph_out_dc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigenvector_manual(graph, tol=1e-6, max_iter=100):    \n",
    "    n = graph.vcount()\n",
    "    edges = np.array(graph.get_edgelist())\n",
    "    weights = np.array(graph.es[\"weight\"])\n",
    "\n",
    "    row, col = edges[:, 1], edges[:, 0]\n",
    "    W = sp.csr_matrix((weights, (row, col)), shape=(n, n))\n",
    "\n",
    "    x = np.ones(n)\n",
    "    for _ in range(max_iter):\n",
    "        x_new = W @ x\n",
    "        x_new /= np.linalg.norm(x_new, ord=2)\n",
    "        \n",
    "        if np.linalg.norm(x_new - x, ord=2) / np.linalg.norm(x, ord=2) < tol:\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "\n",
    "    return {graph.vs[i][\"name\"]: x[i] for i in range(n)}\n",
    "\n",
    "cWETH_igraph_ec = eigenvector_manual(G_cWETH_igraph)\n",
    "comp_igraph_ec = eigenvector_manual(G_comp_igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_manual(graph):\n",
    "    n = graph.vcount()\n",
    "    \n",
    "    W_sparse = graph.get_adjacency_sparse(attribute='weight').tocsc()\n",
    "    W13 = W_sparse.power(1/3)\n",
    "    W13_T = W13.transpose()\n",
    "    \n",
    "    W_sum = W13 + W13_T\n",
    "    W_sum.data **= 3\n",
    "    numerator = np.array(W_sum.sum(axis=1)).flatten()\n",
    "    \n",
    "    d_out = np.array(graph.outdegree())\n",
    "    d_in = np.array(graph.indegree())\n",
    "    d_tot = d_out + d_in\n",
    "    \n",
    "    A_sparse = graph.get_adjacency_sparse().tocsc()\n",
    "    mutual_edges = np.array(A_sparse.multiply(A_sparse.T).sum(axis=1)).flatten()\n",
    "    \n",
    "    denominator = 2 * (d_tot * (d_tot - 1) - 2 * mutual_edges)\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        C = np.where(denominator > 0, numerator / denominator, 0)\n",
    "    \n",
    "    return {graph.vs[i][\"name\"]: C[i] for i in range(n)}\n",
    "\n",
    "cWETH_igraph_cc = clustering_manual(G_cWETH_igraph)\n",
    "comp_igraph_cc = clustering_manual(G_comp_igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-hop neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_hop_weight_sum_ig(G):\n",
    "    weight_sums = [0] * G.vcount()\n",
    "\n",
    "    for node in range(G.vcount()):\n",
    "        visited = set()\n",
    "        total_weight = 0\n",
    "\n",
    "        neighbors = set(G.neighborhood(node, order=1, mode='ALL'))\n",
    "\n",
    "        for n in neighbors:\n",
    "            for e in G.incident(n, mode='ALL'):\n",
    "                if e not in visited:\n",
    "                    total_weight += G.es[e]['weight']\n",
    "                    visited.add(e)\n",
    "\n",
    "        weight_sums[node] = total_weight\n",
    "\n",
    "    return {G.vs[i][\"name\"]: weight_sums[i] for i in range(G.vcount())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_igraph_2_hop_weights = two_hop_weight_sum_ig(G_comp_igraph)\n",
    "\n",
    "with open('centrality_scores/COMP_2_hop_weights.json', 'w') as f:\n",
    "    json.dump(comp_igraph_2_hop_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('centrality_scores/COMP_2_hop_weights.json', 'r', encoding='utf8') as f:\n",
    "    comp_igraph_2_hop_weights = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burstiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_burstiness(group):\n",
    "    if len(group) < 2:\n",
    "        return None\n",
    "    \n",
    "    sigma = group['iet'].std()\n",
    "    mean = group['iet'].mean()\n",
    "    \n",
    "    return sigma / mean if mean > 0 else 0\n",
    "\n",
    "def construct_burstiness_dictionaries(df):\n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].astype('int64'), unit='s')\n",
    "    \n",
    "    # df_sorted_to = df.sort_values(by=['to', 'timestamp'])\n",
    "    # df_sorted_to['iet'] = df_sorted_to.groupby('to')['timestamp'].diff().dt.total_seconds()\n",
    "    # burstiness_to_df = df_sorted_to.dropna().groupby('to').apply(compute_burstiness).reset_index(name='burstiness_in')\n",
    "    # burstiness_to = burstiness_to_df.dropna().set_index('to')['burstiness_in'].to_dict()\n",
    "\n",
    "    # df_sorted_from = df.sort_values(by=['from', 'timestamp'])\n",
    "    # df_sorted_from['iet'] = df_sorted_from.groupby('from')['timestamp'].diff().dt.total_seconds()\n",
    "    # burstiness_from_df = df_sorted_from.dropna().groupby('from').apply(compute_burstiness).reset_index(name='burstiness_out')\n",
    "    # burstiness_from = burstiness_from_df.dropna().set_index('from')['burstiness_out'].to_dict()\n",
    "\n",
    "    df_combined = df.melt(id_vars=['timestamp'], value_vars=['from', 'to'], var_name='direction', value_name='address')\n",
    "    df_combined = df_combined.sort_values(by=['address', 'timestamp'])\n",
    "    df_combined['iet'] = df_combined.groupby('address')['timestamp'].diff().dt.total_seconds()\n",
    "    grouped = df_combined.dropna().groupby('address')\n",
    "    burstiness_total_df = grouped[['iet']].apply(compute_burstiness).reset_index(name='burstiness_total')\n",
    "    burstiness_total = burstiness_total_df.dropna().set_index('address')['burstiness_total'].to_dict()\n",
    "    \n",
    "    return burstiness_total #, burstiness_to, burstiness_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_burstiness_to, comp_burstiness_from, comp_burstiness_total = construct_burstiness_dictionaries(comp_df)\n",
    "# cWETH_burstiness_to, cWETH_burstiness_from, cWETH_burstiness_total = construct_burstiness_dictionaries(cToken_df)\n",
    "\n",
    "comp_burstiness_total = construct_burstiness_dictionaries(comp_df)\n",
    "cWETH_burstiness_total = construct_burstiness_dictionaries(cToken_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing only the overlapping nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_COMP_transfers = comp_df.melt(value_vars=['from', 'to'], id_vars=['value'], value_name='address')\n",
    "average_COMP_tokens = all_COMP_transfers.groupby('address')['value'].mean().to_dict()\n",
    "COMP_transfer_counts = all_COMP_transfers['address'].value_counts().to_dict()\n",
    "\n",
    "all_cToken_transfers = cToken_df.melt(value_vars=['from', 'to'], id_vars=['value'], value_name='address')\n",
    "average_cWETH_tokens = all_cToken_transfers.groupby('address')['value'].mean().to_dict()\n",
    "cWETH_transfer_counts = all_cToken_transfers['address'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_COMP_users = pd.concat([comp_df[['from', 'value']].rename(columns={'from': 'user'}), comp_df[['to', 'value']].rename(columns={'to': 'user'})])\n",
    "\n",
    "avg_all_COMP_transfers = all_COMP_users.groupby('user')['value'].mean().to_dict()\n",
    "avg_outgoing_COMP = comp_df.groupby('from')['value'].mean().to_dict()\n",
    "avg_incoming_COMP = comp_df.groupby('to')['value'].mean().to_dict()\n",
    "\n",
    "\n",
    "all_cWETH_users = pd.concat([cToken_df[['from', 'value']].rename(columns={'from': 'user'}), cToken_df[['to', 'value']].rename(columns={'to': 'user'})])\n",
    "\n",
    "avg_all_cWETH_transfers = all_cWETH_users.groupby('user')['value'].mean().to_dict()\n",
    "avg_outgoing_cWETH = cToken_df.groupby('from')['value'].mean().to_dict()\n",
    "avg_incoming_cWETH = cToken_df.groupby('to')['value'].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_from_COMP_transferred_mapping = avg_outgoing_COMP\n",
    "average_to_COMP_transferred_mapping = avg_incoming_COMP\n",
    "average_from_cWETH_transferred_mapping = avg_outgoing_cWETH\n",
    "average_to_cWETH_transferred_mapping = avg_incoming_cWETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_COMP_transferred_mapping = {}\n",
    "# from_COMP_transferred_mapping = {}\n",
    "# to_COMP_transferred_mapping = {}\n",
    "average_COMP_transferred_mapping = avg_all_COMP_transfers\n",
    "# average_from_COMP_transferred_mapping = avg_outgoing_COMP\n",
    "# average_to_COMP_transferred_mapping = avg_incoming_COMP\n",
    "number_of_COMP_transfers_mapping = COMP_transfer_counts\n",
    "\n",
    "proposals_mapping = {}\n",
    "votes_casted_mapping = {}\n",
    "# delegation_counts_mapping = {}\n",
    "from_delegations_mapping = {}\n",
    "to_delegations_mapping = {}\n",
    "average_vote_weights_mapping = {}\n",
    "\n",
    "total_cWETH_transferred_mapping = {}\n",
    "# from_cWETH_transferred_mapping = {}\n",
    "# to_cWETH_transferred_mapping = {}\n",
    "average_cWETH_transferred_mapping = avg_all_cWETH_transfers\n",
    "# average_from_cWETH_transferred_mapping = avg_outgoing_cWETH\n",
    "# average_to_cWETH_transferred_mapping = avg_incoming_cWETH\n",
    "number_of_cWETH_transfers_mapping = cWETH_transfer_counts\n",
    "\n",
    "for _, row in comp_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if from_address in total_COMP_transferred_mapping:\n",
    "        total_COMP_transferred_mapping[from_address] += value\n",
    "    else:\n",
    "        total_COMP_transferred_mapping[from_address] = value\n",
    "        \n",
    "    # if from_address in from_COMP_transferred_mapping:\n",
    "    #     from_COMP_transferred_mapping[from_address] += value\n",
    "    # else:\n",
    "    #     from_COMP_transferred_mapping[from_address] = value\n",
    "    \n",
    "    if to_address in total_COMP_transferred_mapping:\n",
    "        total_COMP_transferred_mapping[to_address] += value\n",
    "    else:\n",
    "        total_COMP_transferred_mapping[to_address] = value\n",
    "        \n",
    "    # if to_address in to_COMP_transferred_mapping:\n",
    "    #     to_COMP_transferred_mapping[to_address] += value\n",
    "    # else:\n",
    "    #     to_COMP_transferred_mapping[to_address] = value\n",
    "        \n",
    "        \n",
    "for user, average in average_COMP_tokens.items():\n",
    "    if user in average_COMP_transferred_mapping:\n",
    "        average_COMP_transferred_mapping[user] = average\n",
    "        \n",
    "# for user, count in COMP_transfer_counts.items():\n",
    "#     if user in number_of_COMP_transfers_mapping:\n",
    "#         number_of_COMP_transfers_mapping[user] = count\n",
    "\n",
    "for proposer, count in proposal_counts.items():\n",
    "    proposals_mapping[proposer] = count\n",
    "\n",
    "for voter, count in vote_counts.items():\n",
    "    votes_casted_mapping[voter] = count\n",
    "\n",
    "for delegator, count in from_delegations.items():\n",
    "    from_delegations_mapping[delegator] = count\n",
    "\n",
    "for delegate, count in to_delegations.items():\n",
    "    to_delegations_mapping[delegate] = count\n",
    "\n",
    "for voter, weight in vote_weights.items():\n",
    "    average_vote_weights_mapping[voter] = weight\n",
    "\n",
    "\n",
    "for _, row in cToken_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if from_address in total_cWETH_transferred_mapping:\n",
    "        total_cWETH_transferred_mapping[from_address] += value\n",
    "    else:\n",
    "        total_cWETH_transferred_mapping[from_address] = value\n",
    "        \n",
    "    # if from_address in from_cWETH_transferred_mapping:\n",
    "    #     from_cWETH_transferred_mapping[from_address] += value\n",
    "    # else:\n",
    "    #     from_cWETH_transferred_mapping[from_address] = value\n",
    "        \n",
    "    if to_address in total_cWETH_transferred_mapping:\n",
    "        total_cWETH_transferred_mapping[to_address] += value\n",
    "    else:\n",
    "        total_cWETH_transferred_mapping[to_address] = value\n",
    "        \n",
    "    # if to_address in to_cWETH_transferred_mapping:\n",
    "    #     to_cWETH_transferred_mapping[to_address] += value\n",
    "    # else:\n",
    "    #     to_cWETH_transferred_mapping[to_address] = value\n",
    "\n",
    "for user, average in average_cWETH_tokens.items():\n",
    "    if user in average_cWETH_transferred_mapping:\n",
    "        average_cWETH_transferred_mapping[user] = average\n",
    "        \n",
    "# for user, count in aWETH_transfer_counts.items():\n",
    "#     if user in number_of_aWETH_transfers_mapping:\n",
    "#         number_of_aWETH_transfers_mapping[user] = count\n",
    "\n",
    "# addresses = sorted(average_COMP_transferred_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlapping_correlations(x_name, x_map, y_name, y_map, results):\n",
    "    valid_addresses = [address for address in x_map.keys() & y_map.keys()]\n",
    "    \n",
    "    scores_x = [x_map[address] for address in valid_addresses]\n",
    "    scores_y = [y_map[address] for address in valid_addresses]\n",
    "    \n",
    "    spearman = spearmanr(scores_x, scores_y)\n",
    "    kendall = kendalltau(scores_x, scores_y)\n",
    "    \n",
    "    results.append({\n",
    "        'x_name': x_name,\n",
    "        'y_name': y_name,\n",
    "        'spearman': spearman,\n",
    "        'kendall': kendall,\n",
    "        'valid_addresses': valid_addresses\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING THE SCORES AND METRICS WITH OWN COMPUTATIONS INSTEAD OF BUILT-IN FUNCTIONS\n",
    "COMP_mappings = [\n",
    "    ('COMP In-Degree Centrality', comp_igraph_in_dc),\n",
    "    ('COMP Out-Degree Centrality', comp_igraph_out_dc),\n",
    "    ('COMP Eigenvector Centrality', comp_igraph_ec),\n",
    "    ('COMP Clustering Coefficient', comp_igraph_cc),\n",
    "    # ('Total COMP transferred', total_COMP_transferred_mapping),\n",
    "    # ('From COMP', from_COMP_transferred_mapping),\n",
    "    # ('To COMP', to_COMP_transferred_mapping),\n",
    "    ('COMP Avg. per Transfer', average_COMP_transferred_mapping),\n",
    "    # ('Average COMP sent per transfer', average_from_COMP_transferred_mapping),\n",
    "    # ('Average COMP received per transfer', average_to_COMP_transferred_mapping),\n",
    "    ('COMP Transfer Count', number_of_COMP_transfers_mapping),\n",
    "    ('COMP 2-Hop Weight Sum', comp_igraph_2_hop_weights),\n",
    "    ('COMP Burstiness', comp_burstiness_total)\n",
    "]\n",
    "\n",
    "gov_mappings = [\n",
    "    # ('Proposals Made', proposals_mapping),\n",
    "    ('Votes Cast', votes_casted_mapping),\n",
    "    # ('Delegations Given', from_delegations_mapping),\n",
    "    ('Delegations Reveived', to_delegations_mapping),\n",
    "    ('Avg. Vote Weight', average_vote_weights_mapping)\n",
    "]    \n",
    "\n",
    "cWETH_mappings = [\n",
    "    ('cWETH In-Degree Centrality', cWETH_igraph_in_dc),\n",
    "    ('cWETH Out-Degree Centrality', cWETH_igraph_out_dc),\n",
    "    # ('cWETH Eigenvector Centrality', cWETH_igraph_ec),\n",
    "    # ('cWETH Clustering Coefficient', cWETH_igraph_cc),\n",
    "    # ('Total cWETH transferred', total_cWETH_transferred_mapping),\n",
    "    # ('From cWETH', from_cWETH_transferred_mapping),\n",
    "    # ('To cWETH', to_cWETH_transferred_mapping),\n",
    "    ('cWETH Avg. per Transfer', average_cWETH_transferred_mapping),\n",
    "    # ('Average cWETH sent per transfer', average_from_cWETH_transferred_mapping),\n",
    "    # ('Average cWETH received per transfer', average_to_cWETH_transferred_mapping),\n",
    "    ('cWETH Transfer Count', number_of_cWETH_transfers_mapping),\n",
    "    ('cWETH Burstiness', cWETH_burstiness_total)\n",
    "]\n",
    "\n",
    "mapping_dict = dict(COMP_mappings) | dict(gov_mappings) | dict(cWETH_mappings)\n",
    "\n",
    "# nonzero_results = []\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in itertools.product(COMP_mappings, gov_mappings):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in itertools.product(COMP_mappings, cWETH_mappings):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in itertools.product(cWETH_mappings, gov_mappings):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# #######\n",
    "\n",
    "comp_gov_results = []\n",
    "comp_cWETH_results = []\n",
    "cWETH_gov_results = []\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(COMP_mappings, gov_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, comp_gov_results)\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(COMP_mappings, cWETH_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, comp_cWETH_results)\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(cWETH_mappings, gov_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, cWETH_gov_results)\n",
    "    \n",
    "\n",
    "# threshold = 0.75\n",
    "\n",
    "# for res in nonzero_results:\n",
    "#     kendall = res['kendall']\n",
    "    \n",
    "#     if kendall.statistic > threshold:\n",
    "#         valid_addresses = res['valid_addresses']\n",
    "#         percentage_of_addresses = (len(valid_addresses)/len(G_comp_nodes)) * 100\n",
    "        \n",
    "#         print(f'{res[\"x_name\"]} vs {res[\"y_name\"]} -> Kendall: {kendall.statistic:.3f}, number of addresses: {len(valid_addresses)}, percentage of addresses: {percentage_of_addresses:.2f}%')\n",
    "#         print()\n",
    "        \n",
    "#         x_map = mapping_dict.get(res['x_name'])\n",
    "#         y_map = mapping_dict.get(res['y_name'])\n",
    "\n",
    "#         x_vals = [x_map[address] for address in valid_addresses]\n",
    "#         y_vals = [y_map[address] for address in valid_addresses]\n",
    "        \n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "#         axes[0].scatter(x_vals, y_vals, alpha=0.6)\n",
    "#         axes[0].set_xlabel(res['x_name'])\n",
    "#         axes[0].set_ylabel(res['y_name'])\n",
    "#         axes[0].set_title('Original scale')\n",
    "\n",
    "#         # log_x = np.log(x_vals)\n",
    "#         # log_y = np.log(y_vals)\n",
    "#         # sns.regplot(x=log_x, y=log_y, ax=axes[1])\n",
    "#         # axes[1].set_xlabel(f'Log({res[\"x_name\"]})')\n",
    "#         # axes[1].set_ylabel(f'Log({res[\"y_name\"]})')\n",
    "#         # axes[1].set_title('Log-Log Scale')\n",
    "#         ####\n",
    "#         axes[1].scatter(x_vals, y_vals, alpha=0.6)\n",
    "#         axes[1].set_xscale('log')\n",
    "#         axes[1].set_yscale('log')\n",
    "#         axes[1].set_xlabel(res['x_name'])\n",
    "#         axes[1].set_ylabel(res['y_name'])\n",
    "#         axes[1].set_title('Log-scaled axes')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/comp/SI_5_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_5_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_2_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_2_extracted = pickle.load(f)\n",
    "\n",
    "with open('data/SI/comp/SI_1_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_1_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_05_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_05_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_01_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_01_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_001_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_001_extracted = pickle.load(f)\n",
    "    \n",
    "SI_001_mapping = {i['seed']: i['avg_infected'] for i in SI_001_extracted}\n",
    "SI_01_mapping = {i['seed']: i['avg_infected'] for i in SI_01_extracted}\n",
    "SI_05_mapping = {i['seed']: i['avg_infected'] for i in SI_05_extracted}\n",
    "SI_1_mapping = {i['seed']: i['avg_infected'] for i in SI_1_extracted}\n",
    "SI_2_mapping = {i['seed']: i['avg_infected'] for i in SI_2_extracted}\n",
    "SI_5_mapping = {i['seed']: i['avg_infected'] for i in SI_5_extracted}\n",
    "\n",
    "SI_mappings = [\n",
    "    ('SI 0.001', SI_001_mapping),\n",
    "    ('SI 0.01', SI_01_mapping),\n",
    "    ('SI 0.05', SI_05_mapping),\n",
    "    ('SI 1', SI_1_mapping),\n",
    "    ('SI 2', SI_2_mapping),\n",
    "    ('SI 5', SI_5_mapping)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_si_results = []\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(gov_mappings, SI_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, gov_si_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING CORRELATIONS WITHIN THE DATASETS\n",
    "COMP_mappings = [\n",
    "    ('COMP In-Degree Centrality', comp_igraph_in_dc),\n",
    "    ('COMP Out-Degree Centrality', comp_igraph_out_dc),\n",
    "    ('COMP Eigenvector Centrality', comp_igraph_ec),\n",
    "    ('COMP Clustering Coefficient', comp_igraph_cc),\n",
    "    # ('Total COMP transferred', total_COMP_transferred_mapping),\n",
    "    # ('From COMP', from_COMP_transferred_mapping),\n",
    "    # ('To COMP', to_COMP_transferred_mapping),\n",
    "    ('COMP Avg. per Transfer', average_COMP_transferred_mapping),\n",
    "    # ('Average from COMP transferred', average_from_COMP_transferred_mapping),\n",
    "    # ('Average to COMP transferred', average_to_COMP_transferred_mapping),\n",
    "    ('COMP Transfer Count', number_of_COMP_transfers_mapping),\n",
    "    ('COMP 2-Hop Weight Sum', comp_igraph_2_hop_weights),\n",
    "    ('COMP Burstiness', comp_burstiness_total)\n",
    "]\n",
    "\n",
    "gov_mappings = [\n",
    "    ('Votes Cast', votes_casted_mapping),\n",
    "    # ('From delegations', from_delegations_mapping),\n",
    "    ('Governance Delegations Reveived', to_delegations_mapping),\n",
    "    ('Avg. Vote Weight', average_vote_weights_mapping)\n",
    "]    \n",
    "\n",
    "cWETH_mappings = [\n",
    "    ('cWETH In-Degree Centrality', cWETH_igraph_in_dc),\n",
    "    ('cWETH Out-Degree Centrality', cWETH_igraph_out_dc),\n",
    "    # ('cWETH Eigenvector Centrality', cWETH_igraph_ec),\n",
    "    # ('cWETH Clustering Coefficient', cWETH_igraph_cc),\n",
    "    # ('Total cWETH transferred', total_cWETH_transferred_mapping),\n",
    "    # ('From cWETH', from_cWETH_transferred_mapping),\n",
    "    # ('To cWETH', to_cWETH_transferred_mapping),\n",
    "    ('cWETH Avg. per Transfer', average_cWETH_transferred_mapping),\n",
    "    # ('Average from cWETH transferred', average_from_cWETH_transferred_mapping),\n",
    "    # ('Average to cWETH transferred', average_to_cWETH_transferred_mapping),\n",
    "    ('cWETH Transfer Count', number_of_cWETH_transfers_mapping),\n",
    "    ('cWETH Burstiness', cWETH_burstiness_total)\n",
    "]\n",
    "\n",
    "mapping_dict = dict(COMP_mappings) | dict(gov_mappings) | dict(cWETH_mappings)\n",
    "\n",
    "# nonzero_results = []\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(COMP_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(cWETH_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(gov_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "#####\n",
    "\n",
    "comp_results = []\n",
    "cWETH_results = []\n",
    "gov_results = []\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in list(combinations(COMP_mappings, 2)):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, comp_results)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(cWETH_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, cWETH_results)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(gov_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, gov_results)\n",
    "\n",
    "# threshold = 0.7\n",
    "\n",
    "# for res in nonzero_results:\n",
    "#     spearman = res['spearman']\n",
    "#     kendall = res['kendall']\n",
    "    \n",
    "#     if spearman.statistic > threshold or kendall.statistic > threshold:\n",
    "#     # if spearman.statistic < threshold or kendall.statistic < threshold:\n",
    "#         valid_addresses = res['valid_addresses']\n",
    "        \n",
    "#         print(f'{res[\"x_name\"]} vs {res[\"y_name\"]} -> Spearman: {spearman.statistic:.3f}, Kendall: {kendall.statistic:.3f}, number of addresses: {len(valid_addresses)}')\n",
    "#         # print(f'Spearman p-value: {spearman.pvalue:.3f}, Kendall p-value: {kendall.pvalue:.3f}')\n",
    "#         # print()\n",
    "        \n",
    "#         # x_map = mapping_dict.get(res['x_name'])\n",
    "#         # y_map = mapping_dict.get(res['y_name'])\n",
    "        \n",
    "#         # plt.figure(figsize=(10, 6))\n",
    "#         # plt.scatter([x_map[address] for address in valid_addresses], [y_map[address] for address in valid_addresses], alpha=0.6)\n",
    "#         # plt.xlabel(res['x_name'])\n",
    "#         # plt.ylabel(res['y_name'])\n",
    "#         # plt.show()\n",
    "        \n",
    "#         # log_x = np.log1p([x_map[address] for address in valid_addresses])\n",
    "#         # log_y = np.log1p([y_map[address] for address in valid_addresses])\n",
    "\n",
    "#         # sns.regplot(x=log_x, y=log_y)\n",
    "#         # plt.xlabel(f'Log({res[\"x_name\"]})')\n",
    "#         # plt.ylabel(f'Log({res[\"y_name\"]})')\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(results):\n",
    "    def unique_ordered(items):\n",
    "        seen = set()\n",
    "        ordered = []\n",
    "        for item in items:\n",
    "            if item not in seen:\n",
    "                seen.add(item)\n",
    "                ordered.append(item)\n",
    "        return ordered\n",
    "\n",
    "    x_datasets = unique_ordered([entry['x_name'] for entry in results])\n",
    "    y_datasets = unique_ordered([entry['y_name'] for entry in results])\n",
    "\n",
    "    # spearman_corr_matrix = np.zeros((len(x_datasets), len(y_datasets)))\n",
    "    kendall_corr_matrix = np.zeros((len(x_datasets), len(y_datasets)))\n",
    "\n",
    "    for entry in results:\n",
    "        x_idx = x_datasets.index(entry['x_name'])\n",
    "        y_idx = y_datasets.index(entry['y_name'])\n",
    "        # spearman_corr_matrix[x_idx, y_idx] = entry['spearman'].statistic\n",
    "        kendall_corr_matrix[x_idx, y_idx] = entry['kendall'].statistic\n",
    "\n",
    "    # spearman_corr_df = pd.DataFrame(spearman_corr_matrix, index=x_datasets, columns=y_datasets)\n",
    "    kendall_corr_df = pd.DataFrame(kendall_corr_matrix, index=x_datasets, columns=y_datasets)\n",
    "\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(spearman_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Spearman correlation\")\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(kendall_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Kendall correlation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(comp_gov_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(comp_cWETH_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(cWETH_gov_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(gov_si_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lower_triangle_heatmap(results):\n",
    "    def unique_ordered(items):\n",
    "        seen = set()\n",
    "        ordered = []\n",
    "        for item in items:\n",
    "            if item not in seen:\n",
    "                seen.add(item)\n",
    "                ordered.append(item)\n",
    "        return ordered\n",
    "\n",
    "    feature_names = unique_ordered(\n",
    "        [entry['x_name'] for entry in results] + [entry['y_name'] for entry in results]\n",
    "    )\n",
    "\n",
    "    n = len(feature_names)\n",
    "    spearman_corr_matrix = np.zeros((n, n))\n",
    "    kendall_corr_matrix = np.zeros((n, n))\n",
    "\n",
    "    for entry in results:\n",
    "        x_idx = feature_names.index(entry['x_name'])\n",
    "        y_idx = feature_names.index(entry['y_name'])\n",
    "        spearman = entry['spearman'].statistic\n",
    "        kendall = entry['kendall'].statistic\n",
    "\n",
    "        spearman_corr_matrix[x_idx, y_idx] = spearman\n",
    "        spearman_corr_matrix[y_idx, x_idx] = spearman\n",
    "        kendall_corr_matrix[x_idx, y_idx] = kendall\n",
    "        kendall_corr_matrix[y_idx, x_idx] = kendall\n",
    "\n",
    "    mask = np.triu(np.ones_like(spearman_corr_matrix, dtype=bool), k=0)\n",
    "\n",
    "    trimmed_feature_names_y = feature_names[1:]\n",
    "    trimmed_feature_names_x = feature_names[:-1]\n",
    "    spearman_corr_trimmed = spearman_corr_matrix[1:, :-1]\n",
    "    kendall_corr_trimmed = kendall_corr_matrix[1:, :-1]\n",
    "    mask_trimmed = mask[1:, :-1]\n",
    "\n",
    "    spearman_corr_df = pd.DataFrame(spearman_corr_trimmed, index=trimmed_feature_names_y, columns=trimmed_feature_names_x)\n",
    "    kendall_corr_df = pd.DataFrame(kendall_corr_trimmed, index=trimmed_feature_names_y, columns=trimmed_feature_names_x)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(spearman_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1, square=True)\n",
    "    sns.heatmap(spearman_corr_df, mask=mask_trimmed, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.title(\"Spearman correlation\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(kendall_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1, square=True)\n",
    "    sns.heatmap(kendall_corr_df, mask=mask_trimmed, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Kendall correlation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lower_triangle_heatmap(comp_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset comparison per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cWETH_users = (set(cToken_df['from'].unique()) | set(cToken_df['to'].unique()))\n",
    "COMP_users = (set(comp_df['from'].unique()) | set(comp_df['to'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cWETH_users_in_COMP = cWETH_users & COMP_users\n",
    "governance_users_in_COMP = (proposers | voters | delegators) & COMP_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset_vs_rest(feature_mapping, feature_name, use_kde=True):\n",
    "    subsets = {\n",
    "        'Financial users': cWETH_users_in_COMP,\n",
    "        'Governance users': governance_users_in_COMP\n",
    "    }\n",
    "\n",
    "    for subset_name, subset in subsets.items():\n",
    "        in_subset = [feature_mapping[node] for node in subset if node in feature_mapping]\n",
    "        out_subset = [feature_mapping[node] for node in COMP_users - subset if node in feature_mapping]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        if use_kde:\n",
    "            sns.kdeplot(in_subset, fill=True, alpha=0.6, ax=axes[0])\n",
    "        else:\n",
    "            axes[0].hist(in_subset, bins=30, alpha=0.7)\n",
    "        axes[0].set_title(f'{subset_name}')\n",
    "        axes[0].set_xlabel(feature_name)\n",
    "        axes[0].set_ylabel('Density' if use_kde else 'Frequency')\n",
    "\n",
    "        if use_kde:\n",
    "            sns.kdeplot(out_subset, fill=True, alpha=0.6, ax=axes[1], color='gray')\n",
    "        else:\n",
    "            axes[1].hist(out_subset, bins=30, alpha=0.7, color='gray')\n",
    "        axes[1].set_title(f'Not {subset_name}')\n",
    "        axes[1].set_xlabel(feature_name)\n",
    "        axes[1].set_ylabel('Density' if use_kde else 'Frequency')\n",
    "\n",
    "        fig.suptitle(f'{feature_name} â€” {subset_name} vs Rest', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset_vs_rest(feature_mapping, feature_name):\n",
    "    EOA_gov_subset = [feature_mapping[node] for node in feature_mapping if node in EOA and node in governance_users_in_COMP and node not in CEX]\n",
    "    EOA_non_gov_subset = [feature_mapping[node] for node in feature_mapping if node in EOA and node not in governance_users_in_COMP and node not in CEX]\n",
    "    # CA_subset = [feature_mapping[node] for node in feature_mapping if node in CA]\n",
    "    # CEX_subset = [feature_mapping[node] for node in feature_mapping if node in CEX]\n",
    "    CA_CEX_subset = [feature_mapping[node] for node in feature_mapping if node in CEX or node in CA]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    # fig, axes = plt.subplots(1, 4, figsize=(24, 4))\n",
    "\n",
    "    axes[0].hist(EOA_gov_subset, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "    axes[0].set_title('EOA governance')\n",
    "    axes[0].set_xlabel('Feature score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    axes[1].hist(EOA_non_gov_subset, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "    axes[1].set_title('EOA non-governance')\n",
    "    axes[1].set_xlabel('Feature score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[2].hist(CA_CEX_subset, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "    axes[2].set_title('Infrastructural')\n",
    "    axes[2].set_xlabel('Feature score')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    # axes[2].hist(CEX_subset, bins=30, alpha=0.7, color='green', log=True)\n",
    "    # axes[2].set_title(f'CEX')\n",
    "    # axes[2].set_xlabel(feature_name)\n",
    "    # axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    # axes[3].hist(CA_subset, bins=30, alpha=0.7, color='orange', log=True)\n",
    "    # axes[3].set_title(f'CA')\n",
    "    # axes[3].set_xlabel(feature_name)\n",
    "    # axes[3].set_ylabel('Frequency')\n",
    "\n",
    "    # fig.suptitle(f'{feature_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "temp_mapping = [\n",
    "    ('COMP In-Degree Centrality', comp_igraph_in_dc),\n",
    "    ('COMP Out-Degree Centrality', comp_igraph_out_dc),\n",
    "    ('COMP Eigenvector Centrality', comp_igraph_ec),\n",
    "    ('COMP Clustering Coefficient', comp_igraph_cc),\n",
    "    ('COMP Avg. per Transfer', average_COMP_transferred_mapping),\n",
    "    # ('Total transferred', total_AAVE_transferred_mapping),\n",
    "    ('COMP Transfer Count', number_of_COMP_transfers_mapping),\n",
    "    ('COMP 2-Hop Weight Sum', comp_igraph_2_hop_weights),\n",
    "    ('COMP Burstiness', comp_burstiness_total)\n",
    "]\n",
    "\n",
    "for feature_name, feature_mapping in temp_mapping:\n",
    "    plot_subset_vs_rest(feature_mapping, feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SI comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/comp/SI_5_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_5_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_2_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_2_extracted = pickle.load(f)\n",
    "\n",
    "with open('data/SI/comp/SI_1_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_1_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_05_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_05_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_01_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_01_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_001_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_001_extracted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_si_correlations(comp_mappings, si_data_list, si_labels):\n",
    "    spearman_results = []\n",
    "    kendall_results = []\n",
    "    pearson_results = []\n",
    "\n",
    "    for si_data, si_label in zip(si_data_list, si_labels):\n",
    "        si_scores = {entry['seed']: entry['avg_infected'] for entry in si_data if entry['seed'] in all_proposers_voters_delegators_in_G_comp}\n",
    "    \n",
    "        for feature_name, feature_mapping in comp_mappings:\n",
    "            valid_addresses = feature_mapping.keys() & si_scores.keys()\n",
    "\n",
    "            if valid_addresses:\n",
    "                feature_values = [feature_mapping[address] for address in valid_addresses]\n",
    "                si_values = [si_scores[address] for address in valid_addresses]\n",
    "\n",
    "                # spearman_corr = spearmanr(feature_values, si_values).statistic\n",
    "                kendall_corr = kendalltau(feature_values, si_values).statistic\n",
    "                # pearson_corr = pearsonr(feature_values, si_values).statistic\n",
    "\n",
    "                # spearman_results.append((feature_name, si_label, spearman_corr))\n",
    "                kendall_results.append((feature_name, si_label, kendall_corr))\n",
    "                # pearson_results.append((feature_name, si_label, pearson_corr))\n",
    "\n",
    "    return spearman_results, kendall_results, pearson_results\n",
    "\n",
    "def create_correlation_heatmap(correlation_results, comp_mappings, si_labels, title):\n",
    "    feature_names = [feature_name for feature_name, _ in comp_mappings]\n",
    "    correlation_matrix = np.zeros((len(feature_names), len(si_labels)))\n",
    "\n",
    "    for feature_name, si_label, correlation in correlation_results:\n",
    "        feature_idx = feature_names.index(feature_name)\n",
    "        si_idx = si_labels.index(si_label)\n",
    "        correlation_matrix[feature_idx, si_idx] = correlation\n",
    "\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, index=feature_names, columns=si_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(title)\n",
    "    # plt.xlabel('SI configurations')\n",
    "    # plt.ylabel('COMP features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "si_data_list = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "si_labels = ['Î²=0.01', 'Î²=0.1', 'Î²=0.5', 'Î²=1.0', 'Î²=2.0']#, 'Î²=5.0']\n",
    "\n",
    "spearman_results, kendall_results, pearson_results = compute_si_correlations(COMP_mappings, si_data_list, si_labels)\n",
    "\n",
    "# create_correlation_heatmap(spearman_results, COMP_mappings, si_labels, \"Spearman correlation\")\n",
    "create_correlation_heatmap(kendall_results, COMP_mappings, si_labels, \"Kendall correlation\")\n",
    "# create_correlation_heatmap(pearson_results, COMP_mappings, si_labels, \"Pearson correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_data_list = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "si_labels = ['Î²=0.01', 'Î²=0.1', 'Î²=0.5', 'Î²=1.0', 'Î²=2.0']#, 'Î²=5.0']\n",
    "\n",
    "def compute_gov_si_correlations(gov_mappings, si_data_list, si_labels):\n",
    "    kendall_results = []\n",
    "    for si_data, si_label in zip(si_data_list, si_labels):\n",
    "        si_scores = {entry['seed']: entry['avg_infected'] for entry in si_data}\n",
    "        for feature_name, feature_mapping in gov_mappings:\n",
    "            valid_addresses = feature_mapping.keys() & si_scores.keys()\n",
    "            if valid_addresses:\n",
    "                feature_values = [feature_mapping[address] for address in valid_addresses]\n",
    "                si_values = [si_scores[address] for address in valid_addresses]\n",
    "                kendall_corr = kendalltau(feature_values, si_values).statistic\n",
    "                kendall_results.append((feature_name, si_label, kendall_corr))\n",
    "    return kendall_results\n",
    "\n",
    "def create_gov_si_heatmap(correlation_results, gov_mappings, si_labels, title):\n",
    "    feature_names = [feature_name for feature_name, _ in gov_mappings]\n",
    "    correlation_matrix = np.zeros((len(feature_names), len(si_labels)))\n",
    "    for feature_name, si_label, correlation in correlation_results:\n",
    "        feature_idx = feature_names.index(feature_name)\n",
    "        si_idx = si_labels.index(si_label)\n",
    "        correlation_matrix[feature_idx, si_idx] = correlation\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, index=feature_names, columns=si_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.yticks(rotation=0)\n",
    "    # plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "gov_si_kendall_results = compute_gov_si_correlations(gov_mappings, si_data_list, si_labels)\n",
    "create_gov_si_heatmap(gov_si_kendall_results, gov_mappings, si_labels, \"Kendall correlation (Governance vs SI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_2_mapping = {entry['seed']: entry['avg_infected'] for entry in SI_2_extracted}\n",
    "\n",
    "valid_addresses = set(SI_2_mapping.keys()) & set(average_vote_weights_mapping.keys())\n",
    "\n",
    "si2_vals = [SI_2_mapping[address] for address in valid_addresses]\n",
    "votes_vals = [average_vote_weights_mapping[address] for address in valid_addresses]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(votes_vals, si2_vals, alpha=0.6)\n",
    "axes[0].set_xlabel('Avg. Vote Weight')\n",
    "axes[0].set_ylabel('Avg. Infected Nodes (SI Î²=2.0)')\n",
    "axes[0].set_title('Normal scale')\n",
    "\n",
    "axes[1].scatter(votes_vals, si2_vals, alpha=0.6)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel('Avg. Vote Weight (log scale)')\n",
    "axes[1].set_ylabel('Avg. Infected Nodes (SI Î²=2.0, log scale)')\n",
    "axes[1].set_title('Log-Log Scale')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def compute_ndcg(si_scores, si_label, other_feature_scores, other_feature_label, k=None):\n",
    "    si_mapping = {entry['seed']: entry['avg_infected'] for entry in si_scores}\n",
    "    common_addresses = set(si_mapping.keys()) & set(other_feature_scores.keys())\n",
    "\n",
    "    si_scores = [si_mapping[addr] for addr in common_addresses]\n",
    "    vote_weight_scores = [other_feature_scores[addr] for addr in common_addresses]\n",
    "\n",
    "    si_ranks = rankdata(si_scores, method=\"average\")\n",
    "    vote_ranks = rankdata(vote_weight_scores, method=\"average\")\n",
    "    print(si_label)\n",
    "    # ndcg = ndcg_score([vote_ranks], [si_ranks])\n",
    "    # print(f'Ranked: {ndcg}')\n",
    "    # ndcg = ndcg_score([si_ranks], [vote_ranks])\n",
    "    # print(ndcg)\n",
    "\n",
    "    ndcg = ndcg_score([vote_weight_scores], [si_scores])\n",
    "    print(f'Raw: {ndcg}')\n",
    "    # ndcg = ndcg_score([si_scores], [vote_weight_scores])\n",
    "    # print(ndcg)\n",
    "\n",
    "    # print(f\"nDCG between {si_label} and {other_feature_label}: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_data_list = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "si_labels = ['Î²=0.01', 'Î²=0.1', 'Î²=0.5', 'Î²=1.0', 'Î²=2.0']#, 'Î²=5.0']\n",
    "\n",
    "for si_data, si_label in zip(si_data_list, si_labels):\n",
    "    compute_ndcg(si_data, si_label, average_vote_weights_mapping, 'Avg. Vote Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevance_scores, k):\n",
    "    relevance_scores = np.asfarray(relevance_scores)[:k]\n",
    "    return np.sum(relevance_scores / np.log2(np.arange(2, k + 2)))\n",
    "\n",
    "def ndcg(si_outcome: list, vote_weights: dict, k: int = None):\n",
    "    seeds_with_vote = [entry for entry in si_outcome if entry['seed'] in vote_weights]\n",
    "    \n",
    "    if not seeds_with_vote:\n",
    "        return 0\n",
    "    \n",
    "    sorted_seeds = sorted(seeds_with_vote, key=lambda x: x['avg_infected'], reverse=True)\n",
    "    \n",
    "    rel_pred = [vote_weights[entry['seed']] for entry in sorted_seeds]\n",
    "    \n",
    "    ideal_sorted = sorted(seeds_with_vote, key=lambda x: vote_weights[x['seed']], reverse=True)\n",
    "    rel_ideal = [vote_weights[entry['seed']] for entry in ideal_sorted]\n",
    "\n",
    "    k = k or len(rel_pred)\n",
    "\n",
    "    dcg_val = dcg(rel_pred, k)\n",
    "    idcg_val = dcg(rel_ideal, k)\n",
    "\n",
    "    return dcg_val / idcg_val if idcg_val > 0 else 0\n",
    "\n",
    "for si_data in si_data_list:\n",
    "    print(ndcg(si_data, average_vote_weights_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cToken analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tokens = {\n",
    "    'USDC': ['cUSDCv3', 'cArbUSDC'],\n",
    "    'USDT': ['cUSDTv3', 'cPolUSDT', 'cArbUSDT'],\n",
    "    'WETH': ['cWETHv3', 'cArbWETH']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers_per_token = cToken_df.groupby('token').size()\n",
    "\n",
    "transfers_per_token = pd.Series({token: transfers_per_token.loc[names].sum() for token, names in combined_tokens.items()})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "transfers_per_token.plot(kind='bar')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Number of transfers')\n",
    "plt.title('Number of transfers per cToken')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('data/WETH_daily_USD.csv', parse_dates=['snapped_at'])\n",
    "\n",
    "price_data['date'] = pd.to_datetime(price_data['snapped_at']).dt.date\n",
    "cToken_df['date'] = pd.to_datetime(cToken_df['timestamp'], unit='s')\n",
    "cToken_df['date'] = cToken_df['date'].dt.date\n",
    "\n",
    "cToken_df2 = cToken_df.merge(price_data[['date', 'price']], on='date', how='left')\n",
    "\n",
    "tokens_to_multiply = ['cWETHv3', 'cArbWETH']\n",
    "\n",
    "cToken_df2['value_traded'] = cToken_df2.apply(\n",
    "    lambda row: row['value'] * row['price'] if row['token'] in tokens_to_multiply else row['value'], axis=1\n",
    ")\n",
    "\n",
    "transfers_over_time_per_token = cToken_df2.groupby('token')['value_traded'].sum()\n",
    "\n",
    "transfers_over_time_per_token = pd.Series({token: transfers_over_time_per_token.loc[names].sum() for token, names in combined_tokens.items()})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "transfers_over_time_per_token.plot(kind='bar')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Total value traded')\n",
    "plt.title('Total value traded over time per token')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_address = '0x0000000000000000000000000000000000000000'\n",
    "\n",
    "total_transfers_per_token = cToken_df['token'].value_counts()\n",
    "\n",
    "null_address_transfers_per_token = cToken_df[(cToken_df['from'].str.lower() == null_address) | (cToken_df['to'].str.lower() == null_address)].groupby('token').size()\n",
    "\n",
    "percentage_null_address_transfers = (null_address_transfers_per_token / total_transfers_per_token) * 100\n",
    "\n",
    "percentage_non_null_address_transfers = 100 - percentage_null_address_transfers\n",
    "\n",
    "percentage_transfers_df = pd.DataFrame({\n",
    "    'Null Address': percentage_null_address_transfers,\n",
    "    'Non-Null Address': percentage_non_null_address_transfers\n",
    "}).fillna(0)\n",
    "\n",
    "percentage_transfers_df.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Percentage of transfers')\n",
    "plt.title('Percentage of transfers from/to null and non-null addresses for each cToken')\n",
    "plt.legend(title='Transfer Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venn diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    \"COMP\": set(G_comp_nodes),\n",
    "    \"cWETH\": set(G_cToken_nodes)\n",
    "}\n",
    "\n",
    "venn(subsets, fmt=\"{size}\", cmap='plasma', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    \"Proposers\": proposers,\n",
    "    \"Voters\": voters,\n",
    "    \"Delegations\": delegators\n",
    "}\n",
    "\n",
    "venn(subsets, fmt=\"{size}\", cmap='plasma', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    \"Governance token\": G_comp_nodes,\n",
    "    \"Financial token\": G_cToken_nodes,\n",
    "    \"Governance\": (proposers | voters | delegators)\n",
    "}\n",
    "\n",
    "venn(subsets, fmt=\"{size}\", cmap='plasma', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI, NMI, AMI & ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    \n",
    "    return -sum((count/total) * log2(count/total) for count in counts.values())\n",
    "\n",
    "def mutual_information(x, y):\n",
    "    total = len(x)\n",
    "    counter_x = Counter(x)\n",
    "    counter_y = Counter(y)\n",
    "    joint_counter = Counter(zip(x, y))\n",
    "    mi = 0.0\n",
    "    \n",
    "    for (x_val, y_val), joint_count in joint_counter.items():\n",
    "        px = counter_x[x_val] / total\n",
    "        py = counter_y[y_val] / total\n",
    "        pxy = joint_count / total\n",
    "        mi += pxy * log2(pxy / (px * py))\n",
    "    \n",
    "    return mi\n",
    "\n",
    "def variation_of_information(x, y):\n",
    "    return entropy(x) + entropy(y) - 2 * mutual_information(x, y)\n",
    "\n",
    "def run_leiden_iterations(graph, n_iterations=10, resolution=1.0):\n",
    "    memberships = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        partition = la.find_partition(\n",
    "            graph,\n",
    "            la.ModularityVertexPartition,\n",
    "            weights='weight'\n",
    "        )\n",
    "        print(i, partition.modularity)\n",
    "        \n",
    "        memberships.append(partition.membership)\n",
    "    \n",
    "    return memberships\n",
    "\n",
    "def compute_vi_nmi_ami_ari(memberships):\n",
    "    n = len(memberships)\n",
    "    vi_scores = np.zeros((n, n))\n",
    "    nmi_scores = np.zeros((n, n))\n",
    "    ami_scores = np.zeros((n, n))\n",
    "    ari_scores = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            vi = variation_of_information(memberships[i], memberships[j])\n",
    "            nmi = normalized_mutual_info_score(memberships[i], memberships[j])\n",
    "            ami = adjusted_mutual_info_score(memberships[i], memberships[j])\n",
    "            ari = adjusted_rand_score(memberships[i], memberships[j])\n",
    "            \n",
    "            vi_scores[i, j] = vi_scores[j, i] = vi\n",
    "            nmi_scores[i, j] = nmi_scores[j, i] = nmi\n",
    "            ami_scores[i, j] = ami_scores[j, i] = ami\n",
    "            ari_scores[i, j] = ari_scores[j, i] = ari\n",
    "    return vi_scores, nmi_scores, ami_scores, ari_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_comp_igraph = ig.Graph.from_networkx(G_comp, vertex_attr_hashable='name')\n",
    "G_comp_count_igraph = ig.Graph.from_networkx(G_comp_count, vertex_attr_hashable='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100\n",
    "resolution = 1.0\n",
    "memberships = run_leiden_iterations(G_comp_igraph, n_iterations, resolution)\n",
    "\n",
    "vi_scores, nmi_scores, ami_scores, ari_scores = compute_vi_nmi_ami_ari(memberships)\n",
    "\n",
    "avg_vi = np.mean(vi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_nmi = np.mean(nmi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ami = np.mean(ami_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ari = np.mean(ari_scores[np.triu_indices(n_iterations, k=1)])\n",
    "\n",
    "print(f\"Average VI: {avg_vi:.4f}\")\n",
    "print(f\"Average NMI: {avg_nmi:.4f}\")\n",
    "print(f\"Average AMI: {avg_ami:.4f}\")\n",
    "print(f\"Average ARI: {avg_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100\n",
    "resolution = 1.0\n",
    "memberships = run_leiden_iterations(G_comp_count_igraph, n_iterations, resolution)\n",
    "\n",
    "vi_scores, nmi_scores, ami_scores, ari_scores = compute_vi_nmi_ami_ari(memberships)\n",
    "\n",
    "avg_vi = np.mean(vi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_nmi = np.mean(nmi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ami = np.mean(ami_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ari = np.mean(ari_scores[np.triu_indices(n_iterations, k=1)])\n",
    "\n",
    "print(f\"Average VI: {avg_vi:.4f}\")\n",
    "print(f\"Average NMI: {avg_nmi:.4f}\")\n",
    "print(f\"Average AMI: {avg_ami:.4f}\")\n",
    "print(f\"Average ARI: {avg_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Leiden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_comp_count_igraph = ig.Graph.from_networkx(G_comp_count, vertex_attr_hashable='name')\n",
    "\n",
    "G_comp_count_igraph_components = G_comp_count_igraph.connected_components(mode=\"weak\")\n",
    "G_comp_count_igraph_largest_wcc = G_comp_count_igraph_components.giant()\n",
    "\n",
    "partition = la.find_partition(G_comp_count_igraph_largest_wcc, la.ModularityVertexPartition, weights='weight')\n",
    "\n",
    "print(len(partition), partition.modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G_comp_igraph.vs), len(G_comp_count_igraph_largest_wcc.vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_WCC_nodes = set(G_comp_igraph.vs['name']) - set(G_comp_count_igraph_largest_wcc.vs['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(non_WCC_nodes))\n",
    "print(len(non_WCC_nodes & CEX))\n",
    "print(len(non_WCC_nodes & CA))\n",
    "print(len(non_WCC_nodes & EOA & all_proposers_voters_delegators_in_G_comp))\n",
    "print(len(non_WCC_nodes & (EOA - all_proposers_voters_delegators_in_G_comp - CEX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_W_communities = 0\n",
    "G_C_communities = 0\n",
    "\n",
    "G_W_modularity = 0\n",
    "G_C_modularity = 0\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    \n",
    "    p1 = la.find_partition(G_comp_igraph, la.ModularityVertexPartition, weights='weight')\n",
    "    p2 = la.find_partition(G_comp_count_igraph, la.ModularityVertexPartition, weights='weight')\n",
    "    \n",
    "    G_W_communities += len(p1)\n",
    "    G_C_communities += len(p2)\n",
    "    \n",
    "    G_W_modularity += p1.modularity\n",
    "    G_C_modularity += p2.modularity\n",
    "    \n",
    "average_G_W_size = G_W_communities / 100\n",
    "average_G_C_size = G_C_communities / 100\n",
    "\n",
    "average_G_W_mod = G_W_modularity / 100\n",
    "average_G_C_mod = G_C_modularity / 100\n",
    "\n",
    "print(f'Communities: G_W: {average_G_W_size}, G_C: {average_G_C_size}')\n",
    "print(f'Modularity: G_W: {average_G_W_mod}, G_C: {average_G_C_mod}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_sizes = [len(c) for c in partition]\n",
    "\n",
    "sorted_indices = sorted(range(len(community_sizes)), key=lambda i: community_sizes[i], reverse=True)\n",
    "\n",
    "sorted_communities = [partition[i] for i in sorted_indices]\n",
    "\n",
    "top_communities = sorted_communities[:10]\n",
    "\n",
    "top_communities_sizes = [len(community) for community in top_communities]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), top_communities_sizes)\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Size')\n",
    "plt.title('Size of top communities')\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()\n",
    "\n",
    "all_communities_sizes = [len(community) for community in sorted_communities]\n",
    "log_bins = np.logspace(np.log10(min(all_communities_sizes)), np.log10(max(all_communities_sizes)), 30)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(all_communities_sizes, bins=30)\n",
    "# plt.yscale('log')\n",
    "# sns.histplot(all_communities_sizes, bins=log_bins)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Size of top communities')\n",
    "plt.show()\n",
    "\n",
    "communities_with_ids = [\n",
    "    {G_comp_count_igraph_largest_wcc.vs[idx][\"name\"] for idx in community} for community in top_communities\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_distribution = {\n",
    "    'Economic': [],\n",
    "    'Governance': [],\n",
    "    'Both': [],\n",
    "    'None': []\n",
    "}\n",
    "\n",
    "only_economic_users_in_G_wcc = only_economic_users_in_G_comp & set(G_comp_count_igraph_largest_wcc.vs[\"name\"])\n",
    "only_voters_in_G_wcc = only_voters_in_G_comp & set(G_comp_count_igraph_largest_wcc.vs[\"name\"])\n",
    "both_economic_and_governance_users_in_G_wcc = both_economic_and_governance_users & set(G_comp_count_igraph_largest_wcc.vs[\"name\"])\n",
    "all_economic_users_in_G_wcc = all_economic_users_in_G_comp & set(G_comp_count_igraph_largest_wcc.vs[\"name\"])\n",
    "all_voters_in_G_wcc = all_voters_in_G_comp & set(G_comp_count_igraph_largest_wcc.vs[\"name\"])\n",
    "\n",
    "for community in communities_with_ids:\n",
    "    economic_count = len(only_economic_users_in_G_wcc & community)\n",
    "    governance_count = len(only_voters_in_G_wcc & community)\n",
    "    both_count = len(both_economic_and_governance_users_in_G_wcc & community)\n",
    "    none_count = len(community) - economic_count - governance_count - both_count\n",
    "    \n",
    "    community_distribution['Economic'].append(economic_count)\n",
    "    community_distribution['Governance'].append(governance_count)\n",
    "    community_distribution['Both'].append(both_count)\n",
    "    community_distribution['None'].append(none_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8\n",
    "index = np.arange(len(communities_with_ids))\n",
    "\n",
    "economic_array = np.array(community_distribution['Economic'])\n",
    "governance_array = np.array(community_distribution['Governance'])\n",
    "both_array = np.array(community_distribution['Both'])\n",
    "none_array = np.array(community_distribution['None'])\n",
    "\n",
    "bar1 = ax.bar(index, none_array, bar_width, label='None')\n",
    "bar2 = ax.bar(index, economic_array, bar_width, bottom=none_array, label='Economic')\n",
    "bar3 = ax.bar(index, governance_array, bar_width, bottom=none_array + economic_array, label='Governance')\n",
    "bar4 = ax.bar(index, both_array, bar_width, bottom=none_array + economic_array + governance_array, label='Both')\n",
    "\n",
    "ax.set_xlabel('Community index')\n",
    "ax.set_ylabel('Number of nodes')\n",
    "ax.set_title('Stacked distribution of nodes in communities')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([f'Community {i+1}' for i in range(len(communities_with_ids))], rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_distribution_2 = {\n",
    "    'EOA gov': [],\n",
    "    'EOA non gov': [],\n",
    "    'CEX': [],\n",
    "    'CA': []\n",
    "}\n",
    "\n",
    "total_EOA_gov_percentage = 0\n",
    "total_EOA_non_gov_percentage = 0\n",
    "total_CEX_percentage = 0\n",
    "total_CA_percentage = 0\n",
    "\n",
    "EOA_gov_percentages = []\n",
    "EOA_non_gov_percentages = []\n",
    "CEX_percentages = []\n",
    "CA_percentages = []\n",
    "\n",
    "# 'Voters': [i['avg_infected'] for i in dataset if i['seed'] in EOA and i['seed'] in all_proposers_voters_delegators_in_G_aave and i['seed'] not in CEX],\n",
    "# 'Non-voters': [i['avg_infected'] for i in dataset if i['seed'] in EOA and i['seed'] not in all_proposers_voters_delegators_in_G_aave and i['seed'] not in CEX],\n",
    "# 'CA': [i['avg_infected'] for i in dataset if i['seed'] in CA],\n",
    "# 'CEX': [i['avg_infected'] for i in dataset if i['seed'] in CEX],\n",
    "\n",
    "id = 1\n",
    "for community in communities_with_ids:\n",
    "    EOA_gov_count = len(community & (EOA & all_proposers_voters_delegators_in_G_comp))\n",
    "    EOA_non_gov_count = len(community & (EOA - all_proposers_voters_delegators_in_G_comp - CEX))\n",
    "    CEX_count = len(community & CEX)\n",
    "    CA_count = len(community & CA)\n",
    "    \n",
    "    community_distribution_2['EOA gov'].append(EOA_gov_count)\n",
    "    community_distribution_2['EOA non gov'].append(EOA_non_gov_count)\n",
    "    community_distribution_2['CEX'].append(CEX_count)\n",
    "    community_distribution_2['CA'].append(CA_count)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8\n",
    "index = np.arange(len(communities_with_ids))\n",
    "\n",
    "EOA_gov_array = np.array(community_distribution_2['EOA gov'])\n",
    "EOA_non_gov_array = np.array(community_distribution_2['EOA non gov'])\n",
    "CEX_array = np.array(community_distribution_2['CEX'])\n",
    "CA_array = np.array(community_distribution_2['CA'])\n",
    "\n",
    "bar1 = ax.bar(index, EOA_non_gov_array, bar_width, label='EOA non-governance')\n",
    "bar2 = ax.bar(index, EOA_gov_array, bar_width, bottom=EOA_non_gov_array, label='EOA governance')\n",
    "bar3 = ax.bar(index, CA_array + CEX_array, bar_width, bottom=EOA_non_gov_array + EOA_gov_array, label='Infrastructure')\n",
    "# bar3 = ax.bar(index, CEX_array, bar_width, bottom=EOA_non_gov_array + EOA_gov_array, label='CEX')\n",
    "# bar4 = ax.bar(index, CA_array, bar_width, bottom=EOA_non_gov_array + EOA_gov_array + CEX_array, label='CA')\n",
    "\n",
    "ax.set_xlabel('Community index')\n",
    "ax.set_ylabel('Number of nodes')\n",
    "# ax.set_title('Stacked distribution of nodes in communities')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([f'{i+1}' for i in range(len(communities_with_ids))])\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_distribution_3 = {\n",
    "    'gov': [],\n",
    "    'other': []\n",
    "}\n",
    "\n",
    "total_gov_percentage = 0\n",
    "total_other_percentage = 0\n",
    "\n",
    "gov_percentages = []\n",
    "other_percentages = []\n",
    "\n",
    "id = 1\n",
    "for community in communities_with_ids:\n",
    "    gov_count = len(community & all_proposers_voters_delegators_in_G_comp)\n",
    "    other_count = len(community) - gov_count\n",
    "    \n",
    "    community_distribution_3['gov'].append(gov_count)\n",
    "    community_distribution_3['other'].append(other_count)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8\n",
    "index = np.arange(len(communities_with_ids))\n",
    "\n",
    "gov_array = np.array(community_distribution_3['gov'])\n",
    "other_array = np.array(community_distribution_3['other'])\n",
    "\n",
    "bar1 = ax.bar(index, other_array, bar_width, label='Other')\n",
    "bar2 = ax.bar(index, gov_array, bar_width, bottom=other_array, label='Governance')\n",
    "\n",
    "ax.set_xlabel('Community index')\n",
    "ax.set_ylabel('Number of nodes')\n",
    "# ax.set_title('Distribution of nodes per community')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([f'{i+1}' for i in range(len(communities_with_ids))])\n",
    "# ax.set_xticklabels([f'Community {i+1}' for i in range(len(communities_with_ids))], rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gov_array = np.array(community_distribution_2['EOA gov'])\n",
    "# CEX_array = np.array(community_distribution_2['CEX'])\n",
    "# CA_array = np.array(community_distribution_2['CA'])\n",
    "\n",
    "total_gov_percentage = (len(all_proposers_voters_delegators_in_G_comp) / len(G_comp_nodes)) * 100\n",
    "\n",
    "community_gov_percentages = []\n",
    "\n",
    "for i in range(len(gov_array)):\n",
    "    gov_percentage = (gov_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_gov_percentages.append(gov_percentage)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(gov_array)), gov_array)#, color='orange')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Number of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "# plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'{i+1}' for i in range(len(top_communities))])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), community_gov_percentages, label='Community percentage')\n",
    "plt.axhline(y=total_gov_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Percentage of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'{i+1}' for i in range(len(top_communities))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n",
    "\n",
    "axs[0].bar(range(len(EOA_gov_array)), EOA_gov_array)\n",
    "# axs[0].axhline(y=total_EOA_gov_percentage, color='r', linestyle='--', label='Total EOA percentage')\n",
    "axs[0].set_xlabel('Community index')\n",
    "axs[0].set_ylabel('Number of nodes')\n",
    "axs[0].set_title('EOA governance')\n",
    "# axs[0].legend()\n",
    "axs[0].set_xticks(range(len(top_communities)))\n",
    "axs[0].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "\n",
    "infra_array = CA_array + CEX_array\n",
    "axs[1].bar(range(len(infra_array)), infra_array)\n",
    "# axs[1].axhline(y=total_CEX_percentage + total_CA_percentage, color='r', linestyle='--', label='Total percentage')\n",
    "axs[1].set_xlabel('Community index')\n",
    "axs[1].set_title('Infrastructural')\n",
    "# axs[1].legend()\n",
    "axs[1].set_xticks(range(len(top_communities)))\n",
    "axs[1].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(EOA_gov_array)), EOA_gov_array)#, color='orange')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Number of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "# plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(infra_array)), infra_array)#, color='green')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Number of nodes')\n",
    "# plt.title('Percentage of economic nodes per community vs total economic percentage')\n",
    "# plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOA_gov_array = np.array(community_distribution_2['EOA gov'])\n",
    "CEX_array = np.array(community_distribution_2['CEX'])\n",
    "CA_array = np.array(community_distribution_2['CA'])\n",
    "\n",
    "total_EOA_gov_percentage = (len(EOA & all_proposers_voters_delegators_in_G_comp) / len(G_comp_nodes)) * 100\n",
    "total_CEX_percentage = (len(CEX & G_comp_nodes) / len(G_comp_nodes)) * 100\n",
    "total_CA_percentage = (len(CA & G_comp_nodes) / len(G_comp_nodes)) * 100\n",
    "\n",
    "community_EOA_gov_percentages = []\n",
    "community_CEX_percentages = []\n",
    "community_CA_percentages = []\n",
    "\n",
    "for i in range(len(EOA_gov_array)):\n",
    "    EOA_gov_percentage = (EOA_gov_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_EOA_gov_percentages.append(EOA_gov_percentage)\n",
    "    \n",
    "    CEX_percentage = (CEX_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_CEX_percentages.append(CEX_percentage)\n",
    "    \n",
    "    CA_percentage = (CA_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_CA_percentages.append(CA_percentage)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(20, 6), sharex=True)\n",
    "\n",
    "axs[0].bar(range(len(top_communities)), community_EOA_gov_percentages, label='Community percentage')\n",
    "axs[0].axhline(y=total_EOA_gov_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "axs[0].set_xlabel('Community index')\n",
    "axs[0].set_ylabel('Percentage of nodes')\n",
    "axs[0].set_title('EOA governance')\n",
    "axs[0].legend()\n",
    "axs[0].set_xticks(range(len(top_communities)))\n",
    "axs[0].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "\n",
    "CEX_CA_percentages = [community_CA_percentages[i] + community_CEX_percentages[i] for i in range(len(community_CEX_percentages))]\n",
    "axs[1].bar(range(len(top_communities)), CEX_CA_percentages, label='Community percentage')\n",
    "axs[1].axhline(y=total_CEX_percentage + total_CA_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "axs[1].set_xlabel('Community index')\n",
    "axs[1].set_title('Infrastructure')\n",
    "axs[1].legend()\n",
    "axs[1].set_xticks(range(len(top_communities)))\n",
    "axs[1].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "\n",
    "# axs[1].bar(range(len(top_communities)), community_CEX_percentages, label='Community CEX percentage')\n",
    "# axs[1].axhline(y=total_CEX_percentage, color='r', linestyle='--', label='Total CEX percentage')\n",
    "# axs[1].set_xlabel('Community index')\n",
    "# axs[1].set_title('CEX')\n",
    "# axs[1].legend()\n",
    "# axs[1].set_xticks(range(len(top_communities)))\n",
    "# axs[1].set_xticklabels([f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "\n",
    "# axs[2].bar(range(len(top_communities)), community_CA_percentages, label='Community CA percentage')\n",
    "# axs[2].axhline(y=total_CA_percentage, color='r', linestyle='--', label='Total CA percentage')\n",
    "# axs[2].set_xlabel('Community index')\n",
    "# axs[2].set_title('CA')\n",
    "# axs[2].legend()\n",
    "# axs[2].set_xticklabels([f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), community_EOA_gov_percentages, label='Community percentage')\n",
    "plt.axhline(y=total_EOA_gov_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Percentage of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), CEX_CA_percentages, label='Community percentage')\n",
    "plt.axhline(y=total_CEX_percentage + total_CA_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Percentage of nodes')\n",
    "# plt.title('Percentage of economic nodes per community vs total economic percentage')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature averages per community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_community(mapping):\n",
    "    community_averages = {feature_name: [] for feature_name, _ in mapping}\n",
    "    community_distributions = {feature_name: [] for feature_name, _ in mapping}\n",
    "\n",
    "    for community in top_communities:\n",
    "        community_node_ids = {G_comp_count_igraph_largest_wcc.vs[idx][\"name\"] for idx in community}\n",
    "\n",
    "        for feature_name, feature_mapping in mapping:\n",
    "            feature_values = [feature_mapping[node_id] for node_id in community_node_ids if node_id in feature_mapping]\n",
    "            average_value = sum(feature_values) / len(feature_values) if feature_values else 0\n",
    "            \n",
    "            community_averages[feature_name].append(average_value)\n",
    "            community_distributions[feature_name].append(feature_values)\n",
    "    \n",
    "    num_features = len(mapping)\n",
    "    cols = 3\n",
    "    rows = (num_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i, (feature_name, averages) in enumerate(community_averages.items()):\n",
    "        axs[i].plot(range(len(top_communities)), averages, marker='o')\n",
    "        axs[i].set_title(feature_name)\n",
    "        axs[i].set_xlabel(\"Community index\")\n",
    "        axs[i].set_ylabel(\"Average feature score\")\n",
    "        axs[i].set_xticks(range(len(top_communities)))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(len(top_communities))])\n",
    "    \n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_community_2(mapping):\n",
    "    feature_stats = {}\n",
    "    for feature_name, feature_mapping in mapping:\n",
    "        values = list(feature_mapping.values())\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        feature_stats[feature_name] = (mean, std)\n",
    "\n",
    "    community_averages = {feature_name: [] for feature_name, _ in mapping}\n",
    "    community_distributions = {feature_name: [] for feature_name, _ in mapping}\n",
    "\n",
    "    for community in top_communities:\n",
    "        community_node_ids = {G_comp_count_igraph_largest_wcc.vs[idx][\"name\"] for idx in community}\n",
    "\n",
    "        for feature_name, feature_mapping in mapping:\n",
    "            feature_values = [feature_mapping[node_id] for node_id in community_node_ids if node_id in feature_mapping]\n",
    "\n",
    "            if feature_values:\n",
    "                avg_val = sum(feature_values) / len(feature_values)\n",
    "                mean, std = feature_stats[feature_name]\n",
    "                z_score = (avg_val - mean) / std if std > 0 else 0\n",
    "            else:\n",
    "                z_score = 0\n",
    "\n",
    "            community_averages[feature_name].append(z_score)\n",
    "            community_distributions[feature_name].append(feature_values)\n",
    "\n",
    "    num_features = len(mapping)\n",
    "    cols = 3\n",
    "    rows = (num_features + cols - 1) // cols\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (feature_name, averages) in enumerate(community_averages.items()):\n",
    "        axs[i].plot(range(len(top_communities)), averages, marker='o')\n",
    "        axs[i].set_title(f'{feature_name} (z-score)')\n",
    "        axs[i].set_xlabel('Community index')\n",
    "        axs[i].set_ylabel('Z-score')\n",
    "        axs[i].set_xticks(range(len(top_communities)))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(len(top_communities))])\n",
    "\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community(COMP_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community_2(COMP_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community(cWETH_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community_2(cWETH_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community(gov_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community_2(gov_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SI influence comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/comp/SI_5_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_5_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_2_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_2_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_1_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_1_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_05_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_05_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_01_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_01_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/comp/SI_001_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_001_extracted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_influence_multiple_runs(influence_data_list, beta_labels):\n",
    "    num_runs = len(influence_data_list)\n",
    "    num_communities = len(top_communities)\n",
    "\n",
    "    community_scores = [defaultdict(list) for _ in range(num_runs)]\n",
    "\n",
    "    for run_idx, influence_data in enumerate(influence_data_list):\n",
    "        influence_scores = {entry['seed']: entry['avg_infected'] for entry in influence_data if entry['seed'] in all_proposers_voters_delegators_in_G_comp}\n",
    "        for comm_id, community in enumerate(top_communities):\n",
    "            for node_idx in community:\n",
    "                node_id = G_comp_count_igraph_largest_wcc.vs[node_idx]['name']\n",
    "                if node_id in influence_scores:\n",
    "                    community_scores[run_idx][comm_id].append(influence_scores[node_id])\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_runs, figsize=(5 * num_runs, 3))\n",
    "    if num_runs == 1:\n",
    "        axs = [axs]\n",
    "    for i in range(num_runs):\n",
    "        avg_infs = [\n",
    "            np.mean(community_scores[i][cid]) if community_scores[i][cid] else 0\n",
    "            for cid in range(num_communities)\n",
    "        ]\n",
    "        axs[i].plot(range(num_communities), avg_infs, marker='o', label=f\"Î²={beta_labels[i]}\")\n",
    "        axs[i].set_title(f\"Î²={beta_labels[i]}\")\n",
    "        axs[i].set_xlabel(\"Community index\")\n",
    "        axs[i].set_ylabel(\"Average infected nodes\")\n",
    "        axs[i].set_xticks(range(num_communities))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(num_communities)])\n",
    "        # axs[i].grid(True)\n",
    "        # axs[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_runs, figsize=(5 * num_runs, 3))\n",
    "    if num_runs == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        data = [community_scores[i][cid] for cid in range(num_communities)]\n",
    "        axs[i].boxplot(data, positions=range(num_communities))\n",
    "        axs[i].set_title(f\"Î²={beta_labels[i]}\")\n",
    "        axs[i].set_xlabel(\"Community index\")\n",
    "        axs[i].set_ylabel(\"Average infected nodes\")\n",
    "        axs[i].set_xticks(range(num_communities))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(num_communities)])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # fig, axs = plt.subplots(1, num_runs, figsize=(6 * num_runs, 5))\n",
    "    # if num_runs == 1:\n",
    "    #     axs = [axs]\n",
    "\n",
    "    # for i in range(num_runs):\n",
    "    #     data = [community_scores[i][cid] for cid in range(num_communities)]\n",
    "    #     axs[i].violinplot(data, positions=range(num_communities), showmedians=True)\n",
    "    #     axs[i].set_title(f\"Violin plot (Î²={beta_labels[i]})\")\n",
    "    #     axs[i].set_xlabel(\"Community\")\n",
    "    #     axs[i].set_xticks(range(num_communities))\n",
    "    #     axs[i].set_xticklabels([f\"{cid}\" for cid in range(num_communities)])\n",
    "    #     axs[i].set_ylabel(\"Average infected nodes\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_dicts = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "labels = ['0.01', '0.1', '0.5', '1.0', '2.0']#, '5.0']\n",
    "\n",
    "plot_influence_multiple_runs(influence_dicts, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(G):\n",
    "    max_weight = max(data['weight'] for _, _, _, data in G.edges(keys=True, data=True))\n",
    "    \n",
    "    for _, _, _, data in G.edges(keys=True, data=True):\n",
    "        data['norm_weight'] = data['weight'] / max_weight\n",
    "    \n",
    "    return G\n",
    "\n",
    "def log_normalize_weights(G):\n",
    "    max_weight = np.log1p(max(data['weight'] for _, _, _, data in G.edges(keys=True, data=True)))\n",
    "    \n",
    "    for _, _, _, data in G.edges(keys=True, data=True):\n",
    "        data['norm_weight'] = np.log1p(data['weight']) / max_weight\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple SI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = copy.deepcopy(G_comp_complete)\n",
    "# G_T = copy.deepcopy(G_comp_complete)\n",
    "\n",
    "G = log_normalize_weights(G)\n",
    "# G_T = normalize_weights(G_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_out_edges(G, beta=1.0):\n",
    "    out_edges = defaultdict(list)\n",
    "    \n",
    "    for u, v, k, d in G.edges(keys=True, data=True):\n",
    "        out_edges[u].append((v, d['timestamp'], beta * d['norm_weight']))\n",
    "    \n",
    "    return out_edges\n",
    "\n",
    "def simulate_SI(G, seed_node, beta=1.0, cached_edges=None):\n",
    "    infected = {}\n",
    "    infection_queue = []\n",
    "\n",
    "    infected[seed_node] = 1696118400\n",
    "    heapq.heappush(infection_queue, (1696118400, seed_node))\n",
    "\n",
    "    while infection_queue:\n",
    "        curr_time, node = heapq.heappop(infection_queue)\n",
    "        \n",
    "        if node not in cached_edges:\n",
    "            continue\n",
    "        \n",
    "        for neighbor, ts, prob in cached_edges[node]:\n",
    "        # for _, neighbor, _, data in G.out_edges(node, keys=True, data=True):\n",
    "            if neighbor in infected:\n",
    "                continue\n",
    "            \n",
    "            # ts = data['timestamp']\n",
    "            # prob = beta * data['norm_weight']\n",
    "\n",
    "            if ts >= curr_time:\n",
    "                if np.random.rand() < prob:\n",
    "                    infected[neighbor] = ts\n",
    "                    heapq.heappush(infection_queue, (ts, neighbor))\n",
    "\n",
    "    return infected\n",
    "\n",
    "def run_multiple_iterations(G, num_iterations=100, beta=1.0):\n",
    "    # all_nodes = list(G.nodes)\n",
    "    all_nodes = list(all_proposers_voters_delegators_in_G_comp)\n",
    "    infection_counts = defaultdict(list)\n",
    "    infections_per_iteration = defaultdict(list)\n",
    "    \n",
    "    cached_edges = preprocess_out_edges(G, beta)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"Iteration {i+1}/{num_iterations}\")\n",
    "        # for seed in tqdm(all_nodes, desc=f\"SI iteration {i+1}\"):\n",
    "        for seed in all_nodes:\n",
    "            infected = simulate_SI(G, seed, beta, cached_edges)\n",
    "            infection_counts[seed].append(len(infected))\n",
    "            infections_per_iteration[seed].append(infected)\n",
    "\n",
    "    avg_results = [{\n",
    "        'seed': node,\n",
    "        'avg_infected': sum(counts) / len(counts),\n",
    "        'infections_per_iteration': infections_per_iteration[node]\n",
    "        # 'all_infected_counts': counts\n",
    "    } for node, counts in infection_counts.items()]\n",
    "\n",
    "    top_avg_influencers = sorted(avg_results, key=lambda x: x['avg_infected'], reverse=True)\n",
    "\n",
    "    return top_avg_influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_5 = run_multiple_iterations(G, num_iterations=10, beta=5.0)\n",
    "\n",
    "with open('data/SI/comp/SI_5.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_5, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_5_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_5]\n",
    "\n",
    "with open('data/SI/comp/SI_5_extracted_seeds_avg_infected.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_5_extracted, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_001_1_outcome = joblib.load('data/SI/comp/SI_001.pkl')\n",
    "SI_01_1_outcome = joblib.load('data/SI/comp/SI_01.pkl')\n",
    "SI_05_1_outcome = joblib.load('data/SI/comp/SI_05.pkl')\n",
    "SI_1_1_outcome = joblib.load('data/SI/comp/SI_1.pkl')\n",
    "SI_2_1_outcome = joblib.load('data/SI/comp/SI_2.pkl')\n",
    "SI_5_1_outcome = joblib.load('data/SI/comp/SI_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/comp/SI_001_top_10.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_001_1_outcome[:10], file)\n",
    "with open('data/SI/comp/SI_01_top_10.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_01_1_outcome[:10], file)\n",
    "with open('data/SI/comp/SI_05_top_10.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_05_1_outcome[:10], file)\n",
    "with open('data/SI/comp/SI_1_top_10.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_1_1_outcome[:10], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI_001_1_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_001_1_outcome]\n",
    "# SI_01_1_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_01_1_outcome]\n",
    "# SI_05_1_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_05_1_outcome]\n",
    "# SI_1_1_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_1_1_outcome]\n",
    "SI_2_1_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_2_1_outcome]\n",
    "SI_5_1_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_5_1_outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_SIZE = timedelta(days=1)\n",
    "\n",
    "def plot_infections_average(infections_list):\n",
    "    all_binned_counts = []\n",
    "\n",
    "    for infections in infections_list:\n",
    "        for iteration in infections['infections_per_iteration']:\n",
    "            bin_counts = defaultdict(int)\n",
    "            for ts in iteration.values():\n",
    "                t = datetime.fromtimestamp(ts)\n",
    "                bin_time = datetime(t.year, t.month, t.day)\n",
    "                bin_counts[bin_time] += 1\n",
    "            all_binned_counts.append(bin_counts)\n",
    "\n",
    "    all_bins = sorted(set().union(*[d.keys() for d in all_binned_counts]))\n",
    "\n",
    "    counts_matrix = []\n",
    "    for bin_counts in all_binned_counts:\n",
    "        counts = [bin_counts.get(t, 0) for t in all_bins]\n",
    "        counts_matrix.append(counts)\n",
    "\n",
    "    counts_matrix = np.array(counts_matrix)\n",
    "    mean_counts = counts_matrix.mean(axis=0)\n",
    "    std_counts = counts_matrix.std(axis=0)\n",
    "    cumulative_mean = np.cumsum(mean_counts)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(all_bins, mean_counts, label='Mean infection rate')\n",
    "    plt.fill_between(all_bins, mean_counts - std_counts, mean_counts + std_counts, alpha=0.2, label='Standard deviation')\n",
    "    # plt.title('Average infection rate over time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Infections')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(all_bins, cumulative_mean, label='Mean cumulative')\n",
    "    # plt.title('Average cumulative infections over time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative infections')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_infections_average(SI_001_1_outcome[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_infections_average(SI_01_1_outcome[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_infections_average(SI_05_1_outcome[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_001_outcome = joblib.load('data/SI/comp/SI_001_extracted_seeds_avg_infected.pkl')\n",
    "SI_01_outcome = joblib.load('data/SI/comp/SI_01_extracted_seeds_avg_infected.pkl')\n",
    "SI_05_outcome = joblib.load('data/SI/comp/SI_05_extracted_seeds_avg_infected.pkl')\n",
    "SI_1_outcome = joblib.load('data/SI/comp/SI_1_extracted_seeds_avg_infected.pkl')\n",
    "SI_2_outcome = joblib.load('data/SI/comp/SI_2_extracted_seeds_avg_infected.pkl')\n",
    "SI_5_outcome = joblib.load('data/SI/comp/SI_5_extracted_seeds_avg_infected.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_infected_001 = [entry['avg_infected'] for entry in SI_001_outcome]\n",
    "avg_infected_01 = [entry['avg_infected'] for entry in SI_01_outcome]\n",
    "avg_infected_05 = [entry['avg_infected'] for entry in SI_05_outcome]\n",
    "avg_infected_1 = [entry['avg_infected'] for entry in SI_1_outcome]\n",
    "\n",
    "# avg_infected_001_T = [entry['avg_infected'] for entry in SI_T_001_outcome]\n",
    "# avg_infected_01_T = [entry['avg_infected'] for entry in SI_T_01_outcome]\n",
    "# avg_infected_05_T = [entry['avg_infected'] for entry in SI_T_05_outcome]\n",
    "# avg_infected_1_T = [entry['avg_infected'] for entry in SI_T_1_outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_scores = {\n",
    "    'Î²=0.01': {seed['seed']: seed['avg_infected'] for seed in SI_001_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_comp},\n",
    "    'Î²=0.1': {seed['seed']: seed['avg_infected'] for seed in SI_01_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_comp},\n",
    "    'Î²=0.5': {seed['seed']: seed['avg_infected'] for seed in SI_05_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_comp},\n",
    "    'Î²=1.0': {seed['seed']: seed['avg_infected'] for seed in SI_1_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_comp},\n",
    "    'Î²=2.0': {seed['seed']: seed['avg_infected'] for seed in SI_2_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_comp},\n",
    "    # '5.0': {seed['seed']: seed['avg_infected'] for seed in SI_5_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_comp}\n",
    "}\n",
    "\n",
    "influence_df = pd.DataFrame(influence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_corr = influence_df.corr(method='spearman')\n",
    "kendall_corr = influence_df.corr(method='kendall')\n",
    "# pearson_corr = influence_df.corr(method='pearson')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(spearman_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, ax=axs[0])\n",
    "axs[0].set_title(\"Spearman rank correlation between beta configs\")\n",
    "\n",
    "sns.heatmap(kendall_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, ax=axs[1])\n",
    "axs[1].set_title(\"Kendall rank correlation between beta configs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman_corr = influence_df.corr(method='spearman')\n",
    "kendall_corr = influence_df.corr(method='kendall')\n",
    "# pearson_corr = influence_df.corr(method='pearson')\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "# plt.title(\"Spearman correlation\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(kendall_corr, annot=True, cmap=\"coolwarm\", cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.yticks(rotation=0)\n",
    "# plt.title(\"Kendall correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    '0.01': SI_001_outcome,\n",
    "    '0.1': SI_01_outcome,\n",
    "    '0.5': SI_05_outcome,\n",
    "    '1.0': SI_1_outcome,\n",
    "    '2.0': SI_2_outcome,\n",
    "    '5.0': SI_5_outcome\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_comparison_per_beta(datasets):\n",
    "    hist_data = {}\n",
    "    \n",
    "    for beta, dataset in datasets.items():\n",
    "        temp = [i['avg_infected'] for i in dataset if i['seed'] in all_proposers_voters_delegators_in_G_comp]\n",
    "        hist_data[beta] = temp\n",
    "        \n",
    "    fig, axs = plt.subplots(1, 6, figsize=(48, 6))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for ax, (title, data) in zip(axs, hist_data.items()):\n",
    "        ax.hist(data, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Avg. infected nodes')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_comparison_per_beta(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_comparison_per_config(datasets):\n",
    "    for beta, dataset in datasets.items():\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(24, 6))\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        groups = {\n",
    "            'EOA governance': [i['avg_infected'] for i in dataset if i['seed'] in EOA and i['seed'] in all_proposers_voters_delegators_in_G_comp and i['seed'] not in CEX],\n",
    "            'EOA non-governance': [i['avg_infected'] for i in dataset if i['seed'] in EOA and i['seed'] not in all_proposers_voters_delegators_in_G_comp and i['seed'] not in CEX],\n",
    "            # 'CA': [i['avg_infected'] for i in dataset if i['seed'] in CA],\n",
    "            # 'CEX': [i['avg_infected'] for i in dataset if i['seed'] in CEX],\n",
    "            'Infrastructural': [i['avg_infected'] for i in dataset if i['seed'] in CEX or i['seed'] in CA]\n",
    "        }\n",
    "\n",
    "        for ax, (title, data) in zip(axs, groups.items()):\n",
    "            ax.hist(data, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Avg. infected nodes')\n",
    "            ax.set_ylabel('Frequency')\n",
    "\n",
    "        # fig.suptitle(f'Î²={beta}')#, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_comparison_per_config(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('data/COMP_daily_USD.csv', parse_dates=['snapped_at'])\n",
    "\n",
    "price_data['date'] = price_data['snapped_at'].dt.date\n",
    "\n",
    "comp_df['date'] = pd.to_datetime(comp_df['timestamp'], unit='s')\n",
    "comp_df['date'] = comp_df['date'].dt.date\n",
    "\n",
    "merged_data_comp = comp_df.groupby('date').size().reset_index(name='num_entries')\n",
    "merged_data_comp = comp_df.groupby('date').agg(num_entries=('value', 'count'), total_value=('value', 'sum')).reset_index()\n",
    "merged_data_comp = merged_data_comp.merge(price_data[['date', 'price']], on='date', how='left')\n",
    "merged_data_comp['total_traded_value'] = merged_data_comp['total_value'] * merged_data_comp['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax1.plot(merged_data_comp['date'], merged_data_comp['num_entries'])\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of transfers')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(merged_data_comp['date'], merged_data_comp['price'], color='red')\n",
    "# ax2.set_ylabel('Price', color='red')\n",
    "# ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# plt.title('Number of COMP transfers per day')\n",
    "plt.show()\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# ax1.plot(merged_data_comp['date'], merged_data_comp['total_value'])\n",
    "# ax1.set_xlabel('Date')\n",
    "# ax1.set_ylabel('Number of tokens')\n",
    "# ax1.tick_params(axis='y')\n",
    "\n",
    "# # ax2 = ax1.twinx()\n",
    "# # ax2.plot(merged_data_comp['date'], merged_data_comp['price'], color='red')\n",
    "# # ax2.set_ylabel('Price', color='red')\n",
    "# # ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# plt.title('Number of COMP tokens transferred per day and price over time')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged_data_comp['date'], merged_data_comp['total_traded_value'])\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Total traded value in USD')\n",
    "# plt.title('Total COMP value traded per day')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_addresses = pd.concat([comp_df['from'], comp_df['to']])\n",
    "cToken_addresses = pd.concat([cToken_df['from'], cToken_df['to']])\n",
    "\n",
    "comp_transaction_counts = comp_addresses.value_counts()\n",
    "cToken_transaction_counts = cToken_addresses.value_counts()\n",
    "\n",
    "voters_set = set(vote_counts.keys())\n",
    "\n",
    "comp_voters_transaction_counts = comp_transaction_counts[comp_transaction_counts.index.isin(voters_set)]\n",
    "comp_non_voters_transaction_counts = comp_transaction_counts[~comp_transaction_counts.index.isin(voters_set)]\n",
    "\n",
    "cWETH_voters_transaction_counts = cToken_transaction_counts[cToken_transaction_counts.index.isin(voters_set)]\n",
    "cWETH_non_voters_transaction_counts = cToken_transaction_counts[~cToken_transaction_counts.index.isin(voters_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(comp_voters_transaction_counts, bins=100, color='blue', label='Voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Number of transactions per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of transactions per user\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(comp_non_voters_transaction_counts, bins=100, color='red', label='Non-voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Number of transactions per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of transactions per user\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(comp_df['value'], bins=50, edgecolor='black')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of weights')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.log1p(comp_df['value']), bins=50)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Log1p(Value)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of log1p weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposer_counts = pd.Series([proposal['proposer']['id'] for proposal in comp_proposals]).value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "proposer_counts.plot(kind='bar')\n",
    "plt.xlabel('Proposer')\n",
    "plt.ylabel('Number of proposals created')\n",
    "plt.title('Distribution of proposals created by each proposer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_counts = pd.Series([vote['voter']['id'].lower() for votes in comp_votes for vote in votes['votes']]).value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(voter_counts, bins=50)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Voter')\n",
    "plt.ylabel('Number of votes casted')\n",
    "plt.title('Distribution of votes casted by each voter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weights = []\n",
    "\n",
    "for votes in comp_votes:\n",
    "    for vote in votes['votes']:\n",
    "        vote_weights.append(float(vote['weight']))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(vote_weights, bins=50, log_scale=(True, False))\n",
    "plt.xlabel('Vote weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of vote weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_outcomes = pd.Series([proposal['state'] for proposal in comp_proposals])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "proposal_outcomes.value_counts().plot(kind='bar')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Number of proposals')\n",
    "plt.title('Proposal outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegations_df = pd.DataFrame(delegations)\n",
    "delegations_df['date'] = pd.to_datetime(delegations_df['blockTimestamp'], unit='s')\n",
    "delegations_df['date'] = delegations_df['date'].dt.date\n",
    "delegations_over_time = delegations_df.groupby('date').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "delegations_over_time.plot()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of delegations')\n",
    "plt.title('Delegations over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegator_counts = delegations_df['delegator'].value_counts()\n",
    "delegate_counts = delegations_df['delegate'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(delegator_counts, bins=50)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Delegator')\n",
    "plt.ylabel('Number of delegations')\n",
    "plt.title('Distribution of delegators')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(delegate_counts, bins=50)#, log_scale=(True, False))\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('Delegate')\n",
    "plt.ylabel('Number of delegations')\n",
    "plt.title('Distribution of delegates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_votes = {proposal['id']: float(proposal['forWeightedVotes']) for proposal in comp_proposals}\n",
    "\n",
    "against_votes = {proposal['id']: float(proposal['againstWeightedVotes']) for proposal in comp_proposals}\n",
    "                \n",
    "total_votes = {proposal['id']: float(proposal['forWeightedVotes']) + float(proposal['againstWeightedVotes']) for proposal in comp_proposals}\n",
    "\n",
    "# num_voters = {proposal['id']: proposal['totalCurrentVoters'] for proposal in comp_proposals}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(for_votes, bins=50, color='blue', kde=True)\n",
    "plt.xlabel('Number of FOR votes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of FOR votes')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(against_votes, bins=50, color='red', kde=True)\n",
    "plt.xlabel('Number of AGAINST votes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of AGAINST votes')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(total_votes, bins=50, color='yellow', kde=True)\n",
    "plt.xlabel('Number of total votes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of total votes')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(num_voters, bins=50, color='green', kde=True)\n",
    "# plt.xlabel('Number of voters')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of the number of voters')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proposal_dates = [datetime.fromtimestamp(int(proposal['creationTime'])) for proposal in comp_proposals]\n",
    "\n",
    "proposal_dates_df = pd.DataFrame({'date': all_proposal_dates})\n",
    "proposal_dates_df['month'] = proposal_dates_df['date'].dt.to_period('M')\n",
    "\n",
    "proposals_per_month = proposal_dates_df.groupby('month').size().reset_index(name='count')\n",
    "\n",
    "proposals_per_month['month'] = proposals_per_month['month'].dt.to_timestamp()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(proposals_per_month['month'], proposals_per_month['count'])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of proposals')\n",
    "plt.title('Number of proposals per month')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
