{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import itertools\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "import heapq\n",
    "import copy\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from venn import venn\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_mutual_info_score, adjusted_rand_score\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers_df = pd.read_csv('data/1_year/reduced_transfers_1_year.csv')\n",
    "\n",
    "def filter_votes(data):\n",
    "    for proposal in data:\n",
    "        proposal['votes'] = [vote for vote in proposal['votes'] if float(vote['weight']) > 0]\n",
    "        \n",
    "    return data\n",
    "\n",
    "with open('data/proposals/aave/aave_v2_on_chain_proposals_1_year.json', 'r', encoding='utf8') as file:\n",
    "    aave_v2_proposals = json.load(file)\n",
    "with open('data/proposals/aave/aave_v3_on_chain_proposals_1_year.json', 'r', encoding='utf8') as file:\n",
    "    aave_v3_proposals = json.load(file)['data']['proposals']\n",
    "with open('data/proposals/aave/aave_snapshot_proposals_1_year.json', 'r', encoding='utf8') as file:\n",
    "    aave_snapshot_proposals = json.load(file)['data']['proposals']\n",
    "    \n",
    "with open('data/proposals/aave/aave_v2_on_chain_votes_1_year.json', 'r', encoding='utf8') as file:\n",
    "    aave_v2_votes = filter_votes(json.load(file))\n",
    "with open('data/proposals/aave/aave_v3_on_chain_votes_1_year.json', 'r', encoding='utf8') as file:\n",
    "    aave_v3_votes = filter_votes(json.load(file))\n",
    "with open('data/proposals/aave/aave_snapshot_votes_1_year.json', 'r', encoding='utf8') as file:\n",
    "    aave_snapshot_votes = json.load(file)\n",
    "    \n",
    "with open('data/proposals/aave/aave_delegations.json', 'r', encoding='utf8') as file:\n",
    "    delegations = json.load(file)\n",
    "    \n",
    "delegations = [\n",
    "    entry for entry in delegations\n",
    "    if 1696118399 < entry.get('timestamp', 0) < 1727740800\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAVE_tokens = ['AAVE', 'PolAAVE', 'ArbAAVE']\n",
    "aTokens = ['aEthWETH', 'aPolWETH', 'aArbWETH']\n",
    "# aTokens = ['aEthUSDC', 'aPolUSDC', 'aArbUSDC', 'aEthUSDT', 'aPolUSDT', 'aArbUSDT', 'aEthWETH', 'aPolWETH', 'aArbWETH']\n",
    "\n",
    "aave_df = transfers_df[\n",
    "    transfers_df['token'].isin(AAVE_tokens) &\n",
    "    (transfers_df['from'].str.lower() != transfers_df['to'].str.lower()) &\n",
    "    (transfers_df['value'] != 0)\n",
    "]\n",
    "aToken_df = transfers_df[\n",
    "    transfers_df['token'].isin(aTokens) &\n",
    "    (transfers_df['from'].str.lower() != transfers_df['to'].str.lower()) &\n",
    "    (transfers_df['value'] != 0)\n",
    "]\n",
    "\n",
    "# addresses = pd.read_csv('data/addresses/aave_addresses.csv')\n",
    "\n",
    "# EOA = set(addresses.loc[addresses['type'] == False, 'address'])\n",
    "# CA = set(addresses.loc[addresses['type'] == True, 'address'])\n",
    "\n",
    "# CEX_addresses = pd.read_csv('data/exchanges/cex.csv')\n",
    "# CEX = set(CEX_addresses['address'].str.lower())\n",
    "\n",
    "G_aave_nodes = set(aave_df['from'].str.lower()).union(set(aave_df['to'].str.lower()))\n",
    "G_aToken_nodes = set(aToken_df['from'].str.lower()).union(set(aToken_df['to'].str.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAVE GRAPH AGGREGATED BY TRANSFER VALUES\n",
    "\n",
    "G_aave = nx.DiGraph()\n",
    "\n",
    "for _, row in aave_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if G_aave.has_edge(from_address, to_address):\n",
    "        G_aave[from_address][to_address]['weight'] += value\n",
    "    else:\n",
    "        G_aave.add_edge(from_address, to_address, weight=value)\n",
    "\n",
    "largest_wcc_nodes = max(nx.weakly_connected_components(G_aave), key=len)\n",
    "G_aave = G_aave.subgraph(largest_wcc_nodes).copy()\n",
    "\n",
    "G_aave_nodes = set(G_aave.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAVE TWMDG\n",
    "\n",
    "G_aave_complete = nx.MultiDiGraph()\n",
    "\n",
    "for _, row in aave_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    timestamp = row['timestamp']\n",
    "    \n",
    "    G_aave_complete.add_edge(from_address, to_address, weight=value, timestamp=timestamp)\n",
    "    \n",
    "largest_wcc_nodes = max(nx.weakly_connected_components(G_aave_complete), key=len)\n",
    "G_aave_complete = G_aave_complete.subgraph(largest_wcc_nodes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAVE GRAPH AGGREGATED BY NUMBER OF TRANSFERS\n",
    "\n",
    "G_aave_count = nx.DiGraph()\n",
    "\n",
    "for _, row in aave_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    \n",
    "    if G_aave_count.has_edge(from_address, to_address):\n",
    "        G_aave_count[from_address][to_address]['weight'] += 1\n",
    "    else:\n",
    "        G_aave_count.add_edge(from_address, to_address, weight=1)\n",
    "        \n",
    "largest_wcc_nodes = max(nx.weakly_connected_components(G_aave_count), key=len)\n",
    "G_aave_count = G_aave_count.subgraph(largest_wcc_nodes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aWETH GRAPH AGGREGATED BY TRANSFER VALUES\n",
    "\n",
    "G_aWETH = nx.DiGraph()\n",
    "\n",
    "for _, row in aToken_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if G_aWETH.has_edge(from_address, to_address):\n",
    "        G_aWETH[from_address][to_address]['weight'] += value\n",
    "    else:\n",
    "        G_aWETH.add_edge(from_address, to_address, weight=value)\n",
    "        \n",
    "G_aToken_nodes = set(G_aWETH.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aWETH TWMDG\n",
    "\n",
    "G_aWETH_complete = nx.MultiDiGraph()\n",
    "\n",
    "for _, row in aToken_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    timestamp = row['timestamp']\n",
    "    \n",
    "    G_aWETH_complete.add_edge(from_address, to_address, weight=value, timestamp=timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Governance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_counts = {}\n",
    "for proposal in aave_v2_proposals:\n",
    "    proposer = proposal['user']['id']\n",
    "    if proposer in proposal_counts:\n",
    "        proposal_counts[proposer] += 1\n",
    "    else:\n",
    "        proposal_counts[proposer] = 1\n",
    "\n",
    "for proposal in aave_v3_proposals:\n",
    "    proposer = proposal['creator']\n",
    "    if proposer in proposal_counts:\n",
    "        proposal_counts[proposer] += 1\n",
    "    else:\n",
    "        proposal_counts[proposer] = 1\n",
    "\n",
    "vote_counts = {}\n",
    "for votes in aave_v2_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['id'].split('-')[0]\n",
    "        if voter in vote_counts:\n",
    "            vote_counts[voter] += 1\n",
    "        else:\n",
    "            vote_counts[voter] = 1\n",
    "\n",
    "for votes in aave_v3_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['voter'].lower()\n",
    "        if voter in vote_counts:\n",
    "            vote_counts[voter] += 1\n",
    "        else:\n",
    "            vote_counts[voter] = 1\n",
    "            \n",
    "vote_weights = {}\n",
    "for votes in aave_v2_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['id'].split('-')[0]\n",
    "        weight = vote['weight']\n",
    "        if voter in vote_weights:\n",
    "            vote_weights[voter] += float(weight)\n",
    "        else:\n",
    "            vote_weights[voter] = float(weight)\n",
    "            \n",
    "for votes in aave_v3_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['voter'].lower()\n",
    "        weight = vote['weight']\n",
    "        if voter in vote_weights:\n",
    "            vote_weights[voter] += float(weight)\n",
    "        else:\n",
    "            vote_weights[voter] = float(weight)\n",
    "            \n",
    "delegation_counts = {}\n",
    "from_delegations = {}\n",
    "to_delegations = {}\n",
    "\n",
    "for delegation in delegations:\n",
    "    delegator = delegation['from'].lower()\n",
    "    if delegator in delegation_counts:\n",
    "        delegation_counts[delegator] += 1\n",
    "    else:\n",
    "        delegation_counts[delegator] = 1\n",
    "        \n",
    "    if delegator in from_delegations:\n",
    "        from_delegations[delegator] += 1\n",
    "    else:\n",
    "        from_delegations[delegator] = 1\n",
    "                \n",
    "    delegate = delegation['to'].lower()\n",
    "    if delegate in delegation_counts:\n",
    "        delegation_counts[delegate] += 1\n",
    "    else: \n",
    "        delegation_counts[delegate] = 1\n",
    "        \n",
    "    if delegate in to_delegations:\n",
    "        to_delegations[delegate] += 1\n",
    "    else:\n",
    "        to_delegations[delegate] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_proposers = set()\n",
    "v3_proposers = set()\n",
    "snapshot_proposers = set()\n",
    "\n",
    "for proposal in aave_v2_proposals:\n",
    "    v2_proposers.add(proposal['user']['id'])\n",
    "for proposal in aave_v3_proposals:\n",
    "    v3_proposers.add(proposal['creator'])\n",
    "for proposal in aave_snapshot_proposals:\n",
    "    snapshot_proposers.add(proposal['author'].lower())\n",
    "    \n",
    "proposers = (v2_proposers | v3_proposers) & G_aave_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voter in vote_weights:\n",
    "    vote_weights[voter] = vote_weights[voter] / vote_counts[voter]\n",
    "    \n",
    "v2_voters = set()\n",
    "v3_voters = set()\n",
    "snapshot_voters = set()\n",
    "\n",
    "for votes in aave_v2_votes:\n",
    "    for vote in votes['votes']:\n",
    "        v2_voters.add(vote['id'].split('-')[0])\n",
    "            \n",
    "for votes in aave_v3_votes:\n",
    "    for vote in votes['votes']:\n",
    "        v3_voters.add(vote['voter'].lower())\n",
    "\n",
    "for votes in aave_snapshot_votes:\n",
    "    for vote in votes['votes']:\n",
    "        snapshot_voters.add(vote['voter'].lower())\n",
    "        \n",
    "voters = (v2_voters | v3_voters) & G_aave_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegators = set()\n",
    "delegates = set()\n",
    "\n",
    "from_delegations_without_same_to = set()\n",
    "to_delegations_without_same_from = set()\n",
    "\n",
    "for delegate in delegations:\n",
    "    delegator = delegate['from'].lower()\n",
    "    delegate = delegate['to'].lower()\n",
    "    \n",
    "    delegators.add(delegator)\n",
    "    delegates.add(delegate)\n",
    "    \n",
    "    if delegator != delegate:\n",
    "        from_delegations_without_same_to.add(delegator)\n",
    "        to_delegations_without_same_from.add(delegate)    \n",
    "    \n",
    "all_delegations = (delegators | delegates) & G_aave_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_voters_in_G_aave = (v2_voters | v3_voters) & G_aave_nodes\n",
    "all_proposers_voters_delegators_in_G_aave = (v2_proposers | v3_proposers | v2_voters | v3_voters | delegators) & G_aave_nodes\n",
    "all_economic_users_in_G_aave = (set(aToken_df['from'].str.lower()) | set(aToken_df['to'].str.lower())) & G_aave_nodes\n",
    "\n",
    "only_economic_users_in_G_aave = all_economic_users_in_G_aave - all_voters_in_G_aave\n",
    "only_voters_in_G_aave = all_voters_in_G_aave - all_economic_users_in_G_aave\n",
    "both_economic_and_governance_users = all_economic_users_in_G_aave & all_voters_in_G_aave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_proposers_voters_delegators_in_G_aave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total: {len(G_aave_nodes)}\\nProposers: {len(proposers)}\\nVoters: {len(voters)}\\nDelegations: {len(all_delegations)}\\nGovernance: {len(only_voters_in_G_aave)}\\nEconomic: {len(only_economic_users_in_G_aave)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual feature computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_aWETH_igraph = ig.Graph.from_networkx(G_aWETH, vertex_attr_hashable='name')\n",
    "G_aave_igraph = ig.Graph.from_networkx(G_aave, vertex_attr_hashable='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aWETH_igraph_in_dc = [d for d in G_aWETH_igraph.indegree()]\n",
    "aWETH_igraph_out_dc = [d for d in G_aWETH_igraph.outdegree()]\n",
    "\n",
    "aWETH_igraph_in_dc = {G_aWETH_igraph.vs[i][\"name\"]: score for i, score in enumerate(aWETH_igraph_in_dc)}\n",
    "aWETH_igraph_out_dc = {G_aWETH_igraph.vs[i][\"name\"]: score for i, score in enumerate(aWETH_igraph_out_dc)}\n",
    "\n",
    "aave_igraph_in_dc = [d for d in G_aave_igraph.indegree()]\n",
    "aave_igraph_out_dc = [d for d in G_aave_igraph.outdegree()]\n",
    "\n",
    "aave_igraph_in_dc = {G_aave_igraph.vs[i][\"name\"]: score for i, score in enumerate(aave_igraph_in_dc)}\n",
    "aave_igraph_out_dc = {G_aave_igraph.vs[i][\"name\"]: score for i, score in enumerate(aave_igraph_out_dc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigenvector_manual(graph, tol=1e-6, max_iter=100):    \n",
    "    n = graph.vcount()\n",
    "    edges = np.array(graph.get_edgelist())\n",
    "    weights = np.array(graph.es[\"weight\"])\n",
    "\n",
    "    row, col = edges[:, 1], edges[:, 0]\n",
    "    W = sp.csr_matrix((weights, (row, col)), shape=(n, n))\n",
    "\n",
    "    x = np.ones(n)\n",
    "    for _ in range(max_iter):\n",
    "        x_new = W @ x\n",
    "        x_new /= np.linalg.norm(x_new, ord=2)\n",
    "        \n",
    "        if np.linalg.norm(x_new - x, ord=2) / np.linalg.norm(x, ord=2) < tol:\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "\n",
    "    return {graph.vs[i][\"name\"]: x[i] for i in range(n)}\n",
    "\n",
    "aWETH_igraph_ec = eigenvector_manual(G_aWETH_igraph)\n",
    "aave_igraph_ec = eigenvector_manual(G_aave_igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_manual(graph):\n",
    "    n = graph.vcount()\n",
    "    \n",
    "    W_sparse = graph.get_adjacency_sparse(attribute='weight').tocsc()\n",
    "    W13 = W_sparse.power(1/3)\n",
    "    W13_T = W13.transpose()\n",
    "    \n",
    "    W_sum = W13 + W13_T\n",
    "    W_sum.data **= 3\n",
    "    numerator = np.array(W_sum.sum(axis=1)).flatten()\n",
    "    \n",
    "    d_out = np.array(graph.outdegree())\n",
    "    d_in = np.array(graph.indegree())\n",
    "    d_tot = d_out + d_in\n",
    "    \n",
    "    A_sparse = graph.get_adjacency_sparse().tocsc()\n",
    "    mutual_edges = np.array(A_sparse.multiply(A_sparse.T).sum(axis=1)).flatten()\n",
    "    \n",
    "    denominator = 2 * (d_tot * (d_tot - 1) - 2 * mutual_edges)\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        C = np.where(denominator > 0, numerator / denominator, 0)\n",
    "    \n",
    "    return {graph.vs[i][\"name\"]: C[i] for i in range(n)}\n",
    "\n",
    "aWETH_igraph_cc = clustering_manual(G_aWETH_igraph)\n",
    "aave_igraph_cc = clustering_manual(G_aave_igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-hop neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_hop_weight_sum_ig(G):\n",
    "    weight_sums = [0] * G.vcount()\n",
    "\n",
    "    for node in range(G.vcount()):\n",
    "        visited = set()\n",
    "        total_weight = 0\n",
    "\n",
    "        neighbors = set(G.neighborhood(node, order=1, mode='ALL'))\n",
    "\n",
    "        for n in neighbors:\n",
    "            for e in G.incident(n, mode='ALL'):\n",
    "                if e not in visited:\n",
    "                    total_weight += G.es[e]['weight']\n",
    "                    visited.add(e)\n",
    "\n",
    "        weight_sums[node] = total_weight\n",
    "\n",
    "    return {G.vs[i][\"name\"]: weight_sums[i] for i in range(G.vcount())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aave_igraph_2_hop_weights = two_hop_weight_sum_ig(G_aave_igraph)\n",
    "\n",
    "with open('centrality_scores/AAVE_2_hop_weights.json', 'w') as f:\n",
    "    json.dump(aave_igraph_2_hop_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('centrality_scores/AAVE_2_hop_weights.json', 'r', encoding='utf8') as f:\n",
    "    aave_igraph_2_hop_weights = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burstiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_burstiness(group):\n",
    "    if len(group) < 2:\n",
    "        return None\n",
    "    \n",
    "    sigma = group['iet'].std()\n",
    "    mean = group['iet'].mean()\n",
    "    \n",
    "    return sigma / mean if mean > 0 else 0\n",
    "\n",
    "def construct_burstiness_dictionaries(df):\n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].astype('int64'), unit='s')\n",
    "    \n",
    "    # df_sorted_to = df.sort_values(by=['to', 'timestamp'])\n",
    "    # df_sorted_to['iet'] = df_sorted_to.groupby('to')['timestamp'].diff().dt.total_seconds()\n",
    "    # burstiness_to_df = df_sorted_to.dropna().groupby('to').apply(compute_burstiness).reset_index(name='burstiness_in')\n",
    "    # burstiness_to = burstiness_to_df.dropna().set_index('to')['burstiness_in'].to_dict()\n",
    "\n",
    "    # df_sorted_from = df.sort_values(by=['from', 'timestamp'])\n",
    "    # df_sorted_from['iet'] = df_sorted_from.groupby('from')['timestamp'].diff().dt.total_seconds()\n",
    "    # burstiness_from_df = df_sorted_from.dropna().groupby('from').apply(compute_burstiness).reset_index(name='burstiness_out')\n",
    "    # burstiness_from = burstiness_from_df.dropna().set_index('from')['burstiness_out'].to_dict()\n",
    "\n",
    "    df_combined = df.melt(id_vars=['timestamp'], value_vars=['from', 'to'], var_name='direction', value_name='address')\n",
    "    df_combined = df_combined.sort_values(by=['address', 'timestamp'])\n",
    "    df_combined['iet'] = df_combined.groupby('address')['timestamp'].diff().dt.total_seconds()\n",
    "    grouped = df_combined.dropna().groupby('address')\n",
    "    burstiness_total_df = grouped[['iet']].apply(compute_burstiness).reset_index(name='burstiness_total')\n",
    "    burstiness_total = burstiness_total_df.dropna().set_index('address')['burstiness_total'].to_dict()\n",
    "    \n",
    "    return burstiness_total #, burstiness_to, burstiness_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aave_burstiness_total, aave_burstiness_to, aave_burstiness_from = construct_burstiness_dictionaries(aave_df)\n",
    "# aWETH_burstiness_total, aWETH_burstiness_to, aWETH_burstiness_from = construct_burstiness_dictionaries(aToken_df)\n",
    "\n",
    "aave_burstiness_total = construct_burstiness_dictionaries(aave_df)\n",
    "aWETH_burstiness_total = construct_burstiness_dictionaries(aToken_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betweenness & closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_twmdg_cc_nodes = max(nx.weakly_connected_components(G_aave_complete), key=len)\n",
    "twmdg_cc_subgraph = G_aave_complete.subgraph(largest_twmdg_cc_nodes).copy()\n",
    "\n",
    "print(f\"Largest Weakly Connected Component: {twmdg_cc_subgraph.number_of_nodes()} nodes, {twmdg_cc_subgraph.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G_aave_complete.number_of_nodes(), G_aave_complete.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_agg_cc_nodes = max(nx.weakly_connected_components(G_aave), key=len)\n",
    "agg_cc_subgraph = G_aave.subgraph(largest_agg_cc_nodes).copy()\n",
    "\n",
    "print(f\"Largest Weakly Connected Component: {agg_cc_subgraph.number_of_nodes()} nodes, {agg_cc_subgraph.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_aave_count_igraph = ig.Graph.from_networkx(G_aave_count, vertex_attr_hashable='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_aave_count_igraph_components = G_aave_count_igraph.connected_components(mode=\"weak\")\n",
    "G_aave_count_igraph_largest_wcc = G_aave_count_igraph_components.giant()\n",
    "\n",
    "print(G_aave_count_igraph_largest_wcc.vcount(), G_aave_count_igraph_largest_wcc.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_aave_count_igraph_closeness = G_aave_count_igraph_largest_wcc.closeness(mode=\"OUT\", weights=\"weight\", normalized=True)\n",
    "\n",
    "G_aave_count_igraph_closeness2 = {G_aave_count_igraph_largest_wcc.vs[i][\"name\"]: score for i, score in enumerate(G_aave_count_igraph_closeness)}\n",
    "\n",
    "# with open('centrality_scores/AAVE_count_closeness.json', 'w') as f:\n",
    "#     json.dump(G_aave_count_igraph_closeness2, f)\n",
    "\n",
    "with open('centrality_scores/AAVE_count_closeness_2.json', 'w') as f:\n",
    "    json.dump(G_aave_count_igraph_closeness2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('centrality_scores/AAVE_count_closeness.json', 'r', encoding='utf8') as f:\n",
    "    G_aave_count_igraph_closeness = json.load(f)\n",
    "    \n",
    "with open('centrality_scores/AAVE_count_closeness_2.json', 'r', encoding='utf8') as f:\n",
    "    G_aave_count_igraph_closeness_2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_aave_count_igraph_betweenness = G_aave_count_igraph_largest_wcc.betweenness(directed=True, weights=\"weight\")\n",
    "\n",
    "G_aave_count_igraph_betweenness2 = {G_aave_count_igraph_largest_wcc.vs[i][\"name\"]: score for i, score in enumerate(G_aave_count_igraph_betweenness)}\n",
    "\n",
    "with open('centrality_scores/AAVE_count_betweenness.json', 'w') as f:\n",
    "    json.dump(G_aave_count_igraph_betweenness2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('centrality_scores/AAVE_count_betweenness.json', 'r', encoding='utf8') as f:\n",
    "    G_aave_count_igraph_betweenness = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = {k: v for k, v in G_aave_count_igraph_closeness.items() if not math.isnan(v)}\n",
    "sorted(cleaned.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_closeness(closeness_centrality, num_nodes):\n",
    "    return {node: score * (num_nodes - 1) for node, score in closeness_centrality.items()}\n",
    "\n",
    "def normalize_betweenness(betweenness_centrality, num_nodes, directed=True):\n",
    "    if directed:\n",
    "        normalization_factor = (num_nodes - 1) * (num_nodes - 2) / 2\n",
    "    else:\n",
    "        normalization_factor = (num_nodes - 1) * (num_nodes - 2)\n",
    "    return {node: score / normalization_factor for node, score in betweenness_centrality.items()}\n",
    "\n",
    "num_nodes = G_aave_count_igraph_largest_wcc.vcount()\n",
    "normalized_closeness = normalize_closeness(G_aave_count_igraph_closeness_2, num_nodes)\n",
    "# normalized_betweenness = normalize_betweenness(G_aave_count_igraph_betweenness2, num_nodes, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_closeness = {k: v for k, v in G_aave_count_igraph_closeness.items() if not math.isnan(v)}\n",
    "\n",
    "sorted(cleaned_closeness.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_bins = np.logspace(np.log10(min(G_aave_count_igraph_closeness2.values())), np.log10(max(G_aave_count_igraph_closeness2.values())), 50)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(G_aave_count_igraph_closeness2.values(), bins=log_bins)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Closeness centrality')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Closeness centrality distribution')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centrality_distribution(centrality_scores, title, xlabel):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(centrality_scores.values(), bins=50)#, edgecolor='black')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "plot_centrality_distribution(G_aave_count_igraph_closeness2, \n",
    "                              'Closeness centrality distribution', \n",
    "                              'Closeness centrality')\n",
    "\n",
    "plot_centrality_distribution(G_aave_count_igraph_betweenness2, \n",
    "                              'Betweenness centrality distribution', \n",
    "                              'Betweenness centrality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing only the overlapping nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_AAVE_transfers = aave_df.melt(value_vars=['from', 'to'], id_vars=['value'], value_name='address')\n",
    "average_AAVE_tokens = all_AAVE_transfers.groupby('address')['value'].mean().to_dict()\n",
    "AAVE_transfer_counts = all_AAVE_transfers['address'].value_counts().to_dict()\n",
    "\n",
    "all_aToken_transfers = aToken_df.melt(value_vars=['from', 'to'], id_vars=['value'], value_name='address')\n",
    "average_aWETH_tokens = all_aToken_transfers.groupby('address')['value'].mean().to_dict()\n",
    "aWETH_transfer_counts = all_aToken_transfers['address'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_AAVE_users = pd.concat([aave_df[['from', 'value']].rename(columns={'from': 'user'}), aave_df[['to', 'value']].rename(columns={'to': 'user'})])\n",
    "\n",
    "avg_all_AAVE_transfers = all_AAVE_users.groupby('user')['value'].mean().to_dict()\n",
    "avg_outgoing_AAVE = aave_df.groupby('from')['value'].mean().to_dict()\n",
    "avg_incoming_AAVE = aave_df.groupby('to')['value'].mean().to_dict()\n",
    "\n",
    "\n",
    "all_aWETH_users = pd.concat([aToken_df[['from', 'value']].rename(columns={'from': 'user'}), aToken_df[['to', 'value']].rename(columns={'to': 'user'})])\n",
    "\n",
    "avg_all_aWETH_transfers = all_aWETH_users.groupby('user')['value'].mean().to_dict()\n",
    "avg_outgoing_aWETH = aToken_df.groupby('from')['value'].mean().to_dict()\n",
    "avg_incoming_aWETH = aToken_df.groupby('to')['value'].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_mapping = {}\n",
    "\n",
    "for proposer, count in proposal_counts.items():\n",
    "    proposals_mapping[proposer] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_from_AAVE_transferred_mapping = avg_outgoing_AAVE\n",
    "average_to_AAVE_transferred_mapping = avg_incoming_AAVE\n",
    "\n",
    "average_from_aWETH_transferred_mapping = avg_outgoing_aWETH\n",
    "average_to_aWETH_transferred_mapping = avg_incoming_aWETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weights = {}\n",
    "for votes in aave_v2_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['id'].split('-')[0]\n",
    "        weight = vote['weight']\n",
    "        if voter in vote_weights:\n",
    "            vote_weights[voter] += float(weight)\n",
    "        else:\n",
    "            vote_weights[voter] = float(weight)\n",
    "            \n",
    "for votes in aave_v3_votes:\n",
    "    for vote in votes['votes']:\n",
    "        voter = vote['voter'].lower()\n",
    "        weight = vote['weight']\n",
    "        if voter in vote_weights:\n",
    "            vote_weights[voter] += float(weight)\n",
    "        else:\n",
    "            vote_weights[voter] = float(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_AAVE_transferred_mapping = {}\n",
    "# from_AAVE_transferred_mapping = {}\n",
    "# to_AAVE_transferred_mapping = {}\n",
    "average_AAVE_transferred_mapping = avg_all_AAVE_transfers\n",
    "# average_from_AAVE_transferred_mapping = avg_outgoing_AAVE\n",
    "# average_to_AAVE_transferred_mapping = avg_incoming_AAVE\n",
    "number_of_AAVE_transfers_mapping = AAVE_transfer_counts\n",
    "\n",
    "proposals_mapping = {}\n",
    "votes_casted_mapping = {}\n",
    "from_delegations_mapping = {}\n",
    "to_delegations_mapping = {}\n",
    "average_vote_weights_mapping = {}\n",
    "\n",
    "total_aWETH_transferred_mapping = {}\n",
    "# from_aWETH_transferred_mapping = {}\n",
    "# to_aWETH_transferred_mapping = {}\n",
    "average_aWETH_transferred_mapping = avg_all_aWETH_transfers\n",
    "# average_from_aWETH_transferred_mapping = avg_outgoing_aWETH\n",
    "# average_to_aWETH_transferred_mapping = avg_incoming_aWETH\n",
    "number_of_aWETH_transfers_mapping = aWETH_transfer_counts\n",
    "\n",
    "for _, row in aave_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if from_address in total_AAVE_transferred_mapping:\n",
    "        total_AAVE_transferred_mapping[from_address] += value\n",
    "    else:\n",
    "        total_AAVE_transferred_mapping[from_address] = value\n",
    "        \n",
    "    # if from_address in from_AAVE_transferred_mapping:\n",
    "    #     from_AAVE_transferred_mapping[from_address] += value\n",
    "    # else:\n",
    "    #     from_AAVE_transferred_mapping[from_address] = value\n",
    "    \n",
    "    if to_address in total_AAVE_transferred_mapping:\n",
    "        total_AAVE_transferred_mapping[to_address] += value\n",
    "    else:\n",
    "        total_AAVE_transferred_mapping[to_address] = value\n",
    "        \n",
    "    # if to_address in to_AAVE_transferred_mapping:\n",
    "    #     to_AAVE_transferred_mapping[to_address] += value\n",
    "    # else:\n",
    "    #     to_AAVE_transferred_mapping[to_address] = value\n",
    "        \n",
    "        \n",
    "for user, average in average_AAVE_tokens.items():\n",
    "    if user in average_AAVE_transferred_mapping:\n",
    "        average_AAVE_transferred_mapping[user] = average\n",
    "        \n",
    "# for user, count in AAVE_transfer_counts.items():\n",
    "#     if user in number_of_AAVE_transfers_mapping:\n",
    "#         number_of_AAVE_transfers_mapping[user] = count\n",
    "\n",
    "for proposer, count in proposal_counts.items():\n",
    "    proposals_mapping[proposer] = count\n",
    "\n",
    "for voter, count in vote_counts.items():\n",
    "    votes_casted_mapping[voter] = count\n",
    "\n",
    "for delegator, count in from_delegations.items():\n",
    "    from_delegations_mapping[delegator] = count\n",
    "\n",
    "for delegate, count in to_delegations.items():\n",
    "    to_delegations_mapping[delegate] = count\n",
    "\n",
    "for voter, weight in vote_weights.items():\n",
    "    average_vote_weights_mapping[voter] = weight\n",
    "\n",
    "\n",
    "for _, row in aToken_df.iterrows():\n",
    "    from_address = row['from'].lower()\n",
    "    to_address = row['to'].lower()\n",
    "    value = row['value']\n",
    "    \n",
    "    if from_address in total_aWETH_transferred_mapping:\n",
    "        total_aWETH_transferred_mapping[from_address] += value\n",
    "    else:\n",
    "        total_aWETH_transferred_mapping[from_address] = value\n",
    "        \n",
    "    # if from_address in from_aWETH_transferred_mapping:\n",
    "    #     from_aWETH_transferred_mapping[from_address] += value\n",
    "    # else:\n",
    "    #     from_aWETH_transferred_mapping[from_address] = value\n",
    "        \n",
    "    if to_address in total_aWETH_transferred_mapping:\n",
    "        total_aWETH_transferred_mapping[to_address] += value\n",
    "    else:\n",
    "        total_aWETH_transferred_mapping[to_address] = value\n",
    "        \n",
    "    # if to_address in to_aWETH_transferred_mapping:\n",
    "    #     to_aWETH_transferred_mapping[to_address] += value\n",
    "    # else:\n",
    "    #     to_aWETH_transferred_mapping[to_address] = value\n",
    "\n",
    "for user, average in average_aWETH_tokens.items():\n",
    "    if user in average_aWETH_transferred_mapping:\n",
    "        average_aWETH_transferred_mapping[user] = average\n",
    "        \n",
    "# for user, count in aWETH_transfer_counts.items():\n",
    "#     if user in number_of_aWETH_transfers_mapping:\n",
    "#         number_of_aWETH_transfers_mapping[user] = count\n",
    "\n",
    "# addresses = sorted(average_AAVE_transferred_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlapping_correlations(x_name, x_map, y_name, y_map, results):\n",
    "    valid_addresses = [address for address in x_map.keys() & y_map.keys()]\n",
    "    \n",
    "    scores_x = [x_map[address] for address in valid_addresses]\n",
    "    scores_y = [y_map[address] for address in valid_addresses]\n",
    "    \n",
    "    spearman = spearmanr(scores_x, scores_y)\n",
    "    kendall = kendalltau(scores_x, scores_y)\n",
    "    \n",
    "    results.append({\n",
    "        'x_name': x_name,\n",
    "        'y_name': y_name,\n",
    "        'spearman': spearman,\n",
    "        'kendall': kendall,\n",
    "        'valid_addresses': valid_addresses\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING THE SCORES AND METRICS WITH OWN COMPUTATIONS INSTEAD OF BUILT-IN FUNCTIONS\n",
    "AAVE_mappings = [\n",
    "    ('AAVE In-Degree Centrality', aave_igraph_in_dc),\n",
    "    ('AAVE Out-Degree Centrality', aave_igraph_out_dc),\n",
    "    ('AAVE Eigenvector Centrality', aave_igraph_ec),\n",
    "    ('AAVE Clustering Coefficient', aave_igraph_cc),\n",
    "    # ('Total AAVE transferred', total_AAVE_transferred_mapping),\n",
    "    # ('Total AAVE sent', from_AAVE_transferred_mapping),\n",
    "    # ('Total AAVE received', to_AAVE_transferred_mapping),\n",
    "    ('AAVE Avg. per Transfer', average_AAVE_transferred_mapping),\n",
    "    # ('Average AAVE sent per transfer', average_from_AAVE_transferred_mapping),\n",
    "    # ('Average AAVE received per transfer', average_to_AAVE_transferred_mapping),\n",
    "    ('AAVE Transfer Count', number_of_AAVE_transfers_mapping),\n",
    "    ('AAVE 2-Hop Weight Sum', aave_igraph_2_hop_weights),\n",
    "    ('AAVE Burstiness', aave_burstiness_total)\n",
    "]\n",
    "\n",
    "gov_mappings = [\n",
    "    # ('Proposals Made', proposals_mapping),\n",
    "    ('Votes Cast', votes_casted_mapping),\n",
    "    # ('Delegations Given', from_delegations_mapping),\n",
    "    ('Delegations Received', to_delegations_mapping),\n",
    "    ('Avg. Vote Weight', average_vote_weights_mapping)\n",
    "]    \n",
    "\n",
    "aWETH_mappings = [\n",
    "    ('aWETH In-Degree Centrality', aWETH_igraph_in_dc),\n",
    "    ('aWETH Out-Degree Centrality', aWETH_igraph_out_dc),\n",
    "    # ('aWETH Eigenvector Centrality', aWETH_igraph_ec),\n",
    "    # ('aWETH Clustering Coefficient', aWETH_igraph_cc),\n",
    "    # ('Total aWETH transferred', total_aWETH_transferred_mapping),\n",
    "    # ('Total aWETH sent', from_aWETH_transferred_mapping),\n",
    "    # ('Total aWETH received', to_aWETH_transferred_mapping),\n",
    "    ('aWETH Avg. per Transfer', average_aWETH_transferred_mapping),\n",
    "    # ('Average aWETH sent per transfer', average_from_aWETH_transferred_mapping),\n",
    "    # ('Average aWETH received per transfer', average_to_aWETH_transferred_mapping),\n",
    "    ('aWETH Transfer Count', number_of_aWETH_transfers_mapping),\n",
    "    ('aWETH Burstiness', aWETH_burstiness_total)\n",
    "]\n",
    "\n",
    "mapping_dict = dict(AAVE_mappings) | dict(gov_mappings) | dict(aWETH_mappings)\n",
    "\n",
    "# nonzero_results = []\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in itertools.product(AAVE_mappings, gov_mappings):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in itertools.product(AAVE_mappings, aWETH_mappings):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in itertools.product(aWETH_mappings, gov_mappings):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "# #####\n",
    "\n",
    "aave_gov_results = []\n",
    "aave_aWETH_results = []\n",
    "aWETH_gov_results = []\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(AAVE_mappings, gov_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, aave_gov_results)\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(AAVE_mappings, aWETH_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, aave_aWETH_results)\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(aWETH_mappings, gov_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, aWETH_gov_results)\n",
    "\n",
    "\n",
    "# threshold = 0.75\n",
    "\n",
    "# for res in nonzero_results:\n",
    "#     kendall = res['kendall']\n",
    "    \n",
    "#     if kendall.statistic > threshold:\n",
    "#         valid_addresses = res['valid_addresses']\n",
    "#         percentage_of_addresses = (len(valid_addresses)/len(G_aave_nodes)) * 100\n",
    "        \n",
    "#         print(f'{res[\"x_name\"]} vs {res[\"y_name\"]} -> Kendall: {kendall.statistic:.3f}, number of addresses: {len(valid_addresses)}, percentage of addresses: {percentage_of_addresses:.2f}%')\n",
    "#         print()\n",
    "        \n",
    "#         x_map = mapping_dict.get(res['x_name'])\n",
    "#         y_map = mapping_dict.get(res['y_name'])\n",
    "\n",
    "#         x_vals = [x_map[address] for address in valid_addresses]\n",
    "#         y_vals = [y_map[address] for address in valid_addresses]\n",
    "        \n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "#         axes[0].scatter(x_vals, y_vals, alpha=0.6)\n",
    "#         axes[0].set_xlabel(res['x_name'])\n",
    "#         axes[0].set_ylabel(res['y_name'])\n",
    "#         axes[0].set_title('Original scale')\n",
    "\n",
    "#         # log_x = np.log(x_vals)\n",
    "#         # log_y = np.log(y_vals)\n",
    "#         # sns.regplot(x=log_x, y=log_y, ax=axes[1])\n",
    "#         # axes[1].set_xlabel(f'Log({res[\"x_name\"]})')\n",
    "#         # axes[1].set_ylabel(f'Log({res[\"y_name\"]})')\n",
    "#         # axes[1].set_title('Log-Log Scale')\n",
    "#         ####\n",
    "#         axes[1].scatter(x_vals, y_vals, alpha=0.6)\n",
    "#         axes[1].set_xscale('log')\n",
    "#         axes[1].set_yscale('log')\n",
    "#         axes[1].set_xlabel(res['x_name'])\n",
    "#         axes[1].set_ylabel(res['y_name'])\n",
    "#         axes[1].set_title('Log-scaled axes')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/AAVE_mappings.json', 'w') as f:\n",
    "    json.dump({name: mapping for name, mapping in AAVE_mappings}, f)\n",
    "\n",
    "with open('data/gov_mappings.json', 'w') as f:\n",
    "    json.dump({name: mapping for name, mapping in gov_mappings}, f)\n",
    "\n",
    "with open('data/aWETH_mappings.json', 'w') as f:\n",
    "    json.dump({name: mapping for name, mapping in aWETH_mappings}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/AAVE_mappings.json', 'r') as f:\n",
    "    AAVE_mappings = [(name, mapping) for name, mapping in json.load(f).items()]\n",
    "\n",
    "with open('data/gov_mappings.json', 'r') as f:\n",
    "    gov_mappings = [(name, mapping) for name, mapping in json.load(f).items()]\n",
    "\n",
    "with open('data/aWETH_mappings.json', 'r') as f:\n",
    "    aWETH_mappings = [(name, mapping) for name, mapping in json.load(f).items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/aave/SI_5_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_5_extracted = pickle.load(f)\n",
    "\n",
    "with open('data/SI/aave/SI_2_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_2_extracted = pickle.load(f)\n",
    "\n",
    "with open('data/SI/aave/SI_1_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_1_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_05_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_05_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_01_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_01_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_001_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_001_extracted = pickle.load(f)\n",
    "    \n",
    "SI_001_mapping = {i['seed']: i['avg_infected'] for i in SI_001_extracted}\n",
    "SI_01_mapping = {i['seed']: i['avg_infected'] for i in SI_01_extracted}\n",
    "SI_05_mapping = {i['seed']: i['avg_infected'] for i in SI_05_extracted}\n",
    "SI_1_mapping = {i['seed']: i['avg_infected'] for i in SI_1_extracted}\n",
    "SI_2_mapping = {i['seed']: i['avg_infected'] for i in SI_2_extracted}\n",
    "SI_5_mapping = {i['seed']: i['avg_infected'] for i in SI_5_extracted}\n",
    "\n",
    "SI_mappings = [\n",
    "    ('SI 0.001', SI_001_mapping),\n",
    "    ('SI 0.01', SI_01_mapping),\n",
    "    ('SI 0.05', SI_05_mapping),\n",
    "    ('SI 1', SI_1_mapping),\n",
    "    ('SI 2', SI_2_mapping),\n",
    "    ('SI 5', SI_5_mapping)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_si_results = []\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in itertools.product(gov_mappings, SI_mappings):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, gov_si_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING CORRELATIONS WITHIN THE DATASETS\n",
    "AAVE_mappings = [\n",
    "    ('AAVE In-Degree Centrality', aave_igraph_in_dc),\n",
    "    ('AAVE Out-Degree Centrality', aave_igraph_out_dc),\n",
    "    ('AAVE Eigenvector Centrality', aave_igraph_ec),\n",
    "    ('AAVE Clustering Coefficient', aave_igraph_cc),\n",
    "    # ('Total AAVE transferred', total_AAVE_transferred_mapping),\n",
    "    # ('Total AAVE sent', from_AAVE_transferred_mapping),\n",
    "    # ('Total AAVE received', to_AAVE_transferred_mapping),\n",
    "    ('AAVE Avg. per Transfer', average_AAVE_transferred_mapping),\n",
    "    # ('Average AAVE sent per transfer', average_from_AAVE_transferred_mapping),\n",
    "    # ('Average AAVE received per transfer', average_to_AAVE_transferred_mapping),\n",
    "    ('AAVE Transfer Count', number_of_AAVE_transfers_mapping),\n",
    "    ('AAVE 2-Hop Weight Sum', aave_igraph_2_hop_weights),\n",
    "    ('AAVE Burstiness', aave_burstiness_total)\n",
    "]\n",
    "\n",
    "gov_mappings = [\n",
    "    ('Votes Cast', votes_casted_mapping),\n",
    "    ('Delegations Given', from_delegations_mapping),\n",
    "    ('Delegations Received', to_delegations_mapping),\n",
    "    ('Avg. Vote Weight', average_vote_weights_mapping)\n",
    "]    \n",
    "\n",
    "aWETH_mappings = [\n",
    "    ('aWETH In-Degree Centrality', aWETH_igraph_in_dc),\n",
    "    ('aWETH Out-Degree Centrality', aWETH_igraph_out_dc),\n",
    "    # ('aWETH Eigenvector Centrality', aWETH_igraph_ec),\n",
    "    # ('aWETH Clustering Coefficient', aWETH_igraph_cc),\n",
    "    # ('Total aWETH transferred', total_aWETH_transferred_mapping),\n",
    "    # ('Total aWETH sent', from_aWETH_transferred_mapping),\n",
    "    # ('Total aWETH received', to_aWETH_transferred_mapping),\n",
    "    ('aWETH Avg. per Transfer', average_aWETH_transferred_mapping),\n",
    "    # ('Average aWETH sent per transfer', average_from_aWETH_transferred_mapping),\n",
    "    # ('Average aWETH received per transfer', average_to_aWETH_transferred_mapping),\n",
    "    ('aWETH Transfer Count', number_of_aWETH_transfers_mapping),\n",
    "    ('aWETH Burstiness', aWETH_burstiness_total)\n",
    "]\n",
    "\n",
    "mapping_dict = dict(AAVE_mappings) | dict(gov_mappings) | dict(aWETH_mappings)\n",
    "\n",
    "# nonzero_results = []\n",
    "\n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(AAVE_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(aWETH_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(gov_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, nonzero_results)\n",
    "\n",
    "#####\n",
    "\n",
    "aave_results = []\n",
    "aWETH_results = []\n",
    "gov_results = []\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in list(combinations(AAVE_mappings, 2)):\n",
    "    compute_overlapping_correlations(x_name, x_map, y_name, y_map, aave_results)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(aWETH_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, aWETH_results)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(gov_mappings, 2)):\n",
    "#     compute_overlapping_correlations(x_name, x_map, y_name, y_map, gov_results)\n",
    "\n",
    "# threshold = 0.9\n",
    "\n",
    "# for res in nonzero_results:\n",
    "#     kendall = res['kendall']\n",
    "    \n",
    "#     if kendall.statistic > threshold:\n",
    "#         valid_addresses = res['valid_addresses']\n",
    "        \n",
    "#         print(f'{res[\"x_name\"]} vs {res[\"y_name\"]} -> Kendall: {kendall.statistic:.3f}, number of addresses: {len(valid_addresses)}')\n",
    "#         # print()\n",
    "        \n",
    "#         # x_map = mapping_dict.get(res['x_name'])\n",
    "#         # y_map = mapping_dict.get(res['y_name'])\n",
    "        \n",
    "#         # plt.figure(figsize=(10, 6))\n",
    "#         # plt.scatter([x_map[address] for address in valid_addresses], [y_map[address] for address in valid_addresses], alpha=0.6)\n",
    "#         # plt.xlabel(res['x_name'])\n",
    "#         # plt.ylabel(res['y_name'])\n",
    "#         # plt.show()\n",
    "        \n",
    "#         # log_x = np.log1p([x_map[address] for address in valid_addresses])\n",
    "#         # log_y = np.log1p([y_map[address] for address in valid_addresses])\n",
    "\n",
    "#         # sns.regplot(x=log_x, y=log_y)\n",
    "#         # plt.xlabel(f'Log({res[\"x_name\"]})')\n",
    "#         # plt.ylabel(f'Log({res[\"y_name\"]})')\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(results):\n",
    "    def unique_ordered(items):\n",
    "        seen = set()\n",
    "        ordered = []\n",
    "        for item in items:\n",
    "            if item not in seen:\n",
    "                seen.add(item)\n",
    "                ordered.append(item)\n",
    "        return ordered\n",
    "\n",
    "    x_datasets = unique_ordered([entry['x_name'] for entry in results])\n",
    "    y_datasets = unique_ordered([entry['y_name'] for entry in results])\n",
    "\n",
    "    # spearman_corr_matrix = np.zeros((len(x_datasets), len(y_datasets)))\n",
    "    kendall_corr_matrix = np.zeros((len(x_datasets), len(y_datasets)))\n",
    "\n",
    "    for entry in results:\n",
    "        x_idx = x_datasets.index(entry['x_name'])\n",
    "        y_idx = y_datasets.index(entry['y_name'])\n",
    "        # spearman_corr_matrix[x_idx, y_idx] = entry['spearman'].statistic\n",
    "        kendall_corr_matrix[x_idx, y_idx] = entry['kendall'].statistic\n",
    "\n",
    "    # spearman_corr_df = pd.DataFrame(spearman_corr_matrix, index=x_datasets, columns=y_datasets)\n",
    "    kendall_corr_df = pd.DataFrame(kendall_corr_matrix, index=x_datasets, columns=y_datasets)\n",
    "\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(spearman_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Spearman correlation\")\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(kendall_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Kendall correlation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(aave_gov_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(aave_aWETH_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(aWETH_gov_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap(gov_si_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lower_triangle_heatmap(results):\n",
    "    def unique_ordered(items):\n",
    "        seen = set()\n",
    "        ordered = []\n",
    "        for item in items:\n",
    "            if item not in seen:\n",
    "                seen.add(item)\n",
    "                ordered.append(item)\n",
    "        return ordered\n",
    "\n",
    "    feature_names = unique_ordered(\n",
    "        [entry['x_name'] for entry in results] + [entry['y_name'] for entry in results]\n",
    "    )\n",
    "\n",
    "    n = len(feature_names)\n",
    "    spearman_corr_matrix = np.zeros((n, n))\n",
    "    kendall_corr_matrix = np.zeros((n, n))\n",
    "\n",
    "    for entry in results:\n",
    "        x_idx = feature_names.index(entry['x_name'])\n",
    "        y_idx = feature_names.index(entry['y_name'])\n",
    "        spearman = entry['spearman'].statistic\n",
    "        kendall = entry['kendall'].statistic\n",
    "\n",
    "        spearman_corr_matrix[x_idx, y_idx] = spearman\n",
    "        spearman_corr_matrix[y_idx, x_idx] = spearman\n",
    "        kendall_corr_matrix[x_idx, y_idx] = kendall\n",
    "        kendall_corr_matrix[y_idx, x_idx] = kendall\n",
    "\n",
    "    mask = np.triu(np.ones_like(spearman_corr_matrix, dtype=bool), k=0)\n",
    "\n",
    "    trimmed_feature_names_y = feature_names[1:]\n",
    "    trimmed_feature_names_x = feature_names[:-1]\n",
    "    spearman_corr_trimmed = spearman_corr_matrix[1:, :-1]\n",
    "    kendall_corr_trimmed = kendall_corr_matrix[1:, :-1]\n",
    "    mask_trimmed = mask[1:, :-1]\n",
    "\n",
    "    spearman_corr_df = pd.DataFrame(spearman_corr_trimmed, index=trimmed_feature_names_y, columns=trimmed_feature_names_x)\n",
    "    kendall_corr_df = pd.DataFrame(kendall_corr_trimmed, index=trimmed_feature_names_y, columns=trimmed_feature_names_x)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(spearman_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1, square=True)\n",
    "    sns.heatmap(spearman_corr_df, mask=mask_trimmed, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.title(\"Spearman correlation\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    # sns.heatmap(kendall_corr_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1, square=True)\n",
    "    sns.heatmap(kendall_corr_df, mask=mask_trimmed, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Kendall correlation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lower_triangle_heatmap(aave_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlapping_correlations_twice(x_name, x_map, y_name, y_map, results_gov, results_non_gov, results_CEX, results_CA):\n",
    "    valid_addresses_gov = [address for address in x_map.keys() & y_map.keys() if address in EOA and address in all_proposers_voters_delegators_in_G_aave]\n",
    "    valid_addresses_non_gov = [address for address in x_map.keys() & y_map.keys() if address in EOA and address not in all_proposers_voters_delegators_in_G_aave and address not in CEX]\n",
    "    valid_addresses_CEX = [address for address in x_map.keys() & y_map.keys() if address in CEX]\n",
    "    valid_addresses_CA = [address for address in x_map.keys() & y_map.keys() if address in CA]\n",
    "    \n",
    "    scores_x_gov = [x_map[address] for address in valid_addresses_gov]\n",
    "    scores_y_gov = [y_map[address] for address in valid_addresses_gov]\n",
    "    \n",
    "    scores_x_non_gov = [x_map[address] for address in valid_addresses_non_gov]\n",
    "    scores_y_non_gov = [y_map[address] for address in valid_addresses_non_gov]\n",
    "    \n",
    "    scores_x_CEX = [x_map[address] for address in valid_addresses_CEX]\n",
    "    scores_y_CEX = [y_map[address] for address in valid_addresses_CEX]\n",
    "    \n",
    "    scores_x_CA = [x_map[address] for address in valid_addresses_CA]\n",
    "    scores_y_CA = [y_map[address] for address in valid_addresses_CA]\n",
    "        \n",
    "    spearman_gov = spearmanr(scores_x_gov, scores_y_gov)\n",
    "    kendall_gov = kendalltau(scores_x_gov, scores_y_gov)\n",
    "    \n",
    "    spearman_non_gov = spearmanr(scores_x_non_gov, scores_y_non_gov)\n",
    "    kendall_non_gov = kendalltau(scores_x_non_gov, scores_y_non_gov)\n",
    "    \n",
    "    spearman_CEX = spearmanr(scores_x_CEX, scores_y_CEX)\n",
    "    kendall_CEX = kendalltau(scores_x_CEX, scores_y_CEX)\n",
    "    \n",
    "    spearman_CA = spearmanr(scores_x_CA, scores_y_CA)\n",
    "    kendall_CA = kendalltau(scores_x_CA, scores_y_CA)\n",
    "    \n",
    "    results_gov.append({\n",
    "        'x_name': x_name,\n",
    "        'y_name': y_name,\n",
    "        'spearman': spearman_gov,\n",
    "        'kendall': kendall_gov,\n",
    "        'valid_addresses': valid_addresses_gov\n",
    "    })\n",
    "    \n",
    "    results_non_gov.append({\n",
    "        'x_name': x_name,\n",
    "        'y_name': y_name,\n",
    "        'spearman': spearman_non_gov,\n",
    "        'kendall': kendall_non_gov,\n",
    "        'valid_addresses': valid_addresses_non_gov\n",
    "    })\n",
    "    \n",
    "    results_CEX.append({\n",
    "        'x_name': x_name,\n",
    "        'y_name': y_name,\n",
    "        'spearman': spearman_CEX,\n",
    "        'kendall': kendall_CEX,\n",
    "        'valid_addresses': valid_addresses_CEX\n",
    "    })\n",
    "    \n",
    "    results_CA.append({\n",
    "        'x_name': x_name,\n",
    "        'y_name': y_name,\n",
    "        'spearman': spearman_CA,\n",
    "        'kendall': kendall_CA,\n",
    "        'valid_addresses': valid_addresses_CA\n",
    "    })\n",
    "    \n",
    "aave_results_gov = []\n",
    "aave_results_non_gov = []\n",
    "aave_results_CEX = []\n",
    "aave_results_CA = []\n",
    "# aWETH_results_gov = []\n",
    "# aWETH_results_non_gov = []\n",
    "# gov_results_gov = []\n",
    "# gov_results_non_gov = []\n",
    "\n",
    "for (x_name, x_map), (y_name, y_map) in list(combinations(AAVE_mappings, 2)):\n",
    "    compute_overlapping_correlations_twice(x_name, x_map, y_name, y_map, aave_results_gov, aave_results_non_gov, aave_results_CEX, aave_results_CA)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(aWETH_mappings, 2)):\n",
    "#     compute_overlapping_correlations_twice(x_name, x_map, y_name, y_map, aWETH_results_gov, aWETH_results_non_gov)\n",
    "    \n",
    "# for (x_name, x_map), (y_name, y_map) in list(combinations(gov_mappings, 2)):\n",
    "#     compute_overlapping_correlations_twice(x_name, x_map, y_name, y_map, gov_results_gov, gov_results_non_gov)\n",
    "    \n",
    "def create_heatmap_per_subgroup(results_gov, results_non_gov, results_CEX, results_CA):\n",
    "    def unique_ordered(items):\n",
    "        seen = set()\n",
    "        ordered = []\n",
    "        for item in items:\n",
    "            if item not in seen:\n",
    "                seen.add(item)\n",
    "                ordered.append(item)\n",
    "        return ordered\n",
    "\n",
    "    x_datasets_gov = unique_ordered([entry['x_name'] for entry in results_gov])\n",
    "    y_datasets_gov = unique_ordered([entry['y_name'] for entry in results_gov])\n",
    "    \n",
    "    x_datasets_non_gov = unique_ordered([entry['x_name'] for entry in results_non_gov])\n",
    "    y_datasets_non_gov = unique_ordered([entry['y_name'] for entry in results_non_gov])\n",
    "    \n",
    "    x_datasets_CEX = unique_ordered([entry['x_name'] for entry in results_CEX])\n",
    "    y_datasets_CEX = unique_ordered([entry['y_name'] for entry in results_CEX])\n",
    "    \n",
    "    x_datasets_CA = unique_ordered([entry['x_name'] for entry in results_CA])\n",
    "    y_datasets_CA = unique_ordered([entry['y_name'] for entry in results_CA])\n",
    "\n",
    "    spearman_corr_matrix_gov = np.zeros((len(x_datasets_gov), len(y_datasets_gov)))\n",
    "    kendall_corr_matrix_gov = np.zeros((len(x_datasets_gov), len(y_datasets_gov)))\n",
    "    \n",
    "    spearman_corr_matrix_non_gov = np.zeros((len(x_datasets_non_gov), len(y_datasets_non_gov)))\n",
    "    kendall_corr_matrix_non_gov = np.zeros((len(x_datasets_non_gov), len(y_datasets_non_gov)))\n",
    "    \n",
    "    spearman_corr_matrix_CEX = np.zeros((len(x_datasets_CEX), len(y_datasets_CEX)))\n",
    "    kendall_corr_matrix_CEX = np.zeros((len(x_datasets_CEX), len(y_datasets_CEX)))\n",
    "    \n",
    "    spearman_corr_matrix_CA = np.zeros((len(x_datasets_CA), len(y_datasets_CA)))\n",
    "    kendall_corr_matrix_CA = np.zeros((len(x_datasets_CA), len(y_datasets_CA)))\n",
    "\n",
    "    for entry in results_gov:\n",
    "        x_idx = x_datasets_gov.index(entry['x_name'])\n",
    "        y_idx = y_datasets_gov.index(entry['y_name'])\n",
    "        spearman_corr_matrix_gov[x_idx, y_idx] = entry['spearman'].statistic\n",
    "        kendall_corr_matrix_gov[x_idx, y_idx] = entry['kendall'].statistic\n",
    "        \n",
    "    for entry in results_non_gov:\n",
    "        x_idx = x_datasets_non_gov.index(entry['x_name'])\n",
    "        y_idx = y_datasets_non_gov.index(entry['y_name'])\n",
    "        spearman_corr_matrix_non_gov[x_idx, y_idx] = entry['spearman'].statistic\n",
    "        kendall_corr_matrix_non_gov[x_idx, y_idx] = entry['kendall'].statistic\n",
    "        \n",
    "    for entry in results_CEX:\n",
    "        x_idx = x_datasets_CEX.index(entry['x_name'])\n",
    "        y_idx = y_datasets_CEX.index(entry['y_name'])\n",
    "        spearman_corr_matrix_CEX[x_idx, y_idx] = entry['spearman'].statistic\n",
    "        kendall_corr_matrix_CEX[x_idx, y_idx] = entry['kendall'].statistic\n",
    "    \n",
    "    for entry in results_CA:\n",
    "        x_idx = x_datasets_CA.index(entry['x_name'])\n",
    "        y_idx = y_datasets_CA.index(entry['y_name'])\n",
    "        spearman_corr_matrix_CA[x_idx, y_idx] = entry['spearman'].statistic\n",
    "        kendall_corr_matrix_CA[x_idx, y_idx] = entry['kendall'].statistic\n",
    "\n",
    "    spearman_corr_df_gov = pd.DataFrame(spearman_corr_matrix_gov, index=x_datasets_gov, columns=y_datasets_gov)\n",
    "    kendall_corr_df_gov = pd.DataFrame(kendall_corr_matrix_gov, index=x_datasets_gov, columns=y_datasets_gov)\n",
    "    \n",
    "    spearman_corr_df_non_gov = pd.DataFrame(spearman_corr_matrix_non_gov, index=x_datasets_non_gov, columns=y_datasets_non_gov)\n",
    "    kendall_corr_df_non_gov = pd.DataFrame(kendall_corr_matrix_non_gov, index=x_datasets_non_gov, columns=y_datasets_non_gov)\n",
    "    \n",
    "    spearman_corr_df_CEX = pd.DataFrame(spearman_corr_matrix_CEX, index=x_datasets_CEX, columns=y_datasets_CEX)\n",
    "    kendall_corr_df_CEX = pd.DataFrame(kendall_corr_matrix_CEX, index=x_datasets_CEX, columns=y_datasets_CEX)\n",
    "    \n",
    "    spearman_corr_df_CA = pd.DataFrame(spearman_corr_matrix_CA, index=x_datasets_CA, columns=y_datasets_CA)\n",
    "    kendall_corr_df_CA = pd.DataFrame(kendall_corr_matrix_CA, index=x_datasets_CA, columns=y_datasets_CA)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    sns.heatmap(spearman_corr_df_gov, ax=axs[0, 0], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[0, 0].set_title(\"Spearman correlation gov\")\n",
    "\n",
    "    sns.heatmap(spearman_corr_df_non_gov, ax=axs[0, 1], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[0, 1].set_title(\"Spearman correlation non gov\")\n",
    "\n",
    "    sns.heatmap(spearman_corr_df_CEX, ax=axs[1, 0], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[1, 0].set_title(\"Spearman correlation CEX\")\n",
    "\n",
    "    sns.heatmap(spearman_corr_df_CA, ax=axs[1, 1], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[1, 1].set_title(\"Spearman correlation CA\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    sns.heatmap(kendall_corr_df_gov, ax=axs[0, 0], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[0, 0].set_title(\"Kendall correlation gov\")\n",
    "\n",
    "    sns.heatmap(kendall_corr_df_non_gov, ax=axs[0, 1], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[0, 1].set_title(\"Kendall correlation non gov\")\n",
    "\n",
    "    sns.heatmap(kendall_corr_df_CEX, ax=axs[1, 0], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[1, 0].set_title(\"Kendall correlation CEX\")\n",
    "\n",
    "    sns.heatmap(kendall_corr_df_CA, ax=axs[1, 1], annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    axs[1, 1].set_title(\"Kendall correlation CA\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap_per_subgroup(aave_results_gov, aave_results_non_gov, aave_results_CEX, aave_results_CA)\n",
    "# create_heatmap_per_subgroup(aWETH_results_gov, aWETH_results_non_gov)\n",
    "# create_heatmap_per_subgroup(gov_results_gov, gov_results_non_gov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset comparison per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_vs_governance_scores(feature_mapping, feature_name):\n",
    "    valid_addresses_votes = feature_mapping.keys() & votes_casted_mapping.keys()\n",
    "    valid_addresses_weights = feature_mapping.keys() & vote_weights.keys()\n",
    "\n",
    "    x_votes = [votes_casted_mapping[address] for address in valid_addresses_votes]\n",
    "    y_votes = [feature_mapping[address] for address in valid_addresses_votes]\n",
    "\n",
    "    x_weights = [vote_weights[address] for address in valid_addresses_weights]\n",
    "    y_weights = [feature_mapping[address] for address in valid_addresses_weights]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    axes[0].scatter(x_votes, y_votes, alpha=0.6)\n",
    "    axes[0].set_xlabel('Number of Votes')\n",
    "    axes[0].set_ylabel(feature_name)\n",
    "    axes[0].set_title(f'{feature_name} vs Number of Votes')\n",
    "    axes[0].set_xscale('log')\n",
    "    # axes[0].set_yscale('log')\n",
    "\n",
    "    axes[1].scatter(x_weights, y_weights, alpha=0.6, color='orange')\n",
    "    axes[1].set_xlabel('Average Vote Weight')\n",
    "    axes[1].set_ylabel(feature_name)\n",
    "    axes[1].set_title(f'{feature_name} vs Average Vote Weight')\n",
    "    axes[1].set_xscale('log')\n",
    "    # axes[1].set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/aave/SI_1_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_1_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_05_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_05_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_01_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_01_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_001_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_001_extracted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_1_dict = {i['seed']: i['avg_infected'] for i in SI_1_extracted}\n",
    "SI_05_dict = {i['seed']: i['avg_infected'] for i in SI_05_extracted}\n",
    "SI_01_dict = {i['seed']: i['avg_infected'] for i in SI_01_extracted}\n",
    "SI_001_dict = {i['seed']: i['avg_infected'] for i in SI_001_extracted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(feature_mapping):\n",
    "    valid_addresses_votes = feature_mapping.keys() & votes_casted_mapping.keys()\n",
    "    valid_addresses_weights = feature_mapping.keys() & vote_weights.keys()\n",
    "    \n",
    "    x_votes = [votes_casted_mapping[address] for address in valid_addresses_votes]\n",
    "    y_votes = [feature_mapping[address] for address in valid_addresses_votes]\n",
    "\n",
    "    x_weights = [vote_weights[address] for address in valid_addresses_weights]\n",
    "    y_weights = [feature_mapping[address] for address in valid_addresses_weights]\n",
    "    \n",
    "    spearman_weights = spearmanr(x_weights, y_weights)\n",
    "    kendall_weights = kendalltau(x_weights, y_weights)\n",
    "    \n",
    "    # spearman_votes = spearmanr(x_votes, y_votes)\n",
    "    # kendall_votes = kendalltau(x_votes, y_votes)\n",
    "    \n",
    "    return spearman_weights.statistic, kendall_weights.statistic#, spearman_votes.statistic, kendall_votes.statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_correlations(SI_1_dict))\n",
    "print(compute_correlations(SI_05_dict))\n",
    "print(compute_correlations(SI_01_dict))\n",
    "print(compute_correlations(SI_001_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_vs_governance_scores(SI_1_dict, 'SI 1')\n",
    "plot_feature_vs_governance_scores(SI_05_dict, 'SI 0.5')\n",
    "plot_feature_vs_governance_scores(SI_01_dict, 'SI 0.1')\n",
    "plot_feature_vs_governance_scores(SI_001_dict, 'SI 0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name, feature_mapping in AAVE_mappings:\n",
    "    plot_feature_vs_governance_scores(feature_mapping, feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aWETH_users = (set(aToken_df['from'].unique()) | set(aToken_df['to'].unique()))\n",
    "AAVE_users = (set(aave_df['from'].unique()) | set(aave_df['to'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aWETH_users_in_AAVE = aWETH_users & AAVE_users\n",
    "governance_users_in_AAVE = (v2_proposers | v3_proposers | v2_voters | v3_voters | delegators) & AAVE_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset_vs_rest(feature_mapping, feature_name, use_kde=True):\n",
    "    subsets = {\n",
    "        'Financial users': aWETH_users_in_AAVE,\n",
    "        'Governance users': governance_users_in_AAVE\n",
    "    }\n",
    "\n",
    "    for subset_name, subset in subsets.items():\n",
    "        in_subset = [feature_mapping[node] for node in subset if node in feature_mapping]\n",
    "        out_subset = [feature_mapping[node] for node in AAVE_users - subset if node in feature_mapping]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        if use_kde:\n",
    "            sns.kdeplot(in_subset, fill=True, alpha=0.6, ax=axes[0])\n",
    "        else:\n",
    "            axes[0].hist(in_subset, bins=30, alpha=0.7)\n",
    "        axes[0].set_title(f'{subset_name}')\n",
    "        axes[0].set_xlabel(feature_name)\n",
    "        axes[0].set_ylabel('Density' if use_kde else 'Frequency')\n",
    "\n",
    "        if use_kde:\n",
    "            sns.kdeplot(out_subset, fill=True, alpha=0.6, ax=axes[1], color='gray')\n",
    "        else:\n",
    "            axes[1].hist(out_subset, bins=30, alpha=0.7, color='gray')\n",
    "        axes[1].set_title(f'Not {subset_name}')\n",
    "        axes[1].set_xlabel(feature_name)\n",
    "        axes[1].set_ylabel('Density' if use_kde else 'Frequency')\n",
    "\n",
    "        fig.suptitle(f'{feature_name}  {subset_name} vs Rest', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset_vs_rest(feature_mapping, feature_name):\n",
    "    EOA_gov_subset = [feature_mapping[node] for node in feature_mapping if node in EOA and node in governance_users_in_AAVE and node not in CEX]\n",
    "    EOA_non_gov_subset = [feature_mapping[node] for node in feature_mapping if node in EOA and node not in governance_users_in_AAVE and node not in CEX]\n",
    "    # CA_subset = [feature_mapping[node] for node in feature_mapping if node in CA]\n",
    "    # CEX_subset = [feature_mapping[node] for node in feature_mapping if node in CEX]\n",
    "    CA_CEX_subset = [feature_mapping[node] for node in feature_mapping if node in CEX or node in CA]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    # fig, axes = plt.subplots(1, 4, figsize=(24, 4))\n",
    "\n",
    "    axes[0].hist(EOA_gov_subset, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "    axes[0].set_title('EOA governance')\n",
    "    axes[0].set_xlabel('Feature score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    axes[1].hist(EOA_non_gov_subset, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "    axes[1].set_title('EOA non-governance')\n",
    "    axes[1].set_xlabel('Feature score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[2].hist(CA_CEX_subset, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "    axes[2].set_title('Infrastructural')\n",
    "    axes[2].set_xlabel('Feature score')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    # axes[2].hist(CEX_subset, bins=30, alpha=0.7, color='green', log=True)\n",
    "    # axes[2].set_title(f'CEX')\n",
    "    # axes[2].set_xlabel(feature_name)\n",
    "    # axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    # axes[3].hist(CA_subset, bins=30, alpha=0.7, color='orange', log=True)\n",
    "    # axes[3].set_title(f'CA')\n",
    "    # axes[3].set_xlabel(feature_name)\n",
    "    # axes[3].set_ylabel('Frequency')\n",
    "\n",
    "    # fig.suptitle(f'{feature_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "temp_mapping = [\n",
    "    ('AAVE In-Degree Centrality', aave_igraph_in_dc),\n",
    "    ('AAVE Out-Degree Centrality', aave_igraph_out_dc),\n",
    "    ('AAVE Eigenvector Centrality', aave_igraph_ec),\n",
    "    ('AAVE Clustering Coefficient', aave_igraph_cc),\n",
    "    ('AAVE Avg. per Transfer', average_AAVE_transferred_mapping),\n",
    "    # ('Total transferred', total_AAVE_transferred_mapping),\n",
    "    ('AAVE Transfer Count', number_of_AAVE_transfers_mapping),\n",
    "    ('AAVE 2-Hop Weight Sum', aave_igraph_2_hop_weights),\n",
    "    ('AAVE Burstiness', aave_burstiness_total)\n",
    "]\n",
    "\n",
    "for feature_name, feature_mapping in temp_mapping:\n",
    "    plot_subset_vs_rest(feature_mapping, feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SI comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/aave/SI_5_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_5_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_2_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_2_extracted = pickle.load(f)\n",
    "\n",
    "with open('data/SI/aave/SI_1_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_1_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_05_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_05_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_01_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_01_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_001_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_001_extracted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_si_correlations(aave_mappings, si_data_list, si_labels):\n",
    "    spearman_results = []\n",
    "    kendall_results = []\n",
    "    pearson_results = []\n",
    "\n",
    "    for si_data, si_label in zip(si_data_list, si_labels):\n",
    "        si_scores = {entry['seed']: entry['avg_infected'] for entry in si_data if entry['seed'] in all_proposers_voters_delegators_in_G_aave}\n",
    "\n",
    "        for feature_name, feature_mapping in aave_mappings:\n",
    "            valid_addresses = {address for address in feature_mapping.keys() & si_scores.keys()}# if address in EOA}\n",
    "\n",
    "            if valid_addresses:\n",
    "                feature_values = [feature_mapping[address] for address in valid_addresses]\n",
    "                si_values = [si_scores[address] for address in valid_addresses]\n",
    "\n",
    "                # spearman_corr = spearmanr(feature_values, si_values).statistic\n",
    "                kendall_corr = kendalltau(feature_values, si_values).statistic\n",
    "                # pearson_corr = pearsonr(feature_values, si_values).statistic\n",
    "\n",
    "                # spearman_results.append((feature_name, si_label, spearman_corr))\n",
    "                kendall_results.append((feature_name, si_label, kendall_corr))\n",
    "                # pearson_results.append((feature_name, si_label, pearson_corr))\n",
    "\n",
    "    return spearman_results, kendall_results, pearson_results\n",
    "\n",
    "def create_correlation_heatmap(correlation_results, aave_mappings, si_labels, title):\n",
    "    feature_names = [feature_name for feature_name, _ in aave_mappings]\n",
    "    correlation_matrix = np.zeros((len(feature_names), len(si_labels)))\n",
    "\n",
    "    for feature_name, si_label, correlation in correlation_results:\n",
    "        feature_idx = feature_names.index(feature_name)\n",
    "        si_idx = si_labels.index(si_label)\n",
    "        correlation_matrix[feature_idx, si_idx] = correlation\n",
    "\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, index=feature_names, columns=si_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(title)\n",
    "    # plt.xlabel('SI configuration')\n",
    "    # plt.ylabel('AAVE features')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "si_data_list = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "si_labels = ['=0.01', '=0.1', '=0.5', '=1.0', '=2.0']#, '=5.0']\n",
    "\n",
    "spearman_results, kendall_results, pearson_results = compute_si_correlations(AAVE_mappings, si_data_list, si_labels)\n",
    "\n",
    "# create_correlation_heatmap(spearman_results, AAVE_mappings, si_labels, \"Spearman correlation\")\n",
    "create_correlation_heatmap(kendall_results, AAVE_mappings, si_labels, \"Kendall correlation\")\n",
    "# create_correlation_heatmap(pearson_results, AAVE_mappings, si_labels, \"Pearson correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_data_list = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "si_labels = ['=0.01', '=0.1', '=0.5', '=1.0', '=2.0']#, '=5.0']\n",
    "\n",
    "def compute_gov_si_correlations(gov_mappings, si_data_list, si_labels):\n",
    "    kendall_results = []\n",
    "    for si_data, si_label in zip(si_data_list, si_labels):\n",
    "        si_scores = {entry['seed']: entry['avg_infected'] for entry in si_data}\n",
    "        for feature_name, feature_mapping in gov_mappings:\n",
    "            valid_addresses = feature_mapping.keys() & si_scores.keys()\n",
    "            if valid_addresses:\n",
    "                feature_values = [feature_mapping[address] for address in valid_addresses]\n",
    "                si_values = [si_scores[address] for address in valid_addresses]\n",
    "                kendall_corr = kendalltau(feature_values, si_values).statistic\n",
    "                kendall_results.append((feature_name, si_label, kendall_corr))\n",
    "    return kendall_results\n",
    "\n",
    "def create_gov_si_heatmap(correlation_results, gov_mappings, si_labels, title):\n",
    "    feature_names = [feature_name for feature_name, _ in gov_mappings]\n",
    "    correlation_matrix = np.zeros((len(feature_names), len(si_labels)))\n",
    "    for feature_name, si_label, correlation in correlation_results:\n",
    "        feature_idx = feature_names.index(feature_name)\n",
    "        si_idx = si_labels.index(si_label)\n",
    "        correlation_matrix[feature_idx, si_idx] = correlation\n",
    "    correlation_df = pd.DataFrame(correlation_matrix, index=feature_names, columns=si_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    # plt.title(title)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "gov_si_kendall_results = compute_gov_si_correlations(gov_mappings, si_data_list, si_labels)\n",
    "create_gov_si_heatmap(gov_si_kendall_results, gov_mappings, si_labels, \"Kendall correlation (Governance vs SI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_2_mapping = {entry['seed']: entry['avg_infected'] for entry in SI_2_extracted}\n",
    "\n",
    "valid_addresses = set(SI_2_mapping.keys()) & set(average_vote_weights_mapping.keys())\n",
    "\n",
    "si2_vals = [SI_2_mapping[address] for address in valid_addresses]\n",
    "votes_vals = [average_vote_weights_mapping[address] for address in valid_addresses]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(votes_vals, si2_vals, alpha=0.6)\n",
    "axes[0].set_xlabel('Avg. Vote Weight')\n",
    "axes[0].set_ylabel('Avg. Infected Nodes (SI =2.0)')\n",
    "axes[0].set_title('Normal scale')\n",
    "\n",
    "axes[1].scatter(votes_vals, si2_vals, alpha=0.6)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel('Avg. Vote Weight (log scale)')\n",
    "axes[1].set_ylabel('Avg. Infected Nodes (SI =2.0, log scale)')\n",
    "axes[1].set_title('Log-Log Scale')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def compute_ndcg(si_scores, si_label, other_feature_scores, other_feature_label, k=None):\n",
    "    si_mapping = {entry['seed']: entry['avg_infected'] for entry in si_scores}\n",
    "    common_addresses = set(si_mapping.keys()) & set(other_feature_scores.keys())\n",
    "\n",
    "    si_scores = [si_mapping[addr] for addr in common_addresses]\n",
    "    vote_weight_scores = [other_feature_scores[addr] for addr in common_addresses]\n",
    "\n",
    "    si_ranks = rankdata(si_scores, method=\"average\")\n",
    "    vote_ranks = rankdata(vote_weight_scores, method=\"average\")\n",
    "    print(si_label)\n",
    "    # ndcg = ndcg_score([vote_ranks], [si_ranks])\n",
    "    # print(f'Ranked: {ndcg}')\n",
    "    # ndcg = ndcg_score([si_ranks], [vote_ranks])\n",
    "    # print(ndcg)\n",
    "\n",
    "    ndcg = ndcg_score([vote_weight_scores], [si_scores])\n",
    "    print(f'Raw: {ndcg}')\n",
    "    # ndcg = ndcg_score([si_scores], [vote_weight_scores])\n",
    "    # print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_data_list = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "si_labels = ['=0.01', '=0.1', '=0.5', '=1.0', '=2.0']#, '=5.0']\n",
    "\n",
    "for si_data, si_label in zip(si_data_list, si_labels):\n",
    "    compute_ndcg(si_data, si_label, average_vote_weights_mapping, 'Avg. Vote Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevance_scores, k):\n",
    "    relevance_scores = np.asfarray(relevance_scores)[:k]\n",
    "    return np.sum(relevance_scores / np.log2(np.arange(2, k + 2)))\n",
    "\n",
    "def ndcg(si_outcome: list, vote_weights: dict, k: int = None):\n",
    "    seeds_with_vote = [entry for entry in si_outcome if entry['seed'] in vote_weights]\n",
    "    \n",
    "    if not seeds_with_vote:\n",
    "        return 0\n",
    "    \n",
    "    sorted_seeds = sorted(seeds_with_vote, key=lambda x: x['avg_infected'], reverse=True)\n",
    "    \n",
    "    rel_pred = [vote_weights[entry['seed']] for entry in sorted_seeds]\n",
    "    \n",
    "    ideal_sorted = sorted(seeds_with_vote, key=lambda x: vote_weights[x['seed']], reverse=True)\n",
    "    rel_ideal = [vote_weights[entry['seed']] for entry in ideal_sorted]\n",
    "\n",
    "    k = k or len(rel_pred)\n",
    "\n",
    "    dcg_val = dcg(rel_pred, k)\n",
    "    idcg_val = dcg(rel_ideal, k)\n",
    "\n",
    "    return dcg_val / idcg_val if idcg_val > 0 else 0\n",
    "\n",
    "for si_data in si_data_list:\n",
    "    print(ndcg(si_data, average_vote_weights_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aToken analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tokens = {\n",
    "    'USDC': ['aEthUSDC', 'aPolUSDC', 'aArbUSDC'],\n",
    "    'USDT': ['aEthUSDT', 'aPolUSDT', 'aArbUSDT'],\n",
    "    'WETH': ['aEthWETH', 'aPolWETH', 'aArbWETH']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfers_per_token = aToken_df.groupby('token').size()\n",
    "\n",
    "transfers_per_token = pd.Series({token: transfers_per_token.loc[names].sum() for token, names in combined_tokens.items()})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "transfers_per_token.plot(kind='bar')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Number of transfers')\n",
    "plt.title('Number of transfers per aToken')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_data = pd.read_csv('data/WETH_daily_USD.csv', parse_dates=['snapped_at'])\n",
    "\n",
    "# price_data['date'] = pd.to_datetime(price_data['snapped_at']).dt.date\n",
    "# aToken_df['date'] = pd.to_datetime(aToken_df['timestamp'], unit='s')\n",
    "# aToken_df['date'] = aToken_df['date'].dt.date\n",
    "\n",
    "# aToken_df2 = aToken_df.merge(price_data[['date', 'price']], on='date', how='left')\n",
    "\n",
    "# tokens_to_multiply = ['aArbWETH', 'aEthWETH', 'aPolWETH']\n",
    "\n",
    "# aToken_df2['value_traded'] = aToken_df2.apply(\n",
    "#     lambda row: row['value'] * row['price'] if row['token'] in tokens_to_multiply else row['value'], axis=1\n",
    "# )\n",
    "\n",
    "# transfers_over_time_per_token = aToken_df2.groupby('token')['value_traded'].sum()\n",
    "\n",
    "transfers_over_time_per_token = pd.Series({token: transfers_over_time_per_token.loc[names].sum() for token, names in combined_tokens.items()})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "transfers_over_time_per_token.plot(kind='bar')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Total value traded')\n",
    "plt.title('Total value traded over time per token')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_address = '0x0000000000000000000000000000000000000000'\n",
    "\n",
    "total_transfers_per_token = aToken_df['token'].value_counts()\n",
    "\n",
    "null_address_transfers_per_token = aToken_df[(aToken_df['from'].str.lower() == null_address) | (aToken_df['to'].str.lower() == null_address)].groupby('token').size()\n",
    "\n",
    "percentage_null_address_transfers = (null_address_transfers_per_token / total_transfers_per_token) * 100\n",
    "\n",
    "percentage_non_null_address_transfers = 100 - percentage_null_address_transfers\n",
    "\n",
    "percentage_transfers_df = pd.DataFrame({\n",
    "    'Null Address': percentage_null_address_transfers,\n",
    "    'Non-Null Address': percentage_non_null_address_transfers\n",
    "}).fillna(0)\n",
    "\n",
    "percentage_transfers_df.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Percentage of transfers')\n",
    "plt.title('Percentage of transfers from/to null and non-null addresses for each aToken')\n",
    "plt.legend(title='Transfer Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venn diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    'aWETH users': set(G_aToken_nodes),\n",
    "    'AAVE users': set(G_aave_nodes)\n",
    "}\n",
    "\n",
    "venn(subsets, fmt=\"{size}\", cmap='plasma', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    'Proposers': v2_proposers | v3_proposers,\n",
    "    'Voters': v2_voters | v3_voters,\n",
    "    'Delegations': delegators\n",
    "}\n",
    "\n",
    "venn(subsets, fmt=\"{size}\", cmap='plasma', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    \"Governance token\": G_aave_nodes,\n",
    "    \"Financial token\": G_aToken_nodes,\n",
    "    \"Governance\": (v2_proposers | v3_proposers| v2_voters | v3_voters | delegators)\n",
    "}\n",
    "\n",
    "venn(subsets, fmt=\"{size}\", cmap='plasma', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI, NMI, AMI & ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    \n",
    "    return -sum((count/total) * log2(count/total) for count in counts.values())\n",
    "\n",
    "def mutual_information(x, y):\n",
    "    total = len(x)\n",
    "    counter_x = Counter(x)\n",
    "    counter_y = Counter(y)\n",
    "    joint_counter = Counter(zip(x, y))\n",
    "    mi = 0.0\n",
    "    \n",
    "    for (x_val, y_val), joint_count in joint_counter.items():\n",
    "        px = counter_x[x_val] / total\n",
    "        py = counter_y[y_val] / total\n",
    "        pxy = joint_count / total\n",
    "        mi += pxy * log2(pxy / (px * py))\n",
    "    \n",
    "    return mi\n",
    "\n",
    "def variation_of_information(x, y):\n",
    "    return entropy(x) + entropy(y) - 2 * mutual_information(x, y)\n",
    "\n",
    "def run_leiden_iterations(graph, n_iterations=10, resolution=1.0):\n",
    "    memberships = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        partition = la.find_partition(\n",
    "            graph,\n",
    "            la.ModularityVertexPartition,\n",
    "            weights='weight'\n",
    "        )\n",
    "        print(i, partition.modularity)\n",
    "        \n",
    "        memberships.append(partition.membership)\n",
    "    \n",
    "    return memberships\n",
    "\n",
    "def compute_vi_nmi_ami_ari(memberships):\n",
    "    n = len(memberships)\n",
    "    vi_scores = np.zeros((n, n))\n",
    "    nmi_scores = np.zeros((n, n))\n",
    "    ami_scores = np.zeros((n, n))\n",
    "    ari_scores = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            vi = variation_of_information(memberships[i], memberships[j])\n",
    "            nmi = normalized_mutual_info_score(memberships[i], memberships[j])\n",
    "            ami = adjusted_mutual_info_score(memberships[i], memberships[j])\n",
    "            ari = adjusted_rand_score(memberships[i], memberships[j])\n",
    "            \n",
    "            vi_scores[i, j] = vi_scores[j, i] = vi\n",
    "            nmi_scores[i, j] = nmi_scores[j, i] = nmi\n",
    "            ami_scores[i, j] = ami_scores[j, i] = ami\n",
    "            ari_scores[i, j] = ari_scores[j, i] = ari\n",
    "    return vi_scores, nmi_scores, ami_scores, ari_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_aave_igraph = ig.Graph.from_networkx(G_aave, vertex_attr_hashable='name')\n",
    "G_aave_count_igraph = ig.Graph.from_networkx(G_aave_count, vertex_attr_hashable='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100\n",
    "resolution = 1.0\n",
    "memberships = run_leiden_iterations(G_aave_igraph, n_iterations, resolution)\n",
    "\n",
    "vi_scores, nmi_scores, ami_scores, ari_scores = compute_vi_nmi_ami_ari(memberships)\n",
    "\n",
    "avg_vi = np.mean(vi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_nmi = np.mean(nmi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ami = np.mean(ami_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ari = np.mean(ari_scores[np.triu_indices(n_iterations, k=1)])\n",
    "\n",
    "print(f\"Average VI: {avg_vi:.4f}\")\n",
    "print(f\"Average NMI: {avg_nmi:.4f}\")\n",
    "print(f\"Average AMI: {avg_ami:.4f}\")\n",
    "print(f\"Average ARI: {avg_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100\n",
    "resolution = 1.0\n",
    "memberships = run_leiden_iterations(G_aave_count_igraph, n_iterations, resolution)\n",
    "\n",
    "vi_scores, nmi_scores, ami_scores, ari_scores = compute_vi_nmi_ami_ari(memberships)\n",
    "\n",
    "avg_vi = np.mean(vi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_nmi = np.mean(nmi_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ami = np.mean(ami_scores[np.triu_indices(n_iterations, k=1)])\n",
    "avg_ari = np.mean(ari_scores[np.triu_indices(n_iterations, k=1)])\n",
    "\n",
    "print(f\"Average VI: {avg_vi:.4f}\")\n",
    "print(f\"Average NMI: {avg_nmi:.4f}\")\n",
    "print(f\"Average AMI: {avg_ami:.4f}\")\n",
    "print(f\"Average ARI: {avg_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Leiden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_aave_count_igraph = ig.Graph.from_networkx(G_aave_count, vertex_attr_hashable='name')\n",
    "\n",
    "G_aave_count_igraph_components = G_aave_count_igraph.connected_components(mode=\"weak\")\n",
    "G_aave_count_igraph_largest_wcc = G_aave_count_igraph_components.giant()\n",
    "\n",
    "partition = la.find_partition(G_aave_count_igraph_largest_wcc, la.ModularityVertexPartition, weights='weight')\n",
    "\n",
    "print(len(partition), partition.modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G_aave_count_igraph.vs), len(G_aave_count_igraph_largest_wcc.vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_WCC_nodes = set(G_aave_count_igraph.vs['name']) - set(G_aave_count_igraph_largest_wcc.vs['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(non_WCC_nodes))\n",
    "print(len(non_WCC_nodes & CEX))\n",
    "print(len(non_WCC_nodes & CA))\n",
    "print(len(non_WCC_nodes & EOA & all_proposers_voters_delegators_in_G_aave))\n",
    "print(len(non_WCC_nodes & (EOA - all_proposers_voters_delegators_in_G_aave - CEX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_W_communities = 0\n",
    "G_C_communities = 0\n",
    "\n",
    "G_W_modularity = 0\n",
    "G_C_modularity = 0\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    \n",
    "    p1 = la.find_partition(G_aave_igraph, la.ModularityVertexPartition, weights='weight')\n",
    "    p2 = la.find_partition(G_aave_count_igraph, la.ModularityVertexPartition, weights='weight')\n",
    "    \n",
    "    G_W_communities += len(p1)\n",
    "    G_C_communities += len(p2)\n",
    "    \n",
    "    G_W_modularity += p1.modularity\n",
    "    G_C_modularity += p2.modularity\n",
    "    \n",
    "average_G_W_size = G_W_communities / 100\n",
    "average_G_C_size = G_C_communities / 100\n",
    "\n",
    "average_G_W_mod = G_W_modularity / 100\n",
    "average_G_C_mod = G_C_modularity / 100\n",
    "\n",
    "print(f'Communities: G_W: {average_G_W_size}, G_C: {average_G_C_size}')\n",
    "print(f'Modularity: G_W: {average_G_W_mod}, G_C: {average_G_C_mod}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_sizes = [len(c) for c in partition]\n",
    "\n",
    "sorted_indices = sorted(range(len(community_sizes)), key=lambda i: community_sizes[i], reverse=True)\n",
    "\n",
    "sorted_communities = [partition[i] for i in sorted_indices]\n",
    "\n",
    "top_communities = sorted_communities[:10]\n",
    "\n",
    "top_communities_sizes = [len(community) for community in top_communities]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), top_communities_sizes)\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Size')\n",
    "plt.title('Size of top communities')\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()\n",
    "\n",
    "all_communities_sizes = [len(community) for community in sorted_communities]\n",
    "log_bins = np.logspace(np.log10(min(all_communities_sizes)), np.log10(max(all_communities_sizes)), 30)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(all_communities_sizes, bins=30)\n",
    "# plt.yscale('log')\n",
    "# sns.histplot(all_communities_sizes, bins=log_bins)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Size of top communities')\n",
    "plt.show()\n",
    "\n",
    "communities_with_ids = [\n",
    "    {G_aave_count_igraph_largest_wcc.vs[idx][\"name\"] for idx in community} for community in top_communities\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_distribution = {\n",
    "    'Economic': [],\n",
    "    'Governance': [],\n",
    "    'Both': [],\n",
    "    'None': []\n",
    "}\n",
    "\n",
    "only_economic_users_in_G_wcc = only_economic_users_in_G_aave & set(G_aave_count_igraph_largest_wcc.vs[\"name\"])\n",
    "only_voters_in_G_wcc = only_voters_in_G_aave & set(G_aave_count_igraph_largest_wcc.vs[\"name\"])\n",
    "both_economic_and_governance_users_in_G_wcc = both_economic_and_governance_users & set(G_aave_count_igraph_largest_wcc.vs[\"name\"])\n",
    "all_economic_users_in_G_wcc = all_economic_users_in_G_aave & set(G_aave_count_igraph_largest_wcc.vs[\"name\"])\n",
    "all_voters_in_G_wcc = all_voters_in_G_aave & set(G_aave_count_igraph_largest_wcc.vs[\"name\"])\n",
    "\n",
    "for community in communities_with_ids:\n",
    "    economic_count = len(only_economic_users_in_G_wcc & community)\n",
    "    governance_count = len(only_voters_in_G_wcc & community)\n",
    "    both_count = len(both_economic_and_governance_users_in_G_wcc & community)\n",
    "    none_count = len(community) - economic_count - governance_count - both_count\n",
    "    \n",
    "    community_distribution['Economic'].append(economic_count)\n",
    "    community_distribution['Governance'].append(governance_count)\n",
    "    community_distribution['Both'].append(both_count)\n",
    "    community_distribution['None'].append(none_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8\n",
    "index = np.arange(len(communities_with_ids))\n",
    "\n",
    "economic_array = np.array(community_distribution['Economic'])\n",
    "governance_array = np.array(community_distribution['Governance'])\n",
    "both_array = np.array(community_distribution['Both'])\n",
    "none_array = np.array(community_distribution['None'])\n",
    "\n",
    "bar1 = ax.bar(index, none_array, bar_width, label='None')\n",
    "bar2 = ax.bar(index, economic_array, bar_width, bottom=none_array, label='Economic')\n",
    "bar3 = ax.bar(index, governance_array, bar_width, bottom=none_array + economic_array, label='Governance')\n",
    "bar4 = ax.bar(index, both_array, bar_width, bottom=none_array + economic_array + governance_array, label='Both')\n",
    "\n",
    "ax.set_xlabel('Community index')\n",
    "ax.set_ylabel('Number of nodes')\n",
    "ax.set_title('Stacked distribution of nodes in communities')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([f'Community {i+1}' for i in range(len(communities_with_ids))], rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_distribution_2 = {\n",
    "    'EOA gov': [],\n",
    "    'EOA non gov': [],\n",
    "    'CEX': [],\n",
    "    'CA': []\n",
    "}\n",
    "\n",
    "total_EOA_gov_percentage = 0\n",
    "total_EOA_non_gov_percentage = 0\n",
    "total_CEX_percentage = 0\n",
    "total_CA_percentage = 0\n",
    "\n",
    "EOA_gov_percentages = []\n",
    "EOA_non_gov_percentages = []\n",
    "CEX_percentages = []\n",
    "CA_percentages = []\n",
    "\n",
    "id = 1\n",
    "for community in communities_with_ids:\n",
    "    EOA_gov_count = len(community & (EOA & all_proposers_voters_delegators_in_G_aave))\n",
    "    EOA_non_gov_count = len(community & (EOA - all_proposers_voters_delegators_in_G_aave - CEX))\n",
    "    CEX_count = len(community & CEX)\n",
    "    CA_count = len(community & CA)\n",
    "    \n",
    "    community_distribution_2['EOA gov'].append(EOA_gov_count)\n",
    "    community_distribution_2['EOA non gov'].append(EOA_non_gov_count)\n",
    "    community_distribution_2['CEX'].append(CEX_count)\n",
    "    community_distribution_2['CA'].append(CA_count)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8\n",
    "index = np.arange(len(communities_with_ids))\n",
    "\n",
    "EOA_gov_array = np.array(community_distribution_2['EOA gov'])\n",
    "EOA_non_gov_array = np.array(community_distribution_2['EOA non gov'])\n",
    "CEX_array = np.array(community_distribution_2['CEX'])\n",
    "CA_array = np.array(community_distribution_2['CA'])\n",
    "\n",
    "bar1 = ax.bar(index, EOA_non_gov_array, bar_width, label='EOA non-governance')\n",
    "bar2 = ax.bar(index, EOA_gov_array, bar_width, bottom=EOA_non_gov_array, label='EOA governance')\n",
    "bar3 = ax.bar(index, CA_array + CEX_array, bar_width, bottom=EOA_non_gov_array + EOA_gov_array, label='Infrastructure')\n",
    "\n",
    "ax.set_xlabel('Community index')\n",
    "ax.set_ylabel('Number of nodes')\n",
    "# ax.set_title('Distribution of nodes per community')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([f'{i+1}' for i in range(len(communities_with_ids))])\n",
    "# ax.set_xticklabels([f'Community {i+1}' for i in range(len(communities_with_ids))], rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_distribution_3 = {\n",
    "    'gov': [],\n",
    "    'other': []\n",
    "}\n",
    "\n",
    "total_gov_percentage = 0\n",
    "total_other_percentage = 0\n",
    "\n",
    "gov_percentages = []\n",
    "other_percentages = []\n",
    "\n",
    "id = 1\n",
    "for community in communities_with_ids:\n",
    "    gov_count = len(community & all_proposers_voters_delegators_in_G_aave)\n",
    "    other_count = len(community) - gov_count\n",
    "    \n",
    "    community_distribution_3['gov'].append(gov_count)\n",
    "    community_distribution_3['other'].append(other_count)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.8\n",
    "index = np.arange(len(communities_with_ids))\n",
    "\n",
    "gov_array = np.array(community_distribution_3['gov'])\n",
    "other_array = np.array(community_distribution_3['other'])\n",
    "\n",
    "bar1 = ax.bar(index, other_array, bar_width, label='Other')\n",
    "bar2 = ax.bar(index, gov_array, bar_width, bottom=other_array, label='Governance')\n",
    "\n",
    "ax.set_xlabel('Community index')\n",
    "ax.set_ylabel('Number of nodes')\n",
    "# ax.set_title('Distribution of nodes per community')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([f'{i+1}' for i in range(len(communities_with_ids))])\n",
    "# ax.set_xticklabels([f'Community {i+1}' for i in range(len(communities_with_ids))], rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gov_array = np.array(community_distribution_2['EOA gov'])\n",
    "# CEX_array = np.array(community_distribution_2['CEX'])\n",
    "# CA_array = np.array(community_distribution_2['CA'])\n",
    "\n",
    "total_gov_percentage = (len(all_proposers_voters_delegators_in_G_aave) / len(G_aave_nodes)) * 100\n",
    "\n",
    "community_gov_percentages = []\n",
    "\n",
    "for i in range(len(gov_array)):\n",
    "    gov_percentage = (gov_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_gov_percentages.append(gov_percentage)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(gov_array)), gov_array)#, color='orange')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Number of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "# plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'{i+1}' for i in range(len(top_communities))])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), community_gov_percentages, label='Community percentage')\n",
    "plt.axhline(y=total_gov_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Percentage of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'{i+1}' for i in range(len(top_communities))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n",
    "\n",
    "axs[0].bar(range(len(EOA_gov_array)), EOA_gov_array)#, color='orange')\n",
    "# axs[0].axhline(y=total_EOA_gov_percentage, color='r', linestyle='--', label='Total EOA percentage')\n",
    "axs[0].set_xlabel('Community index')\n",
    "axs[0].set_ylabel('Number of nodes')\n",
    "axs[0].set_title('EOA governance')\n",
    "# axs[0].legend()\n",
    "axs[0].set_xticks(range(len(top_communities)))\n",
    "axs[0].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "\n",
    "infra_array = CA_array + CEX_array\n",
    "axs[1].bar(range(len(infra_array)), infra_array)#, color='green')\n",
    "# axs[1].axhline(y=total_CEX_percentage + total_CA_percentage, color='r', linestyle='--', label='Total percentage')\n",
    "axs[1].set_xlabel('Community index')\n",
    "axs[1].set_title('Infrastructural')\n",
    "# axs[1].legend()\n",
    "axs[1].set_xticks(range(len(top_communities)))\n",
    "axs[1].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(EOA_gov_array)), EOA_gov_array)#, color='orange')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Number of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "# plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(infra_array)), infra_array)#, color='green')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Number of nodes')\n",
    "# plt.title('Percentage of economic nodes per community vs total economic percentage')\n",
    "# plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOA_gov_array = np.array(community_distribution_2['EOA gov'])\n",
    "CEX_array = np.array(community_distribution_2['CEX'])\n",
    "CA_array = np.array(community_distribution_2['CA'])\n",
    "\n",
    "total_EOA_gov_percentage = (len(EOA & all_proposers_voters_delegators_in_G_aave) / len(G_aave_nodes)) * 100\n",
    "total_CEX_percentage = (len(CEX & G_aave_nodes) / len(G_aave_nodes)) * 100\n",
    "total_CA_percentage = (len(CA & G_aave_nodes) / len(G_aave_nodes)) * 100\n",
    "\n",
    "community_EOA_gov_percentages = []\n",
    "community_CEX_percentages = []\n",
    "community_CA_percentages = []\n",
    "\n",
    "for i in range(len(EOA_gov_array)):\n",
    "    EOA_gov_percentage = (EOA_gov_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_EOA_gov_percentages.append(EOA_gov_percentage)\n",
    "    \n",
    "    CEX_percentage = (CEX_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_CEX_percentages.append(CEX_percentage)\n",
    "    \n",
    "    CA_percentage = (CA_array[i] / len(communities_with_ids[i])) * 100\n",
    "    community_CA_percentages.append(CA_percentage)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(20, 6), sharex=True)\n",
    "\n",
    "axs[0].bar(range(len(top_communities)), community_EOA_gov_percentages, label='Community percentage')\n",
    "axs[0].axhline(y=total_EOA_gov_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "axs[0].set_xlabel('Community index')\n",
    "axs[0].set_ylabel('Percentage of nodes')\n",
    "axs[0].set_title('EOA governance')\n",
    "axs[0].legend()\n",
    "axs[0].set_xticks(range(len(top_communities)))\n",
    "axs[0].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "# axs[0].set_xticklabels([f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "\n",
    "CEX_CA_percentages = [community_CA_percentages[i] + community_CEX_percentages[i] for i in range(len(community_CEX_percentages))]\n",
    "axs[1].bar(range(len(top_communities)), CEX_CA_percentages, label='Community percentage')\n",
    "axs[1].axhline(y=total_CEX_percentage + total_CA_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "axs[1].set_xlabel('Community index')\n",
    "axs[1].set_title('Infrastructure')\n",
    "axs[1].legend()\n",
    "axs[1].set_xticks(range(len(top_communities)))\n",
    "axs[0].set_xticklabels([f'{i+1}' for i in range(len(top_communities))])\n",
    "# axs[1].set_xticklabels([f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "\n",
    "# axs[1].bar(range(len(top_communities)), community_CEX_percentages, label='Community CEX percentage')\n",
    "# axs[1].axhline(y=total_CEX_percentage, color='r', linestyle='--', label='Total CEX percentage')\n",
    "# axs[1].set_xlabel('Community index')\n",
    "# axs[1].set_title('CEX')\n",
    "# axs[1].legend()\n",
    "# axs[1].set_xticks(range(len(top_communities)))\n",
    "# axs[1].set_xticklabels([f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "\n",
    "# axs[2].bar(range(len(top_communities)), community_CA_percentages, label='Community CA percentage')\n",
    "# axs[2].axhline(y=total_CA_percentage, color='r', linestyle='--', label='Total CA percentage')\n",
    "# axs[2].set_xlabel('Community index')\n",
    "# axs[2].set_title('CA')\n",
    "# axs[2].legend()\n",
    "# axs[2].set_xticklabels([f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), community_EOA_gov_percentages, label='Community percentage')\n",
    "plt.axhline(y=total_EOA_gov_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Percentage of nodes')\n",
    "# plt.title('Percentage of governance nodes per community vs total governance percentage')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(top_communities)), CEX_CA_percentages, label='Community percentage')\n",
    "plt.axhline(y=total_CEX_percentage + total_CA_percentage, color='r', linestyle='--', label='Average percentage')\n",
    "plt.xlabel('Community index')\n",
    "plt.ylabel('Percentage of nodes')\n",
    "# plt.title('Percentage of economic nodes per community vs total economic percentage')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(top_communities)), [f'Community {i+1}' for i in range(len(top_communities))], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature averages per community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_community(mapping):\n",
    "    community_averages = {feature_name: [] for feature_name, _ in mapping}\n",
    "    community_distributions = {feature_name: [] for feature_name, _ in mapping}\n",
    "\n",
    "    for community in top_communities:\n",
    "        community_node_ids = {G_aave_count_igraph_largest_wcc.vs[idx][\"name\"] for idx in community}\n",
    "\n",
    "        for feature_name, feature_mapping in mapping:\n",
    "            feature_values = [feature_mapping[node_id] for node_id in community_node_ids if node_id in feature_mapping]\n",
    "            average_value = sum(feature_values) / len(feature_values) if feature_values else 0\n",
    "            \n",
    "            community_averages[feature_name].append(average_value)\n",
    "            community_distributions[feature_name].append(feature_values)\n",
    "    \n",
    "    num_features = len(mapping)\n",
    "    cols = 3\n",
    "    rows = (num_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i, (feature_name, averages) in enumerate(community_averages.items()):\n",
    "        axs[i].plot(range(len(top_communities)), averages, marker='o')\n",
    "        axs[i].set_title(feature_name)\n",
    "        axs[i].set_xlabel(\"Community index\")\n",
    "        axs[i].set_ylabel(\"Average feature score\")\n",
    "        axs[i].set_xticks(range(len(top_communities)))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(len(top_communities))])\n",
    "    \n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_community_2(mapping):\n",
    "    feature_stats = {}\n",
    "    for feature_name, feature_mapping in mapping:\n",
    "        values = list(feature_mapping.values())\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        feature_stats[feature_name] = (mean, std)\n",
    "\n",
    "    community_averages = {feature_name: [] for feature_name, _ in mapping}\n",
    "    community_distributions = {feature_name: [] for feature_name, _ in mapping}\n",
    "\n",
    "    for community in top_communities:\n",
    "        community_node_ids = {G_aave_count_igraph_largest_wcc.vs[idx][\"name\"] for idx in community}\n",
    "\n",
    "        for feature_name, feature_mapping in mapping:\n",
    "            feature_values = [feature_mapping[node_id] for node_id in community_node_ids if node_id in feature_mapping]\n",
    "\n",
    "            if feature_values:\n",
    "                avg_val = sum(feature_values) / len(feature_values)\n",
    "                mean, std = feature_stats[feature_name]\n",
    "                z_score = (avg_val - mean) / std if std > 0 else 0\n",
    "            else:\n",
    "                z_score = 0\n",
    "\n",
    "            community_averages[feature_name].append(z_score)\n",
    "            community_distributions[feature_name].append(feature_values)\n",
    "\n",
    "    num_features = len(mapping)\n",
    "    cols = 3\n",
    "    rows = (num_features + cols - 1) // cols\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (feature_name, averages) in enumerate(community_averages.items()):\n",
    "        axs[i].plot(range(len(top_communities)), averages, marker='o')\n",
    "        axs[i].set_title(f'{feature_name} (z-score)')\n",
    "        axs[i].set_xlabel('Community index')\n",
    "        axs[i].set_ylabel('Z-score')\n",
    "        axs[i].set_xticks(range(len(top_communities)))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(len(top_communities))])\n",
    "\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community(AAVE_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community_2(AAVE_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community(aWETH_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community_2(aWETH_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community(gov_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_community_2(gov_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SI influence comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SI/aave/SI_5_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_5_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_2_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_2_extracted = pickle.load(f)\n",
    "\n",
    "with open('data/SI/aave/SI_1_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_1_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_05_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_05_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_01_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_01_extracted = pickle.load(f)\n",
    "    \n",
    "with open('data/SI/aave/SI_001_extracted_seeds_avg_infected.pkl', 'rb') as f:\n",
    "    SI_001_extracted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_influence_multiple_runs(influence_data_list, beta_labels):\n",
    "    num_runs = len(influence_data_list)\n",
    "    num_communities = len(top_communities)\n",
    "\n",
    "    community_scores = [defaultdict(list) for _ in range(num_runs)]\n",
    "\n",
    "    for run_idx, influence_data in enumerate(influence_data_list):\n",
    "        influence_scores = {entry['seed']: entry['avg_infected'] for entry in influence_data if entry['seed'] in all_proposers_voters_delegators_in_G_aave}\n",
    "        for comm_id, community in enumerate(top_communities):\n",
    "            for node_idx in community:\n",
    "                node_id = G_aave_count_igraph_largest_wcc.vs[node_idx]['name']\n",
    "                if node_id in influence_scores:\n",
    "                    community_scores[run_idx][comm_id].append(influence_scores[node_id])\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_runs, figsize=(5 * num_runs, 3))\n",
    "    if num_runs == 1:\n",
    "        axs = [axs]\n",
    "    for i in range(num_runs):\n",
    "        avg_infs = [\n",
    "            np.mean(community_scores[i][cid]) if community_scores[i][cid] else 0\n",
    "            for cid in range(num_communities)\n",
    "        ]\n",
    "        axs[i].plot(range(num_communities), avg_infs, marker='o', label=f\"={beta_labels[i]}\")\n",
    "        axs[i].set_title(f\"={beta_labels[i]}\")\n",
    "        axs[i].set_xlabel(\"Community index\")\n",
    "        axs[i].set_ylabel(\"Average infected nodes\")\n",
    "        axs[i].set_xticks(range(num_communities))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(num_communities)])\n",
    "        # axs[i].grid(True)\n",
    "        # axs[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_runs, figsize=(5 * num_runs, 3))\n",
    "    if num_runs == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        data = [community_scores[i][cid] for cid in range(num_communities)]\n",
    "        axs[i].boxplot(data, positions=range(num_communities))\n",
    "        axs[i].set_title(f\"={beta_labels[i]}\")\n",
    "        axs[i].set_xlabel(\"Community index\")\n",
    "        axs[i].set_ylabel(\"Average infected nodes\")\n",
    "        axs[i].set_xticks(range(num_communities))\n",
    "        axs[i].set_xticklabels([f\"{i+1}\" for i in range(num_communities)])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # fig, axs = plt.subplots(1, num_runs, figsize=(6 * num_runs, 5))\n",
    "    # if num_runs == 1:\n",
    "    #     axs = [axs]\n",
    "\n",
    "    # for i in range(num_runs):\n",
    "    #     data = [community_scores[i][cid] for cid in range(num_communities)]\n",
    "    #     axs[i].violinplot(data, positions=range(num_communities), showmedians=True)\n",
    "    #     axs[i].set_title(f\"Violin plot (={beta_labels[i]})\")\n",
    "    #     axs[i].set_xlabel(\"Community\")\n",
    "    #     axs[i].set_xticks(range(num_communities))\n",
    "    #     axs[i].set_xticklabels([f\"{cid}\" for cid in range(num_communities)])\n",
    "    #     axs[i].set_ylabel(\"Average infected nodes\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_dicts = [SI_001_extracted, SI_01_extracted, SI_05_extracted, SI_1_extracted, SI_2_extracted]#, SI_5_extracted]\n",
    "labels = ['0.01', '0.1', '0.5', '1.0', '2.0']#, '5.0']\n",
    "\n",
    "plot_influence_multiple_runs(influence_dicts, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_against_ratio = {\n",
    "    proposal_id: for_votes[proposal_id] / against_votes[proposal_id] if against_votes[proposal_id] != 0 else None\n",
    "    for proposal_id in for_votes\n",
    "}\n",
    "\n",
    "for i in for_against_ratio:\n",
    "    if for_against_ratio[i] is not None and for_against_ratio[i] < 100:\n",
    "        print(i, for_against_ratio[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter proposals where the number of votes against was greater than 100000\n",
    "proposals_against_gt_100k = [\n",
    "    proposal for proposal in aave_v2_proposals if float(proposal['currentNoVote']) > 100000\n",
    "] + [\n",
    "    proposal for proposal in aave_v3_proposals if proposal['votes'] and float(proposal['votes']['againstVotes']) > 100000\n",
    "]\n",
    "\n",
    "print(\"Proposals with votes against greater than 100,000:\")\n",
    "for proposal in proposals_against_gt_100k:\n",
    "    print(proposal)\n",
    "\n",
    "# Filter proposals where the number of voters was greater than 300\n",
    "# proposals_voters_gt_300 = [\n",
    "#     proposal for proposal in aave_v2_proposals if proposal['totalCurrentVoters'] > 300\n",
    "# ] + [\n",
    "#     proposal for proposal in aave_v3_proposals if proposal['totalCurrentVoters'] > 300\n",
    "# ]\n",
    "\n",
    "# print(\"\\nProposals with voters greater than 300:\")\n",
    "# for proposal in proposals_voters_gt_300:\n",
    "#     print(proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in aave_v2_proposals:\n",
    "    id = p['id']\n",
    "    title = p['title']\n",
    "    print(f'{id}: {title}')\n",
    "    \n",
    "for p in aave_v3_proposals:\n",
    "    id = p['id']\n",
    "    title = p['proposalMetadata']['title']\n",
    "    print(f'{id}: {title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    \"Treasury & Financial Management\": [\n",
    "        \"treasury\", \"funding\", \"gho\", \"liquidity\", \n",
    "        \"safety module\", \"emissions\", \"finance update\"\n",
    "    ],\n",
    "    \"Governance Ops & Service Providers\": [\n",
    "        \"emission manager\", \"governance\", \"tokenlogic\", \n",
    "        \"chaos labs\", \"gauntlet\", \"service provider\", \"proposal activation\"\n",
    "    ],\n",
    "    \"Risk & Parameter Updates\": [\n",
    "        \"risk parameter\", \"reserve factor\", \"interest rate\", \"borrow cap\", \n",
    "        \"lt/ltv\", \"debt ceiling\", \"onboard\", \"offboard\", \"deprecate\"\n",
    "    ],\n",
    "    \"Protocol/Technical Upgrades\": [\n",
    "        \"upgrade\", \"activation\", \"maintenance\", \"deprecation\", \n",
    "        \"periphery\", \"adapter\", \"implementation\", \"bug\"\n",
    "    ],\n",
    "    \"Security\": [\n",
    "        \"security\", \"sentinel\", \"emergency admin\", \n",
    "        \"freeze\", \"bug bounty\"\n",
    "    ],\n",
    "    \"Misc/Experimental\": [\n",
    "        \"orbit program\", \"event\", \"competition\", \n",
    "        \"strategy\", \"liquidity incentive\", \"embassy\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# CATEGORIES = {\n",
    "#     \"Treasury & Financial Management\": [\n",
    "#         \"treasury\", \"funding\", \"gho\", \"liquidity\", \"allowance\", \n",
    "#         \"transfer\", \"financial\", \"payment\", \"revenue\"\n",
    "#     ],\n",
    "#     \"Governance & Service Providers\": [\n",
    "#         \"governance\", \"emission manager\", \"provider\", \"tokenlogic\", \n",
    "#         \"gauntlet\", \"chaos labs\", \"aci\", \"delegate\", \"committee\"\n",
    "#     ],\n",
    "#     \"Risk & Parameter Updates\": [\n",
    "#         \"risk parameter\", \"lt/ltv\", \"lt reduction\", \"debt ceiling\",\n",
    "#         \"reserve factor\", \"interest rate\", \"borrow cap\", \n",
    "#         \"supply cap\", \"price feed\", \"curve amendment\"\n",
    "#     ],\n",
    "#     \"Protocol/Technical Upgrades\": [\n",
    "#         \"upgrade\", \"activation\", \"implementation\", \"periphery\", \n",
    "#         \"adapter\", \"robot\", \"migration\", \"scroll\", \"maintenance\", \n",
    "#         \"freeze\", \"deprecation\", \"bug\"\n",
    "#     ],\n",
    "#     \"Security & Emergency\": [\n",
    "#         \"security\", \"sentinel\", \"emergency\", \"bounty\", \"admin\", \"incident\"\n",
    "#     ],\n",
    "#     \"Miscellaneous / Ecosystem\": [\n",
    "#         \"orbit\", \"event\", \"competition\", \"embassy\", \n",
    "#         \"strategy\", \"superfest\", \"program\", \"grant\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "def categorize_proposal(title):\n",
    "    title_lower = title.lower()\n",
    "    matched = [cat for cat, keys in CATEGORIES.items() if any(k in title_lower for k in keys)]\n",
    "    return matched if matched else [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dicts = defaultdict(dict)\n",
    "\n",
    "for proposal in aave_v2_proposals:\n",
    "    id = proposal['id']\n",
    "    title = proposal['title']\n",
    "    # if categorize_proposal(title) == ['None']:\n",
    "    #     print(id, categorize_proposal(proposal['description']))\n",
    "    # print(f'{id}: {categorize_proposal(title)}')\n",
    "    \n",
    "    categories = categorize_proposal(title)\n",
    "    \n",
    "    if categories == [\"\"]:\n",
    "        continue\n",
    "    \n",
    "    cat = categories[0]\n",
    "    category_dicts[cat][proposal['id']] = {\n",
    "        \"timestamp\": proposal['timestamp'],\n",
    "        \"voters\": proposal['totalCurrentVoters'],\n",
    "        \"state\": proposal['state'],\n",
    "        \"for_votes\": proposal['currentYesVote'],\n",
    "        \"against_votes\": proposal['currentNoVote']\n",
    "    }\n",
    "    \n",
    "# print('')\n",
    "    \n",
    "for proposal in aave_v3_proposals:\n",
    "    id = proposal['id']\n",
    "    title = proposal['proposalMetadata']['title']\n",
    "    # if categorize_proposal(title) == ['None']:\n",
    "    #     print(id, categorize_proposal(proposal['proposalMetadata']['rawContent']))\n",
    "    # print(f'{id}: {categorize_proposal(title)}')\n",
    "    \n",
    "    categories = categorize_proposal(title)\n",
    "    \n",
    "    if categories == [\"\"]:\n",
    "        continue\n",
    "    \n",
    "    cat = categories[0]\n",
    "    category_dicts[cat][proposal['id']] = {\n",
    "        \"timestamp\": int(proposal['transactions']['created']['timestamp']),\n",
    "        \"voters\": proposal['totalCurrentVoters'],\n",
    "        \"state\": proposal['state'],\n",
    "        \"for_votes\": proposal['votes']['forVotes'] if proposal['votes'] else 0,\n",
    "        \"against_votes\": proposal['votes']['againstVotes'] if proposal['votes'] else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "governance_service_dict = category_dicts['Governance Ops & Service Providers']\n",
    "treasury_dict = category_dicts['Treasury & Financial Management']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = Counter()\n",
    "for proposal in aave_v2_proposals + aave_v3_proposals:\n",
    "    title = proposal['title'] if 'title' in proposal else proposal['proposalMetadata']['title']\n",
    "    categories = categorize_proposal(title)\n",
    "    for category in categories:\n",
    "        category_counts[category] += 1\n",
    "\n",
    "categories, counts = zip(*sorted(category_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(categories, counts, color='skyblue')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Proposals')\n",
    "plt.title('Distribution of proposals over categories')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(G):\n",
    "    max_weight = max(data['weight'] for _, _, _, data in G.edges(keys=True, data=True))\n",
    "    \n",
    "    for _, _, _, data in G.edges(keys=True, data=True):\n",
    "        data['norm_weight'] = data['weight'] / max_weight\n",
    "    \n",
    "    return G\n",
    "\n",
    "def log_normalize_weights(G):\n",
    "    max_weight = np.log1p(max(data['weight'] for _, _, _, data in G.edges(keys=True, data=True)))\n",
    "    \n",
    "    for _, _, _, data in G.edges(keys=True, data=True):\n",
    "        data['norm_weight'] = np.log1p(data['weight']) / max_weight\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple SI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = copy.deepcopy(G_aave_complete)\n",
    "# G_T = copy.deepcopy(G_aave_complete)\n",
    "\n",
    "G = log_normalize_weights(G)\n",
    "# G_T = normalize_weights(G_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_out_edges(G, beta=1.0):\n",
    "    out_edges = defaultdict(list)\n",
    "    \n",
    "    for u, v, k, d in G.edges(keys=True, data=True):\n",
    "        out_edges[u].append((v, d['timestamp'], beta * d['norm_weight']))\n",
    "    \n",
    "    return out_edges\n",
    "\n",
    "def simulate_SI(G, seed_node, beta=1.0, cached_edges=None):\n",
    "    infected = {}\n",
    "    infection_queue = []\n",
    "\n",
    "    infected[seed_node] = 1696118400\n",
    "    heapq.heappush(infection_queue, (1696118400, seed_node))\n",
    "\n",
    "    while infection_queue:\n",
    "        curr_time, node = heapq.heappop(infection_queue)\n",
    "        \n",
    "        if node not in cached_edges:\n",
    "            continue\n",
    "        \n",
    "        for neighbor, ts, prob in cached_edges[node]:\n",
    "        # for _, neighbor, _, data in G.out_edges(node, keys=True, data=True):\n",
    "            if neighbor in infected:\n",
    "                continue\n",
    "            \n",
    "            # ts = data['timestamp']\n",
    "            # prob = beta * data['norm_weight']\n",
    "\n",
    "            if ts >= curr_time:\n",
    "                if np.random.rand() < prob:\n",
    "                    infected[neighbor] = ts\n",
    "                    heapq.heappush(infection_queue, (ts, neighbor))\n",
    "\n",
    "    return infected\n",
    "\n",
    "def run_multiple_iterations(G, num_iterations=100, beta=1.0):\n",
    "    # all_nodes = list(G.nodes)\n",
    "    all_nodes = list(all_proposers_voters_delegators_in_G_aave)\n",
    "    infection_counts = defaultdict(list)\n",
    "    infections_per_iteration = defaultdict(list)\n",
    "    \n",
    "    cached_edges = preprocess_out_edges(G, beta)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"Iteration {i+1}/{num_iterations}\")\n",
    "        # for seed in tqdm(all_nodes, desc=f\"SI iteration {i+1}\"):\n",
    "        for seed in all_nodes:\n",
    "            infected = simulate_SI(G, seed, beta, cached_edges)\n",
    "            infection_counts[seed].append(len(infected))\n",
    "            infections_per_iteration[seed].append(infected)\n",
    "\n",
    "    avg_results = [{\n",
    "        'seed': node,\n",
    "        'avg_infected': sum(counts) / len(counts),\n",
    "        'infections_per_iteration': infections_per_iteration[node]\n",
    "        # 'all_infected_counts': counts\n",
    "    } for node, counts in infection_counts.items()]\n",
    "\n",
    "    top_avg_influencers = sorted(avg_results, key=lambda x: x['avg_infected'], reverse=True)\n",
    "\n",
    "    return top_avg_influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_5 = run_multiple_iterations(G, num_iterations=10, beta=5.0)\n",
    "\n",
    "with open('data/SI/aave/SI_5.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_5, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_5_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_5]\n",
    "\n",
    "with open('data/SI/aave/SI_5_extracted_seeds_avg_infected.pkl', 'wb') as file:\n",
    "    pickle.dump(SI_5_extracted, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_001_1_outcome = joblib.load('data/SI/aave/SI_001_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_001_1_extracted = [{'seed': entry['seed'], 'avg_infected': entry['avg_infected']} for entry in SI_001_1_outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_SIZE = timedelta(days=1)\n",
    "\n",
    "def plot_infections(infections):\n",
    "    binned_iterations = []\n",
    "\n",
    "    for iteration in infections['infections_per_iteration']:\n",
    "        bin_counts = defaultdict(int)\n",
    "        for ts in iteration.values():\n",
    "            t = datetime.fromtimestamp(ts)\n",
    "            bin_time = datetime(t.year, t.month, t.day)\n",
    "            bin_counts[bin_time] += 1\n",
    "        binned_iterations.append(bin_counts)\n",
    "\n",
    "    all_bins = sorted(set().union(*[d.keys() for d in binned_iterations]))\n",
    "\n",
    "    counts_matrix = []\n",
    "    for bins in binned_iterations:\n",
    "        counts = [bins.get(t, 0) for t in all_bins]\n",
    "        counts_matrix.append(counts)\n",
    "\n",
    "    counts_matrix = np.array(counts_matrix)\n",
    "\n",
    "    mean_counts = counts_matrix.mean(axis=0)\n",
    "    std_counts = counts_matrix.std(axis=0)\n",
    "    cumulative_mean = np.cumsum(mean_counts)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(all_bins, mean_counts, label='Mean infection rate')\n",
    "    plt.fill_between(all_bins, mean_counts - std_counts, mean_counts + std_counts, alpha=0.2, label='Standard deviation')\n",
    "    # plt.title('Average infection rate over time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Infections')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(all_bins, cumulative_mean, label='Mean cumulative')\n",
    "    # plt.title('Average cumulative infections over time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative infections')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in SI_001_1_outcome[:10]:\n",
    "    plot_infections(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_SIZE = timedelta(days=1)\n",
    "\n",
    "def plot_infections_average(infections_list):\n",
    "    all_binned_counts = []\n",
    "\n",
    "    for infections in infections_list:\n",
    "        for iteration in infections['infections_per_iteration']:\n",
    "            bin_counts = defaultdict(int)\n",
    "            for ts in iteration.values():\n",
    "                t = datetime.fromtimestamp(ts)\n",
    "                bin_time = datetime(t.year, t.month, t.day)\n",
    "                bin_counts[bin_time] += 1\n",
    "            all_binned_counts.append(bin_counts)\n",
    "\n",
    "    all_bins = sorted(set().union(*[d.keys() for d in all_binned_counts]))\n",
    "\n",
    "    counts_matrix = []\n",
    "    for bin_counts in all_binned_counts:\n",
    "        counts = [bin_counts.get(t, 0) for t in all_bins]\n",
    "        counts_matrix.append(counts)\n",
    "\n",
    "    counts_matrix = np.array(counts_matrix)\n",
    "    mean_counts = counts_matrix.mean(axis=0)\n",
    "    std_counts = counts_matrix.std(axis=0)\n",
    "    cumulative_mean = np.cumsum(mean_counts)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(all_bins, mean_counts, label='Mean infection rate')\n",
    "    plt.fill_between(all_bins, mean_counts - std_counts, mean_counts + std_counts, alpha=0.2, label='Standard deviation')\n",
    "    # plt.title('Average infection rate over time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Infections')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(all_bins, cumulative_mean, label='Mean cumulative')\n",
    "    # plt.title('Average cumulative infections over time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative infections')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_infections_average(SI_001_1_outcome[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_001_outcome = joblib.load('data/SI/aave/SI_001_extracted_seeds_avg_infected.pkl')\n",
    "SI_01_outcome = joblib.load('data/SI/aave/SI_01_extracted_seeds_avg_infected.pkl')\n",
    "SI_05_outcome = joblib.load('data/SI/aave/SI_05_extracted_seeds_avg_infected.pkl')\n",
    "SI_1_outcome = joblib.load('data/SI/aave/SI_1_extracted_seeds_avg_infected.pkl')\n",
    "SI_2_outcome = joblib.load('data/SI/aave/SI_2_extracted_seeds_avg_infected.pkl')\n",
    "SI_5_outcome = joblib.load('data/SI/aave/SI_5_extracted_seeds_avg_infected.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_infected_001 = [entry['avg_infected'] for entry in SI_001_outcome]\n",
    "avg_infected_01 = [entry['avg_infected'] for entry in SI_01_outcome]\n",
    "avg_infected_05 = [entry['avg_infected'] for entry in SI_05_outcome]\n",
    "avg_infected_1 = [entry['avg_infected'] for entry in SI_1_outcome]\n",
    "\n",
    "# avg_infected_001_T = [entry['avg_infected'] for entry in SI_T_001_outcome]\n",
    "# avg_infected_01_T = [entry['avg_infected'] for entry in SI_T_01_outcome]\n",
    "# avg_infected_05_T = [entry['avg_infected'] for entry in SI_T_05_outcome]\n",
    "# avg_infected_1_T = [entry['avg_infected'] for entry in SI_T_1_outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_scores = {\n",
    "    '=0.01': {seed['seed']: seed['avg_infected'] for seed in SI_001_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_aave},\n",
    "    '=0.1': {seed['seed']: seed['avg_infected'] for seed in SI_01_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_aave},\n",
    "    '=0.5': {seed['seed']: seed['avg_infected'] for seed in SI_05_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_aave},\n",
    "    '=1.0': {seed['seed']: seed['avg_infected'] for seed in SI_1_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_aave},\n",
    "    '=2.0': {seed['seed']: seed['avg_infected'] for seed in SI_2_outcome if seed['seed'] in all_proposers_voters_delegators_in_G_aave},\n",
    "    # '5.0': {seed['seed']: seed['avg_infected'] for seed in SI_5_outcome}\n",
    "}\n",
    "\n",
    "influence_df = pd.DataFrame(influence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_corr = influence_df.corr(method='spearman')\n",
    "kendall_corr = influence_df.corr(method='kendall')\n",
    "# pearson_corr = influence_df.corr(method='pearson')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(spearman_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, ax=axs[0])\n",
    "axs[0].set_title(\"Spearman correlation\")\n",
    "\n",
    "sns.heatmap(kendall_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, ax=axs[1])\n",
    "axs[1].set_title(\"Kendall correlation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman_corr = influence_df.corr(method='spearman')\n",
    "kendall_corr = influence_df.corr(method='kendall')\n",
    "# pearson_corr = influence_df.corr(method='pearson')\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "# plt.title(\"Spearman correlation\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(kendall_corr, annot=True, cmap=\"coolwarm\", cbar=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "# plt.title(\"Kendall correlation\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    '0.01': SI_001_outcome,\n",
    "    '0.1': SI_01_outcome,\n",
    "    '0.5': SI_05_outcome,\n",
    "    '1.0': SI_1_outcome,\n",
    "    '2.0': SI_2_outcome,\n",
    "    '5.0': SI_5_outcome\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_comparison_per_beta(datasets):\n",
    "    hist_data = {}\n",
    "    \n",
    "    for beta, dataset in datasets.items():\n",
    "        temp = [i['avg_infected'] for i in dataset if i['seed'] in all_proposers_voters_delegators_in_G_aave]\n",
    "        hist_data[beta] = temp\n",
    "        \n",
    "    fig, axs = plt.subplots(1, 6, figsize=(48, 6))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for ax, (title, data) in zip(axs, hist_data.items()):\n",
    "        ax.hist(data, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Avg. infected nodes')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_comparison_per_beta(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_comparison_per_config(datasets):\n",
    "    for beta, dataset in datasets.items():\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(24, 6))\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        groups = {\n",
    "            'EOA governance': [i['avg_infected'] for i in dataset if i['seed'] in EOA and i['seed'] in all_proposers_voters_delegators_in_G_aave and i['seed'] not in CEX],\n",
    "            'EOA non-governance': [i['avg_infected'] for i in dataset if i['seed'] in EOA and i['seed'] not in all_proposers_voters_delegators_in_G_aave and i['seed'] not in CEX],\n",
    "            # 'CA': [i['avg_infected'] for i in dataset if i['seed'] in CA],\n",
    "            # 'CEX': [i['avg_infected'] for i in dataset if i['seed'] in CEX],\n",
    "            'Infrastructural': [i['avg_infected'] for i in dataset if i['seed'] in CEX or i['seed'] in CA]\n",
    "        }\n",
    "\n",
    "        for ax, (title, data) in zip(axs, groups.items()):\n",
    "            ax.hist(data, bins=30, alpha=0.7, edgecolor='black', log=True)\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Avg. infected nodes')\n",
    "            ax.set_ylabel('Frequency')\n",
    "\n",
    "        fig.suptitle(f'={beta}')#, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_comparison_per_config(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer SI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_1 = copy.deepcopy(G_aave_complete)\n",
    "G_2 = copy.deepcopy(G_aWETH_complete)\n",
    "\n",
    "G_1 = normalize_weights(G_1)\n",
    "G_2 = normalize_weights(G_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_1_log = copy.deepcopy(G_aave_complete)\n",
    "G_2_log = copy.deepcopy(G_aWETH_complete)\n",
    "\n",
    "G_1_log = log_normalize_weights(G_1_log)\n",
    "G_2_log = log_normalize_weights(G_2_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_SI_combined = nx.MultiDiGraph()\n",
    "\n",
    "for u, v, data in G_1.edges(data=True):\n",
    "    G_SI_combined.add_edge((u, \"gov\"), (v, \"gov\"), **data)\n",
    "    \n",
    "for u, v, data in G_2.edges(data=True):\n",
    "    G_SI_combined.add_edge((u, \"fin\"), (v, \"fin\"), **data)\n",
    "\n",
    "users_in_both = set(G_1.nodes()).intersection(set(G_2.nodes()))\n",
    "\n",
    "for user in users_in_both:\n",
    "    G_SI_combined.add_edge((user, \"gov\"), (user, \"fin\"), layer_bridge=True)\n",
    "    G_SI_combined.add_edge((user, \"fin\"), (user, \"gov\"), layer_bridge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_SI_log = nx.MultiDiGraph()\n",
    "\n",
    "for u, v, data in G_1_log.edges(data=True):\n",
    "    G_SI_log.add_edge((u, \"gov\"), (v, \"gov\"), **data)\n",
    "    \n",
    "for u, v, data in G_2_log.edges(data=True):\n",
    "    G_SI_log.add_edge((u, \"fin\"), (v, \"fin\"), **data)\n",
    "\n",
    "users_in_both = set(G_1_log.nodes()).intersection(set(G_2_log.nodes()))\n",
    "\n",
    "for user in users_in_both:\n",
    "    G_SI_log.add_edge((user, \"gov\"), (user, \"fin\"), layer_bridge=True)\n",
    "    G_SI_log.add_edge((user, \"fin\"), (user, \"gov\"), layer_bridge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_si_multilayer_simulation(G, seed_nodes, p_inter=0.2, weight_attr='norm_weight', timestamp_attr='timestamp', max_time=None):\n",
    "    # infection_info = {node: {\"time\": 1696118400, \"layer\": node[1]} for node in seed_nodes}\n",
    "    infection_info = {node: 1696118400 for node in seed_nodes}\n",
    "    visited_interlayer = set()\n",
    "\n",
    "    queue = []\n",
    "\n",
    "    for u in seed_nodes:\n",
    "        for _, v, data in G.out_edges(u, data=True):\n",
    "            if data.get('layer_bridge'):\n",
    "                continue\n",
    "            \n",
    "            t_edge = data[timestamp_attr]\n",
    "            \n",
    "            if max_time is None or t_edge <= max_time:\n",
    "                heapq.heappush(queue, (t_edge, u, v, False))\n",
    "                \n",
    "        for _, v, data in G.out_edges(u, data=True):\n",
    "            if data.get('layer_bridge') and v not in infection_info:\n",
    "                    heapq.heappush(queue, (1696118400, u, v, True))\n",
    "\n",
    "    while queue:\n",
    "        t, u, v, is_inter = heapq.heappop(queue)\n",
    "        \n",
    "        if v in infection_info:\n",
    "            continue\n",
    "\n",
    "        if infection_info.get(u, float('inf')) > t:\n",
    "            continue\n",
    "\n",
    "        if is_inter:\n",
    "            if v in visited_interlayer:\n",
    "                continue\n",
    "\n",
    "            if random.random() <= p_inter:\n",
    "                infection_info[v] = t\n",
    "                visited_interlayer.add(v)\n",
    "\n",
    "                for _, w, data in G.out_edges(v, data=True):\n",
    "                    if data.get('layer_bridge'):\n",
    "                        continue\n",
    "                    \n",
    "                    t_edge = data[timestamp_attr]\n",
    "                    \n",
    "                    if max_time is not None and t_edge > max_time:\n",
    "                        continue\n",
    "                    \n",
    "                    heapq.heappush(queue, (t_edge, v, w, False))\n",
    "\n",
    "        else:\n",
    "            # Use the edge with the earliest timestamp (choose the first if multiple)\n",
    "            edge_data = G.get_edge_data(u, v)\n",
    "            if not edge_data:\n",
    "                continue\n",
    "\n",
    "            p = edge_data[0].get(weight_attr, 0)\n",
    "            if random.random() <= p:\n",
    "                infection_info[v] = t\n",
    "\n",
    "                # Immediately try inter-layer infection\n",
    "                for _, cross_v, data in G.out_edges(v, data=True):\n",
    "                    if data.get('layer_bridge') and cross_v not in infection_info:\n",
    "                        heapq.heappush(queue, (t, v, cross_v, True))\n",
    "\n",
    "                # Schedule intra-layer infections\n",
    "                for _, w, data in G.out_edges(v, data=True):\n",
    "                    if data.get('layer_bridge'):\n",
    "                        continue\n",
    "                    \n",
    "                    t_edge = data[timestamp_attr]\n",
    "                    \n",
    "                    if max_time is not None and t_edge > max_time:\n",
    "                        continue\n",
    "                    \n",
    "                    heapq.heappush(queue, (t_edge, v, w, False))\n",
    "\n",
    "    return infection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_nodes = random.sample([n for n in G_SI_combined.nodes if n[1] == \"gov\"], 100)\n",
    "seed_nodes = [(node['seed'], \"gov\") for node in top_spreaders_AAVE[:100]]\n",
    "# seed_nodes = [(node, \"gov\") for node in G_1.nodes]\n",
    "# seed_nodes = [(\"0x6980a47bee930a4584b09ee79ebe46484fbdbdd0\", \"gov\")] #(\"0x28c6c06298d514db089934071355e5743bf21d60\", \"gov\")]\n",
    "\n",
    "# result = run_si_multilayer_simulation(G_SI_combined, seed_nodes, p_inter=0.5)\n",
    "result = run_si_multilayer_simulation(G_SI_log, seed_nodes, p_inter=0.5)\n",
    "\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = list(result.values())\n",
    "counts = Counter(times)\n",
    "xs = sorted(counts)\n",
    "ys = [counts[x] for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"New infections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_counts = defaultdict(int)\n",
    "for node in result:\n",
    "    layer = node[1]\n",
    "    layer_counts[layer] += 1\n",
    "print(layer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_infection_df(infection_times):\n",
    "    records = []\n",
    "    for (user, layer), t in infection_times.items():\n",
    "        records.append({\"user\": user, \"layer\": layer, \"timestamp\": t})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df_infections = prepare_infection_df(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_infection_curve(df, total_nodes):\n",
    "    df_sorted = df.sort_values(\"timestamp\")\n",
    "    t0 = df_sorted[\"timestamp\"].min()\n",
    "    df_sorted[\"days\"] = (df_sorted[\"timestamp\"] - t0) / 86400\n",
    "    df_sorted[\"cumulative\"] = range(1, len(df_sorted)+1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df_sorted[\"days\"], df_sorted[\"cumulative\"] / total_nodes * 100, label=\"Total\")\n",
    "\n",
    "    plt.xlabel(\"Days Since First Infection\")\n",
    "    plt.ylabel(\"% of All Nodes Infected\")\n",
    "    plt.title(\"Infection Spread Over Time\")\n",
    "    # plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_infection_curve(df_infections, total_nodes=G_SI_log.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layerwise_infection(df, total_node_dict):\n",
    "    df = df.sort_values(\"timestamp\")\n",
    "    t0 = df[\"timestamp\"].min()\n",
    "    df[\"days\"] = (df[\"timestamp\"] - t0) / 86400\n",
    "\n",
    "    df[\"count\"] = 1\n",
    "    df_grouped = df.groupby([\"days\", \"layer\"]).count().reset_index()\n",
    "    df_grouped[\"cumulative\"] = df_grouped.groupby(\"layer\")[\"count\"].cumsum()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for layer in df[\"layer\"].unique():\n",
    "        subset = df_grouped[df_grouped[\"layer\"] == layer]\n",
    "        total_layer_nodes = total_node_dict.get(layer, 1)  # avoid division by 0\n",
    "        plt.plot(subset[\"days\"], subset[\"cumulative\"] / total_layer_nodes * 100, label=layer)\n",
    "\n",
    "    plt.xlabel(\"Days Since First Infection\")\n",
    "    plt.ylabel(\"% of All Nodes in Layer Infected\")\n",
    "    plt.title(\"Layer-wise Infection Spread Over Time\")\n",
    "    plt.legend()\n",
    "    # plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "total_node_dict = {\n",
    "    \"gov\": len([n for n in G_SI_log.nodes if n[1] == \"gov\"]),\n",
    "    \"fin\": len([n for n in G_SI_log.nodes if n[1] == \"fin\"]),\n",
    "}\n",
    "\n",
    "plot_layerwise_infection(df_infections, total_node_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_minus_ranking(timestamps):\n",
    "    \"\"\"\n",
    "     : newer  larger value, then normalised 01\n",
    "    \"\"\"\n",
    "    sorted_ts = sorted(timestamps, reverse=True)\n",
    "    rank_map   = {ts: idx + 1 for idx, ts in enumerate(sorted_ts)}\n",
    "    max_rank   = len(timestamps)\n",
    "    return [(max_rank - rank_map[ts] + 1) / max_rank for ts in timestamps]\n",
    "\n",
    "def eta_plus_linear(weights, eps=1e-9):\n",
    "    \"\"\"\n",
    "    + : linear but attenuated () so smallweight edges still get probability.\n",
    "    \"\"\"\n",
    "    max_w = max(weights)# + eps\n",
    "    return [math.sqrt(w / max_w) for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_random_walk_multilayer(G, start_node, steps, p_switch=0.2, alpha=0.5, weight_attr='weight', timestamp_attr='timestamp'):\n",
    "    current_node = start_node\n",
    "    walk = [current_node]\n",
    "\n",
    "    for _ in range(steps):\n",
    "        neighbors = list(G.successors(current_node))\n",
    "        if not neighbors:\n",
    "            break\n",
    "\n",
    "        # Separate inter-layer and intra-layer edges\n",
    "        intra_layer_edges = []\n",
    "        inter_layer_edges = []\n",
    "        for neighbor in neighbors:\n",
    "            edge_data = G.get_edge_data(current_node, neighbor)\n",
    "            if edge_data:\n",
    "                if edge_data[0].get('layer_bridge', False):\n",
    "                    inter_layer_edges.append((neighbor, edge_data[0]))\n",
    "                else:\n",
    "                    intra_layer_edges.append((neighbor, edge_data[0]))\n",
    "\n",
    "        # Decide whether to switch layers or stay in the current layer\n",
    "        if inter_layer_edges and random.random() < p_switch:\n",
    "            # Switch layers\n",
    "            next_node = random.choice(inter_layer_edges)[0]\n",
    "        elif intra_layer_edges:\n",
    "            # Stay in the current layer\n",
    "            # Calculate probabilities based on weights and timestamps\n",
    "            weights = [edge[1][weight_attr] for edge in intra_layer_edges]\n",
    "            timestamps = [edge[1][timestamp_attr] for edge in intra_layer_edges]\n",
    "            \n",
    "            eta_t  = eta_minus_ranking(timestamps)\n",
    "            P_T    = [x / sum(eta_t) for x in eta_t]\n",
    "            \n",
    "            eta_w  = eta_plus_linear(weights)\n",
    "            P_W    = [x / sum(eta_w) for x in eta_w]\n",
    "            \n",
    "            P_e = [(p_t**alpha)*(p_w**(1-alpha)) for p_t,p_w in zip(P_T,P_W)]\n",
    "            P_e = [p/sum(P_e) for p in P_e]      # normalise\n",
    "            next_node = random.choices(intra_layer_edges, weights=P_e)[0]\n",
    "        else:\n",
    "            # No valid edges to follow\n",
    "            break\n",
    "\n",
    "        walk.append(next_node)\n",
    "        current_node = next_node\n",
    "\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('data/AAVE_daily_USD.csv', parse_dates=['snapped_at'])\n",
    "\n",
    "price_data['date'] = price_data['snapped_at'].dt.date\n",
    "\n",
    "aave_df['date'] = pd.to_datetime(aave_df['timestamp'], unit='s')\n",
    "aave_df['date'] = aave_df['date'].dt.date\n",
    "\n",
    "merged_data_aave = aave_df.groupby('date').size().reset_index(name='num_entries')\n",
    "merged_data_aave = aave_df.groupby('date').agg(num_entries=('value', 'count'), total_value=('value', 'sum')).reset_index()\n",
    "merged_data_aave = merged_data_aave.merge(price_data[['date', 'price']], on='date', how='left')\n",
    "merged_data_aave['total_traded_value'] = merged_data_aave['total_value'] * merged_data_aave['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax1.plot(merged_data_aave['date'], merged_data_aave['num_entries'])\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of transfers')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(merged_data_aave['date'], merged_data_aave['price'], color='red')\n",
    "# ax2.set_ylabel('Price', color='red')\n",
    "# ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# plt.title('Number of AAVE transfers per day')\n",
    "plt.show()\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# ax1.plot(merged_data_aave['date'], merged_data_aave['total_value'])\n",
    "# ax1.set_xlabel('Date')\n",
    "# ax1.set_ylabel('Number of tokens')\n",
    "# ax1.tick_params(axis='y')\n",
    "\n",
    "# # ax2 = ax1.twinx()\n",
    "# # ax2.plot(merged_data_aave['date'], merged_data_aave['price'], color='red')\n",
    "# # ax2.set_ylabel('Price', color='red')\n",
    "# # ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# plt.title('Number of AAVE tokens transferred per day and price over time')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged_data_aave['date'], merged_data_aave['total_traded_value'])\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Total traded value in USD')\n",
    "# plt.title('Total AAVE value traded per day')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aave_addresses = pd.concat([aave_df['from'], aave_df['to']])\n",
    "aToken_addresses = pd.concat([aToken_df['from'], aToken_df['to']])\n",
    "\n",
    "aave_transaction_counts = aave_addresses.value_counts()\n",
    "aToken_transaction_counts = aToken_addresses.value_counts()\n",
    "\n",
    "voters_set = set(vote_counts.keys())\n",
    "\n",
    "aave_voters_transaction_counts = aave_transaction_counts[aave_transaction_counts.index.isin(voters_set)]\n",
    "aave_non_voters_transaction_counts = aave_transaction_counts[~aave_transaction_counts.index.isin(voters_set)]\n",
    "\n",
    "aWETH_voters_transaction_counts = aToken_transaction_counts[aToken_transaction_counts.index.isin(voters_set)]\n",
    "aWETH_non_voters_transaction_counts = aToken_transaction_counts[~aToken_transaction_counts.index.isin(voters_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aave_voters_transaction_counts, bins=100, color='blue', label='Voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Number of transactions per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of transactions per user\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aave_non_voters_transaction_counts, bins=100, color='red', label='Non-voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Number of transactions per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of transactions per user\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aave_avg_transaction_values = aave_df.groupby('from')['value'].mean().add(\n",
    "    aave_df.groupby('to')['value'].mean(), fill_value=0\n",
    ") / 2\n",
    "\n",
    "aave_voters_avg_transaction_values = aave_avg_transaction_values[aave_avg_transaction_values.index.isin(voters_set)]\n",
    "aave_non_voters_avg_transaction_values = aave_avg_transaction_values[~aave_avg_transaction_values.index.isin(voters_set)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aave_voters_avg_transaction_values, bins=100, color='blue', label='Voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Average transaction value per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of average transaction values per user (Voters)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aave_non_voters_avg_transaction_values, bins=100, color='red', label='Non-voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Average transaction value per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of average transaction values per user (Non-voters)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(aave_df['value'], bins=50)#, log_scale=(True, False))\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of AAVE token transfer values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aave_df['value'], bins=50, edgecolor='black')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of weights')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.log1p(aave_df['value']), bins=50)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Log1p(Value)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of log1p weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aWETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('data/WETH_daily_USD.csv', parse_dates=['snapped_at'])\n",
    "\n",
    "price_data['date'] = price_data['snapped_at'].dt.date\n",
    "\n",
    "aToken_df['date'] = pd.to_datetime(aToken_df['timestamp'], unit='s')\n",
    "aToken_df['date'] = aToken_df['date'].dt.date\n",
    "\n",
    "merged_data_WETH = aToken_df.groupby('date').size().reset_index(name='num_entries')\n",
    "merged_data_WETH = aToken_df.groupby('date').agg(num_entries=('value', 'count'), total_value=('value', 'sum')).reset_index()\n",
    "merged_data_WETH = merged_data_WETH.merge(price_data[['date', 'price']], on='date', how='left')\n",
    "merged_data_WETH['total_traded_value'] = merged_data_WETH['total_value'] * merged_data_WETH['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax1.plot(merged_data_WETH['date'], merged_data_WETH['num_entries'])\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of transfers')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(merged_data_WETH['date'], merged_data_WETH['price'], color='red')\n",
    "# ax2.set_ylabel('Price', color='red')\n",
    "# ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title('Number of WETH transfers per day and price over time')\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax1.plot(merged_data_WETH['date'], merged_data_WETH['total_value'])\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of tokens')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(merged_data_WETH['date'], merged_data_WETH['price'], color='red')\n",
    "# ax2.set_ylabel('Price', color='red')\n",
    "# ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title('Number of WETH tokens transferred per day and price over time')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_data_WETH['date'], merged_data_WETH['total_traded_value'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total traded value in USD')\n",
    "plt.title('Total WETH value traded per day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aWETH_voters_transaction_counts, bins=100, color='blue', label='Voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Number of transactions per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of transactions per user\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aWETH_non_voters_transaction_counts, bins=100, color='red', label='Non-voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Number of transactions per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of transactions per user\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aWETH_avg_transaction_values = aToken_df.groupby('from')['value'].mean().add(\n",
    "    aToken_df.groupby('to')['value'].mean(), fill_value=0\n",
    ") / 2\n",
    "\n",
    "aWETH_voters_avg_transaction_values = aWETH_avg_transaction_values[aWETH_avg_transaction_values.index.isin(voters_set)]\n",
    "aWETH_non_voters_avg_transaction_values = aWETH_avg_transaction_values[~aWETH_avg_transaction_values.index.isin(voters_set)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aWETH_voters_avg_transaction_values, bins=100, color='blue', label='Voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Average transaction value per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of average transaction values per user (Voters)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(aWETH_non_voters_avg_transaction_values, bins=100, color='red', label='Non-voters', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Average transaction value per user\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of average transaction values per user (Non-voters)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposer_counts = pd.Series([proposal['user']['id'] for proposal in aave_v2_proposals] + \n",
    "                            [proposal['creator'] for proposal in aave_v3_proposals]).value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "proposer_counts.plot(kind='bar')\n",
    "plt.xlabel('Proposer')\n",
    "plt.ylabel('Number of proposals created')\n",
    "plt.title('Distribution of proposals created by each proposer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_counts = pd.Series([vote['id'].split('-')[0] for votes in aave_v2_votes for vote in votes['votes']] + \n",
    "                         [vote['voter'].lower() for votes in aave_v3_votes for vote in votes['votes']]).value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(voter_counts, bins=50)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Voter')\n",
    "plt.ylabel('Number of votes casted')\n",
    "plt.title('Distribution of votes casted by each voter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_weights = []\n",
    "\n",
    "for votes in aave_v2_votes:\n",
    "    for vote in votes['votes']:\n",
    "        vote_weights.append(float(vote['weight']))\n",
    "\n",
    "for votes in aave_v3_votes:\n",
    "    for vote in votes['votes']:\n",
    "        vote_weights.append(float(vote['weight']))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(vote_weights, bins=50, log_scale=(True, False))\n",
    "plt.xlabel('Vote weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of vote weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_outcomes = pd.Series([proposal['state'] for proposal in aave_v2_proposals + aave_v3_proposals])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "proposal_outcomes.value_counts().plot(kind='bar')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Number of proposals')\n",
    "plt.title('Proposal outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegations_df = pd.DataFrame(delegations)\n",
    "delegations_df['date'] = pd.to_datetime(delegations_df['timestamp'], unit='s')\n",
    "delegations_df['date'] = delegations_df['date'].dt.date\n",
    "delegations_over_time = delegations_df.groupby('date').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "delegations_over_time.plot()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of delegations')\n",
    "plt.title('Delegations over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegator_counts = delegations_df['from'].value_counts()\n",
    "delegate_counts = delegations_df['to'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(delegator_counts, bins=50)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Delegator')\n",
    "plt.ylabel('Number of delegations')\n",
    "plt.title('Distribution of delegators')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(delegate_counts, bins=50)#, log_scale=(True, False))\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('Delegate')\n",
    "plt.ylabel('Number of delegations')\n",
    "plt.title('Distribution of delegates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_votes = {proposal['id']: float(proposal['currentYesVote']) for proposal in aave_v2_proposals} | \\\n",
    "            {proposal['id']: float(proposal['votes']['forVotes']) if proposal['votes'] else 0 for proposal in aave_v3_proposals}\n",
    "\n",
    "against_votes = {proposal['id']: float(proposal['currentNoVote']) for proposal in aave_v2_proposals} | \\\n",
    "                {proposal['id']: float(proposal['votes']['againstVotes']) if proposal['votes'] else 0 for proposal in aave_v3_proposals}\n",
    "                \n",
    "total_votes = {proposal['id']: float(proposal['currentYesVote']) + float(proposal['currentNoVote']) for proposal in aave_v2_proposals} | \\\n",
    "              {proposal['id']: float(proposal['votes']['forVotes']) + float(proposal['votes']['againstVotes']) if proposal['votes'] else 0 for proposal in aave_v3_proposals}\n",
    "\n",
    "num_voters = {proposal['id']: proposal['totalCurrentVoters'] for proposal in aave_v2_proposals} | \\\n",
    "             {proposal['id']: proposal['totalCurrentVoters'] for proposal in aave_v3_proposals}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(for_votes, bins=50, color='blue', kde=True)\n",
    "plt.xlabel('Number of FOR votes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of FOR votes')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(against_votes, bins=50, color='red', kde=True)\n",
    "plt.xlabel('Number of AGAINST votes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of AGAINST votes')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(total_votes, bins=50, color='yellow', kde=True)\n",
    "plt.xlabel('Number of total votes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of total votes')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(num_voters, bins=50, color='green', kde=True)\n",
    "plt.xlabel('Number of voters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of the number of voters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proposal_dates = [datetime.fromtimestamp(proposal['timestamp']) for proposal in aave_v2_proposals] + \\\n",
    "                     [datetime.fromtimestamp(int(proposal['transactions']['created']['timestamp'])) for proposal in aave_v3_proposals]\n",
    "\n",
    "proposal_dates_df = pd.DataFrame({'date': all_proposal_dates})\n",
    "proposal_dates_df['month'] = proposal_dates_df['date'].dt.to_period('M')\n",
    "\n",
    "proposals_per_month = proposal_dates_df.groupby('month').size().reset_index(name='count')\n",
    "\n",
    "proposals_per_month['month'] = proposals_per_month['month'].dt.to_timestamp()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(proposals_per_month['month'], proposals_per_month['count'])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of proposals')\n",
    "plt.title('Number of proposals per month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_votes_df = pd.DataFrame({\n",
    "    'date': all_proposal_dates,\n",
    "    'for_votes': [float(proposal['currentYesVote']) for proposal in aave_v2_proposals] +\n",
    "                 [float(proposal['votes']['forVotes']) if proposal['votes'] else 0 for proposal in aave_v3_proposals],\n",
    "    'against_votes': [float(proposal['currentNoVote']) for proposal in aave_v2_proposals] +\n",
    "                     [float(proposal['votes']['againstVotes']) if proposal['votes'] else 0 for proposal in aave_v3_proposals],\n",
    "    'num_voters': [proposal['totalCurrentVoters'] for proposal in aave_v2_proposals] +\n",
    "                  [proposal['totalCurrentVoters'] for proposal in aave_v3_proposals]\n",
    "})\n",
    "proposal_votes_df['month'] = proposal_votes_df['date'].dt.to_period('M')\n",
    "\n",
    "delegations_df = pd.DataFrame(delegations)\n",
    "delegations_df['date'] = pd.to_datetime(delegations_df['timestamp'], unit='s')\n",
    "delegations_df['date'] = delegations_df['date'].dt.date\n",
    "delegations_over_time = delegations_df.groupby('date').size()\n",
    "\n",
    "avg_votes_per_proposal = proposal_votes_df.groupby('month')['num_voters'].mean().reset_index(name='avg_num_voters')\n",
    "avg_votes_per_proposal['month'] = avg_votes_per_proposal['month'].dt.to_timestamp()\n",
    "\n",
    "avg_for_votes = proposal_votes_df.groupby('month')['for_votes'].mean().reset_index(name='avg_for_votes')\n",
    "avg_against_votes = proposal_votes_df.groupby('month')['against_votes'].mean().reset_index(name='avg_against_votes')\n",
    "avg_for_votes['month'] = avg_for_votes['month'].dt.to_timestamp()\n",
    "avg_against_votes['month'] = avg_against_votes['month'].dt.to_timestamp()\n",
    "\n",
    "delegations_df['month'] = pd.to_datetime(delegations_df['timestamp'], unit='s').dt.to_period('M')\n",
    "delegations_per_month = delegations_df.groupby('month').size().reset_index(name='delegation_count')\n",
    "delegations_per_month['month'] = delegations_per_month['month'].dt.to_timestamp()\n",
    "\n",
    "all_votes = [\n",
    "    {'date': datetime.fromtimestamp(proposal['timestamp']), 'voter': vote['id'].split('-')[0]}\n",
    "    for proposal, votes in zip(aave_v2_proposals, aave_v2_votes)\n",
    "    for vote in votes['votes']\n",
    "] + [\n",
    "    {'date': datetime.fromtimestamp(int(proposal['transactions']['created']['timestamp'])), 'voter': vote['voter'].lower()}\n",
    "    for proposal, votes in zip(aave_v3_proposals, aave_v3_votes)\n",
    "    for vote in votes['votes']\n",
    "]\n",
    "\n",
    "votes_df = pd.DataFrame(all_votes)\n",
    "votes_df['month'] = votes_df['date'].dt.to_period('M')\n",
    "\n",
    "unique_voters_per_month = votes_df.groupby('month')['voter'].nunique().reset_index(name='unique_voters')\n",
    "unique_voters_per_month['month'] = unique_voters_per_month['month'].dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(avg_votes_per_proposal['month'], avg_votes_per_proposal['avg_num_voters'])\n",
    "plt.ylabel('Avg. number of voters per proposal')\n",
    "plt.title('Average Number of Votes per Proposal per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(delegations_per_month['month'], delegations_per_month['delegation_count'])\n",
    "plt.ylabel('Number of delegations')\n",
    "plt.title('Number of Delegation Events per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(avg_for_votes['month'], avg_for_votes['avg_for_votes'], label='Avg. FOR votes')\n",
    "plt.plot(avg_against_votes['month'], avg_against_votes['avg_against_votes'], label='Avg. AGAINST votes')\n",
    "plt.ylabel('Avg. votes per proposal')\n",
    "plt.title('Average FOR and AGAINST Votes per Proposal per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unique_voters_per_month['month'], unique_voters_per_month['unique_voters'])\n",
    "plt.ylabel('Number of unique voters')\n",
    "plt.title('Number of Unique Voters per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
